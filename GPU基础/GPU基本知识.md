# 1  GPU编程技术的发展历程及现状

**阅读目录**

- [前言](https://www.cnblogs.com/muchen/p/6134374.html#_label0)
- [冯诺依曼计算机架构的瓶颈](https://www.cnblogs.com/muchen/p/6134374.html#_label1)
- [对 GPU 编程技术发展具有启发意义的几件事](https://www.cnblogs.com/muchen/p/6134374.html#_label2)
- [多点计算模型](https://www.cnblogs.com/muchen/p/6134374.html#_label3)
- [GPU 解决方案](https://www.cnblogs.com/muchen/p/6134374.html#_label4)
- [几款新的显卡及其配置 (仅列 N 卡)](https://www.cnblogs.com/muchen/p/6134374.html#_label5)
- [主流 GPU 编程接口](https://www.cnblogs.com/muchen/p/6134374.html#_label6)
- [学习 GPU 编程的意义](https://www.cnblogs.com/muchen/p/6134374.html#_label7)

[回到顶部](https://www.cnblogs.com/muchen/p/6134374.html#_labelTop)

## **前言**

​    本文通过介绍 GPU 编程技术的发展历程，让大家初步地了解 GPU 编程，走进 GPU 编程的世界。

[回到顶部](https://www.cnblogs.com/muchen/p/6134374.html#_labelTop)

## **冯诺依曼计算机架构的瓶颈**

​    曾经，几乎所有的处理器都是以冯诺依曼计算机架构为基础的。该系统架构简单来说就是处理器从存储器中不断取指，解码，执行。

​    但如今这种系统架构遇到了瓶颈：内存的读写速度跟不上 CPU 时钟频率。具有此特征的系统被称为内存受限型系统，目前的绝大多数计算机系统都属于此类型。

​    为了解决此问题，传统解决方案是使用缓存技术。通过给 CPU 设立多级缓存，能大大地降低存储系统的压力：

![img](https://images0.cnblogs.com/i/550724/201407/292117017747226.png)

​    然而随着缓存容量的增大，使用更大缓存所带来的收益增速会迅速下降，这也就意味着我们要寻找新的办法了。

[回到顶部](https://www.cnblogs.com/muchen/p/6134374.html#_labelTop)

## **对 GPU 编程技术发展具有启发意义的几件事**

\1. 70年代末期，克雷系列超级计算机研制成功 (克雷1当年耗资800万美元)。

​    此类计算机采用若干内存条的共享内存结构，即这些内存条可以与多个处理器相连接，从而发展成今天的对称多处理器系统 (SMD)。

​    克雷2是向量机 - 一个操作处理多个操作数。

​    如今的 GPU 设备的核心也正是向量处理器。

\2. 80年代初期，一家公司设计并研制了一种被称为连接机的计算机系统。

​    该系统具有16个 CPU 核，采用的是标准的单指令多数据 (SIMD) 并行处理。连接机通过这种设计能够消除多余的访存操作，并将内存读写周期变为原来的 1/16 。

\3. CELL 处理器的发明

​    这类处理器很有意思，其架构大致如下图所示：

![img](https://images0.cnblogs.com/i/550724/201407/302113544939820.png)

​    在此结构中，一个 PPC 处理器作为监管处理器，与大量的 SPE流处理器相连通，组成了一个工作流水线。

​    对于一个图形处理过程来说，某个 SPE 可负责提取数据，另一个 SPE 负责变换，再另一个负责存回。这样可构成一道完完整整的流水线，大大提高了处理速度。

​    顺便提一句，2010年超级计算机排名第三的计算机就是基于这种设计理念实现的，占地面积达560平方米，耗资 1.25 亿美元。

[回到顶部](https://www.cnblogs.com/muchen/p/6134374.html#_labelTop)

## **多点计算模型**

​    集群计算是指通过将多个性能一般的计算机组成一个运算网络，达到高性能计算的目的。这是一种典型的多点计算模型。

​    而 GPU 的本质，也同样是多点计算模型。其相对于当今比较火的Hadoop/Spark集群来说：“点”由单个计算机变成了 单个SM (流处理器簇)，通过网络互连变成了通过显存互连 (多点计算模型中点之间的通信永远是要考虑的重要问题)。

[回到顶部](https://www.cnblogs.com/muchen/p/6134374.html#_labelTop)

## **GPU 解决方案**

​    随着 CPU "功耗墙" 问题的产生，GPU 解决方案开始正式走上舞台。

​    GPU 特别适合用于并行计算浮点类型的情况，下图展示了这种情况下 GPU 和 CPU 计算能力的差别：

![img](https://images0.cnblogs.com/i/550724/201407/301225446964134.png)

​    但这可不能说明 GPU 比 CPU 更好，CPU应当被淘汰。 上图的测试是在计算可完全并行的情况下进行的。

​    对于逻辑更灵活复杂的串行程序，GPU 执行起来则远不如 CPU 高效 (没有分支预测等高级机制)。

​    另外，GPU 的应用早已不局限于图像处理。事实上 CUDA 目前的高端板卡 Tesla 系列就是专门用来进行科学计算的，它们连 VGA 接口都没。

[回到顶部](https://www.cnblogs.com/muchen/p/6134374.html#_labelTop)

## **几款新的显卡及其配置 (仅列 N 卡)**

![img](https://images0.cnblogs.com/i/550724/201407/301242117432484.png)

​    注：

​    \1. 各参数的具体含义将在以后的文章中做细致分析

​    \2. 当前显卡的具体参数信息可通过调试工具获取到 (方法略)

[回到顶部](https://www.cnblogs.com/muchen/p/6134374.html#_labelTop)

## **主流 GPU 编程接口**

​    **1. CUDA**

​    是英伟达公司推出的，专门针对 N 卡进行 GPU 编程的接口。文档资料很齐全，几乎适用于所有 N 卡。

​    本专栏讲述的 GPU 编程技术均基于此接口。

​    **2. Open CL**

​    开源的 GPU 编程接口，使用范围最广，几乎适用于所有的显卡。

​    但相对 CUDA，其掌握较难一些，建议先学 CUDA，在此基础上进行 Open CL 的学习则会非常简单轻松。

​    **3. DirectCompute**

​    微软开发出来的 GPU 编程接口。功能很强大，学习起来也最为简单，但只能用于 Windows 系统，在许多高端服务器都是 UNIX 系统无法使用。

​    总结，这几种接口各有优劣，需要根据实际情况选用。但它们使用起来方法非常相近，掌握了其中一种再学习其他两种会很容易。

[回到顶部](https://www.cnblogs.com/muchen/p/6134374.html#_labelTop)

## **学习 GPU 编程的意义**

​    1. 不单能学会如何使用 GPU 解决问题，更让我们更加深入地了解并行编程思想，为以后全面地掌握各种并行技术打下铺垫。

​    2. 并行计算相关知识的研究与发展势必会成为未来IT业界与学界的一大热点。

# 2. 从GPU的角度理解并行计算

## **前言**

　　本文从使用 GPU 编程技术的角度来了解计算中并行实现的方法思路。

[回到顶部](https://www.cnblogs.com/muchen/p/6138042.html#_labelTop)

## **并行计算中需要考虑的三个重要问题**

​    **1. 同步问题**

​    在操作系统原理的相关课程中我们学习过进程间的死锁问题，以及由于资源共享带来的临界资源问题等，这里不做累述。

　　**2. 并发度**

​    有一些问题属于 “易并行” 问题：如矩阵乘法。在这类型问题中，各个运算单元输出的结果是相互独立的，这类问题能够得到很轻松的解决 (通常甚至调用几个类库就能搞定问题)。

​    然而，若各个运算单元之间有依赖关系，那问题就复杂了。在 CUDA 中，块内的通信通过共享内存来实现，而块间的通信，则只能通过全局内存。

​    CUDA 并行编程架构可以用网格 (GRID) 来形容：一个网格好比一只军队。网格被分成好多个块，这些块好比军队的每个部门 (后勤部，指挥部，通信部等)。每个块又分成好多个线程束，这些线程束好比部门内部的小分队，下图可帮助理解：

![img](https://images0.cnblogs.com/i/550724/201407/311251107901983.png)

​    **3. 局部性**

​    在操作系统原理中，对局部性做过重点介绍，简单来说就是将之前访问过的数据 (时间局部性) 和之前访问过的数据的附近数据 (空间局部性) 保存在缓存中。

​    在 GPU 编程中，局部性也是非常重要的，这体现在要计算的数据应当在计算之前尽可能的一次性的送进显存，在迭代的过程中一定要尽可能减少数据在内存和显存之间的传输，实际项目中发现这点十分重要的。

​    对于 GPU 编程来说，需要程序猿自己去管理内存，或者换句话来说，自己实现局部性。

[回到顶部](https://www.cnblogs.com/muchen/p/6138042.html#_labelTop)

## **并行计算的两种类型**

​    **1. 基于任务的并行处理**

​    这种并行模式将计算任务拆分成若干个小的但不同的任务，如有的运算单元负责取数，有的运算单元负责计算，有的负责...... 这样一个大的任务可以组成一道流水线。

​    需要注意的是流水线的效率瓶颈在于其中效率最低的那个计算单元。

　　**2. 基于数据的并行处理**

​    这种并行模式将数据分解为多个部分，让多个运算单元分别去计算这些小块的数据，最后再将其汇总起来。

​    一般来说，CPU 的多线程编程偏向于第一种并行模式，GPU 并行编程模式则偏向于第二种。

[回到顶部](https://www.cnblogs.com/muchen/p/6138042.html#_labelTop)

## **常见的并行优化对象**

​    **1. 循环**

​    这也是最常见的一种模式，让每个线程处理循环中的一个或一组数据。

​    这种类型的优化一定要小心各个运算单元，以及每个运算单元何其自身上一次迭代结果的依赖性。

​    **2. 派生/汇集模式**

​    该模式下大多数是串行代码，但代码中的某一段可以并行处理。

​    典型的情况就是某个输入队列当串行处理到某个时刻，需要对其中不同部分进行不同处理，这样就可以划分成多个计算单元对改队列进行处理 (也即派生)，最后再将其汇总 (也即汇集)。

​    这种模式常用于并发事件事先不定的情况，具有 “动态并行性”。

​    **3. 分条/分块模式**

​    对于特别庞大的数据 (如气候模型)，可以将数据分为过个块来进行并行计算。

​    **4. 分而治之**

​    绝大多数的递归算法，比如快速排序，都可以转换为迭代模型，而迭代模型又能映射到 GPU 编程模型上。

​    特别说明：虽然费米架构和开普勒架构的 GPU 都支持缓冲栈，能够直接实现递归模型到 GPU 并行模型的转换。但为了程序的效率，在开发时间允许的情况下，我们最好还是先将其转换为迭代模型。

# 3  GPU并行编程的运算架构

## **前言**

   GPU 是如何实现并行的？它实现的方式较之 CPU 的多线程又有什么分别？

   本文将做一个较为细致的分析。

[回到顶部](https://www.cnblogs.com/muchen/p/6138691.html#_labelTop)

## **GPU 并行计算架构**

   GPU 并行编程的核心在于线程，一个线程就是程序中的一个单一指令流，一个个线程组合在一起就构成了并行计算网格，成为了并行的程序，下图展示了多核 CPU 与 GPU 的计算网格：

![img](https://images0.cnblogs.com/i/550724/201407/311232306021574.png)　　

   二者的区别将在后面探讨。

   下图展示了一个更为细致的 GPU 并行计算架构:

![img](https://images0.cnblogs.com/i/550724/201407/311236141333850.png)

   该图表示，计算网格由多个流处理器构成，每个流处理器又包含 n 多块。

   下面进一步对 GPU 计算网格中的一些概念做细致分析。

   **1. 线程**

   线程是 GPU 运算中的最小执行单元，线程能够完成一个最小的逻辑意义操作。

   **2. 线程束**

   线程束是 GPU 中的基本执行单元。GPU 是一组 SIMD 处理器的集合，因此每个线程束中的线程是同时执行的。这个概念是为了隐藏对显存进行读写带来的延迟所引入的。

   目前英伟达公司的显卡此值为 32，不可改动，也不应该对其进行改动。

   **3. 线程块**

   一个线程块包含多个线程束，在一个线程块内的所有线程，都可以使用共享内存来进行通信、同步。但一个线程块能拥有的最大线程/线程束，和显卡型号有关。

   **4. 流多处理器**

   流多处理器就相当于 CPU 中的核，负责线程束的执行。同一时刻只能有一个线程束执行。

   **5. 流处理器**

   流处理器只负责执行线程，结构相对简单。

[回到顶部](https://www.cnblogs.com/muchen/p/6138691.html#_labelTop)

## **GPU 和 CPU 在并行计算方面的不同**

   **1. 任务数量**

   CPU 适合比较少量的任务，而 GPU 则适合做大量的任务。

   **2. 任务复杂度**

   CPU 适合逻辑比较复杂的任务，而 GPU 则适合处理逻辑上相对简单的任务 (可用比较少的语句描述)。

   **3. 线程支持方式**

   由于 CPU 中线程的寄存器组是公用的，因此CPU 在切换线程的时候，会将线程的寄存器内容保存在 RAM 中，当线程再次启动的时候则会从 RAM 中恢复数据到寄存器。

   而 GPU 中的各个线程则各自拥有其自身的寄存器组，因此其切换速度会快上不少。

   当然，对于单个的线程处理能力来说，CPU 更强。

   **4. 处理器分配原则**

   CPU 一般是基于时间片轮转调度原则，每个线程固定地执行单个时间片；而 GPU 的策略则是在线程阻塞的时候迅速换入换出。

   **5. 数据吞吐量**

   GPU 中的每个流处理器就相当于一个 CPU 核，一个 GPU 一般具有 16 个流处理器，而且每个流处理器一次能计算 32 个数。

[回到顶部](https://www.cnblogs.com/muchen/p/6138691.html#_labelTop)

## **总结**

\1. 了解 CUDA 的线程模型是 GPU 并行编程的基础。

\2. 根据待处理数据类型来组织线程结构是非常非常重要的，而这并不轻松，尤其是当出现了需要共享的数据时。

# 4  GPU并行编程的存储系统架构

## 前言

​    在用 CUDA 对 GPU 进行并行编程的过程中，除了需要对线程架构要有深刻的认识外，也需要对存储系统架构有深入的了解。

​    这两个部分是 GPU 编程中最为基础，也是最为重要的部分，需要花时间去理解吸收，加深内功。　

[回到顶部](https://www.cnblogs.com/muchen/p/6297149.html#_labelTop)

## 了解 GPU 存储系统架构的意义

​    CUDA 编程架构的设计思路本身也就是让程序员去使用缓存，而不是让缓存像 CPU 编程结构那样对程序员透明。

​    通过对所使用存储结构的优化，能够让程序的并行后的效果得到很大提高。

​    因此，这个问题是需要我们在开发全程中考虑的。

[回到顶部](https://www.cnblogs.com/muchen/p/6297149.html#_labelTop)

## 第一层：寄存器

​    每个流处理器中的寄存器数以千计，每个线程都能分配到其私有的寄存器，这样做的好处是使得线程的切换几乎是零开销 (也许说是线程束的切换会更为准确)。

​    应当在硬件条件允许的情况下，尽可能地使用寄存器 (注意是硬件条件的允许之下)。

​    在核函数中定义的变量就是寄存器变量。

[回到顶部](https://www.cnblogs.com/muchen/p/6297149.html#_labelTop)

## 第二层：共享内存

​    共享内存的本质是可受用户控制的一级缓存。每个 SM 中的一级缓存与共享内存共享一个 64 KB的内存段。在费米架构中，可以为每个块定义 16 KB的共享内存。灵活地使用共享内存，能够大幅度提高显存的带宽。此外，共享内存也是实现块内线程间通信的有效工具。

​    使用时需要注意的一个地方是，只有在确定需要重复利用此空间的数据，或者明确要使块内线程进行通信的前提下，才考虑使用共享内存。(原因不解释)

​    使用时需要注意的另一个地方是应当尽可能地避免存储体冲突。这里所谓的存储体是指实现共享内存的硬件 - 一个费米架构的设备上有 32 个存储体。解决此问题的关键在于：顺序访问存储体。

​    实际开发中，常常将一个任务分解成多个部分(不论是任务分解还是数据分解)，共享内存在其中扮演着任务块工作任务汇总或者数据块工作任务汇总的角色。

​    核函数中定义的变量加上__shared__声明后就会存放在共享内存中了。

[回到顶部](https://www.cnblogs.com/muchen/p/6297149.html#_labelTop)

## 第三层：常量内存

​    常量内存其实只是全局内存的一种虚拟地址形式，并没有特殊保留的常量内存块。

​    使用起来非常方便，在主机端对需要放到常量内存区的变量添加 __constant__ 关键字声明之即可。

​    唯独需要注意的是，如果一个常量仅仅是一个字面值，那么将它声明为宏也行，例如 PI 这样的常数就一般定义为宏。

[回到顶部](https://www.cnblogs.com/muchen/p/6297149.html#_labelTop)

## 第四层：全局内存

​    全局内存，也就是显存。

​    在主机端开辟的显存空间均属于全局内存范畴。

​    使用全局内存的时候，需要注意的是应当学会对显存采取合并的访问方式。何谓合并的访问方式呢？请参阅下篇文章。

# 5   浅谈GPU并行编程和CPU并行编程的区别

## 前言

　　CPU 的并行编程技术，也是高性能计算中的热点，也是今后要努力学习的方向。那么它和 GPU 并行编程有何区别呢？

　　本文将做出详细的对比，分析各自的特点，为将来深入学习 CPU 并行编程技术打下铺垫。

[回到顶部](https://www.cnblogs.com/muchen/p/6297166.html#_labelTop)

## 区别一：缓存管理方式的不同

　　GPU：缓存对程序员不透明，程序员可根据实际情况操纵大部分缓存 (也有一部分缓存是由硬件自行管理)。

　　CPU：缓存对程序员透明。应用程序员无法通过编程手段操纵缓存。

[回到顶部](https://www.cnblogs.com/muchen/p/6297166.html#_labelTop)

## 区别二：指令模型的不同

　　GPU：采用 SIMT - 单指令多线程模型，一条指令配备一组硬件，对应32个线程 (一个线程束)。

　　CPU：采用 MIMD - 多指令多数据类型。多条指令构成指令流水线，且每个线程都有独立的硬件来操纵整个指令流。

　　用通俗易懂的话来说，GPU 采用频繁的线程切换来隐藏存储延迟，而 CPU 采用复杂的分支预测技术来达到此目的。

[回到顶部](https://www.cnblogs.com/muchen/p/6297166.html#_labelTop)

## 区别三：硬件结构的不同

　　GPU 内部有很多流多处理器。每个流多处理器都相当于一个“核"，而且一个流多处理器每次处理 32 个线程。

　　故 GPU 的数据吞吐量非常大，倾向于进行数据并发型优化；而 CPU 则倾向于任务并发型优化。

# 6  GPU并行优化的几种典型策略

## 前言

​    如何对现有的程序进行并行优化，是 GPU 并行编程技术最为关注的实际问题。本文将提供几种优化的思路，为程序并行优化指明道路方向。

[回到顶部](https://www.cnblogs.com/muchen/p/6297191.html#_labelTop)

## 优化前准备

​    首先，要明确优化的目标 - 是要将程序提速 2 倍？还是 10 倍？100倍？也许你会不假思索的说当然是提升越高越好。

​    但这里存在一个优化成本的问题。在同样的技术水平硬件水平下，提升 2 倍也许只要一个下午的工作量，但提高 10 倍可能要考虑到更多的东西，也许是一周的工作量。提高 100 倍， 1000 倍需要的成本，时间就更多了。

​    然后，需要将这个问题进行分解。通常来说先对数据集进行分解，然后将任务进行分解。这里要从数据集这样的矩阵角度来分析数据，将输入集和输出集中各个格点的对应关系找出来，然后分派给各个块，各个线程。

[回到顶部](https://www.cnblogs.com/muchen/p/6297191.html#_labelTop)

## 策略一：识别代码中的瓶颈所在

​    分析程序效率的瓶颈所在一方面靠的是分析。这种方式对于代码结构比较简单的程序非常有用，但对于实际应用中的复杂项目，人脑分析往往会导致错误的结论 - 也许你费尽心思想出来了瓶颈，然后对它做了优化，之后却发现效率仅仅提升了 1%。

​    因此更有效的方法是使用分析工具来找出瓶颈，可以使用 CUDA Profiler 或者 Parallel Nsight。

​    使用 Parallel Nsight 分析并行程序的方法请参考我的这篇文章：(准备中...)

​    还有一点要特别说明的是，在 GPU 进行数据处理的时候，CPU 可以考虑做点别的事情，比如去服务器取数之类的，这样就将 CPU 并行和 GPU 并行结合起来了，程序效率自然会大大提高。

[回到顶部](https://www.cnblogs.com/muchen/p/6297191.html#_labelTop)

## 策略二：合理的利用内存

​    首先，要灵活的使用显卡中的各类内存结构，如共享内存，常量内存等。特别要注意共享内存的使用，它的速度可是接近一级缓存的。

​    此外，必要时对多个内核函数进行融合。因为这样可以避免启动新的内核函数时需要进行的数据传递问题，还可以重用前面的任务遗留下的一些有用的数据。不过，如果是对别人写的多个内核函数进行融合的话，一定要注意其中隐含的同步问题 - 上个内核函数的代码彻底执行完毕之后，下个内核函数才会开始执行。

​    然后，对于数据的访问应该采取合并访问的方式 - 尽量使用 cudaMalloc 函数。一次访问的数据应当大于 128 字节，这样才能充分地利用显卡的带宽。

[回到顶部](https://www.cnblogs.com/muchen/p/6297191.html#_labelTop)

## 策略三：传输过程的优化

​    前面的文章已经提到过很多次了，数据在内存和显存之间进行交换是非常费时的。

​    对于这样的问题，首先我们可以以锁页内存的方式使用主机端内存。所谓锁页内存，是指该区域内存和显卡的传递不需要 CPU 来干预，如果某区域不声明为锁页内存，那么在内存往显存中或者显存往内存中传递数据前，会发生一些开销不小的锁定操作(表示该区域内存正在和显存发生数据传递，CPU勿扰)。

​    使用方法是调用 cudaHostAlloc 函数。这个函数的功能不单单是声明锁页内存那么简单。通过设置函数的参数，该函数还能实现很多非常实用的功能，个人非常推荐。

​    然后，还需要重点推荐的是零复制内存。它是一种特殊的锁页内存，一种特殊的内存映射。它允许你将主机内存映射到 GPU 的内存空间。如果你的程序是计算密集型的，那么这个机制就会非常有用，它会自动将数据传输和计算重叠。具体用法请参考我的这 篇文章。

[回到顶部](https://www.cnblogs.com/muchen/p/6297191.html#_labelTop)

## 策略四：线程结构布局的优化

​    建立科学的计算网格，通过设定合适的维数，块数，以及块内线程数来尽量实现合并的内存访问，保证最大的内存带宽。

​    要学会灵活使用多维度的计算网格，而不是仅仅局限于一维。多维计算网格的使用请参考我的这篇文章。

​    尤其在单维度的块数受到限制的时候，多维网格就必须被考虑进来了。

[回到顶部](https://www.cnblogs.com/muchen/p/6297191.html#_labelTop)

## 策略五：从算法本身进行任务级的分解

​    将算法的步骤分解各个不相关的部分，步骤内采用GPU并行，这几个步骤则采用CPU并行。

[回到顶部](https://www.cnblogs.com/muchen/p/6297191.html#_labelTop)

## 策略六：灵活使用 CUDA C 的一些库还有 API

​    CUDA C 提供了很多实用的 API，且提供相当多的C++支持 (非全部)。能大大地提高开发效率。如原子操作函数等等，很方便。

​    CUDA 提供了许多实用的库：如 cuBlas cuSparse等，不在此一一介绍。尤其是 Thrust 库，简直就是 STL 的并行实现，拿来直接用非常方便。

[回到顶部](https://www.cnblogs.com/muchen/p/6297191.html#_labelTop)

## 小结

​    优化思路可以说是 CUDA 并行编程最为核心，也是最为关键所在。

​    本文仅仅是提供优化的总体策略和思路，至于具体的实现方法，请参考相关资料实现之。