[TOC]

# 1  Baseline实验设置

## 1.1  模型：YOLOv3

- backbone：Darknet53
- Neck：YOLOv3Neck
- Head：YOLOv3head

## 1.2  数据集KITTI --2d object的划分

- train：6000张图片
- val：1481张图片
- test：7518张图片（无label）

## 1.3  类别数

​	8类（不含背景）

## 1.4  训练策略（采取默认）

train：

- data
  - 输入数据：多尺度输入
  - resize：img_scale=[(320, 320), (608, 608)]）
  - RandomFlip（flip_ratio=0.5）
  - Normalize（mean = [0,0,0]，std=[255., 255., 255.]，to_rgb=True）
  - pad（size_divisor=32）   
  -  assigner的类型，type='GridAssigner'
    -  pos_iou_thr=0.5
    -  neg_iou_thr=0.5
    - min_pos_iou=0
- loss fuction（class、confidence、xy、wh）
  - type：CrossEntropyLoss（wh的loss是MSELoss）
  - use_sigmoid=True
  - loss_weight=1.0（xy、wh的loss_weight是2.0）
- GPU
  - samples_per_gpu=8,即 每个gpu分配的样本数量为8    （当时用的单卡，后面可用两个卡DistributedDataParallel跑）
  - workers_per_gpu=2,即 每个gpu分配2个线程数用来pre-fetch 数据
- optimizer
  - type：SGD
  - lr=0.001  
  - momentum=0.9
  - weight_decay=0.0005
  - grad_clip=dict(max_norm=35, norm_type=2)   (shreshold为35防止梯度爆炸，梯度采用2范数与shreshold对比)
- learning policy
  - policy='step'  
  - warmup='linear'  
  - warmup_iters=2000  
  - warmup_ratio=0.1  
  - step=[218, 246])  
- runtime settings
  - total_epochs = 273  **实际只跑了260epochs，然后因为\home目录内存不足报错**
  - evaluation = dict(interval=1, metric=['bbox']) 

test：

- data
  - resize：img_scale = (608, 608)
  - Normalize（mean = [0,0,0]，std=[255., 255., 255.]，to_rgb=True） **要改**
  - pad（size_divisor=32）
- nms
  - nms_pre=1000  #  NMS之前的boxs个数
  - min_bbox_size=0
  - score_thr=0.05  
  - conf_thr=0.005
  - nms=dict(type='nms', iou_threshold=0.45)
  - max_per_img=100  #  Max number of detections of each image

## 1.5  评价指标

$$AP，AP_{50}，AP_{75}，AP_{S}，AM_{M}，AP_{L}$$

## 1.6  baseline

> pytorch 17.1,cuda10.1,2080Ti

train:

```
2020-12-30 08:31:00,195 - mmdet - INFO - Epoch [147][750/750]	lr: 1.987e-05, eta: 0:12:02, time: 0.313, data_time: 0.011, memory: 6934, loss_cls: 3.9693, loss_conf: 38.6206, loss_xy: 84.2320, loss_wh: 1.9434, loss: 128.7652, grad_norm: 865.6558
```



val:

```
2020-12-30 08:31:32,180 - mmdet - INFO - Epoch(val) \[147][750]	bbox_mAP: 0.4910, bbox_mAP_50: 0.8370, bbox_mAP_75: 0.5050, bbox_mAP_s: 0.4340, bbox_mAP_m: 0.5150, bbox_mAP_l: 0.5180
```





****

# 2  计划测试

计划的测试方案列表，包括超参调节和模型修改

## 2.1  Augmentation相关

- color_jitter
- aa
- randomErasing (reprob remode recount)
- mixup (mixup_off_epoch)
- smoothing
- Mosaic

> 除了Augmentatio

> 除了Augmentation相关，其他都是默认设置：epochs=30，opt=SGD，batchsize=64，lr_sched=0.01(*0.1/10epoch) ，min_lr=1e-5

|           Type           | $AP_{50}$ | $AP_{75}$ | $AP$ | time | 显存 |
| :----------------------: | :-------: | :-------: | :--: | ---- | ---- |
|         baseline         |           |           |      |      |      |
|       aa_original        |           |           |      |      |      |
|     color_jitter0.4      |           |           |      |      |      |
|      labelsmooth0.2      |           |           |      |      |      |
|         mixup0.3         |           |           |      |      |      |
|   rand_erase0.9_pixel    |           |           |      |      |      |
| rand_erase0.9_constwhite |           |           |      |      |      |
|   rand_erase0.9_const    |           |           |      |      |      |
|    rand_erase0.9_rand    |           |           |      |      |      |
|          Mosaic          |           |           |      |      |      |



## 2.2  learning rate相关

learning rate相关

- sched
- lr
- min_lr
- decay_epochs  #非零
- cooldown_epochs
- decay_rate

|                  lr<br />schedule                  | $AP_{50}$ | $AP_{75}$ | $AP$ |
| :------------------------------------------------: | :-------: | :-------: | :--: |
|                        tanh                        |           |           |      |
| CosineAnnealingWarmRestarts（T_0=5, **T_mult**=2） |           |           |      |
|                    Exponential                     |           |           |      |



|   lr   | $AP_{50}$ | $AP_{75}$ | $AP$ |
| :----: | :-------: | :-------: | :--: |
| 0.001  |           |           |      |
| 0.002  |           |           |      |
| 0.0005 |           |           |      |



| decay_epochs | $AP_{50}$ | $AP_{75}$ | $AP$ |
| :----------: | :-------: | :-------: | :--: |
|      8       |           |           |      |
|      5       |           |           |      |



| decay_rate | $AP_{50}$ | $AP_{75}$ | $AP$ |
| :--------: | :-------: | :-------: | :--: |
|    0.05    |           |           |      |
|    0.2     |           |           |      |
|    0.1     |           |           |      |



## 2.3  optimization

|                             opt                              | $AP_{50}$ | $AP_{75}$ | $AP$ |
| :----------------------------------------------------------: | :-------: | :-------: | :--: |
|                        baseline(sgd)                         |           |           |      |
|                           novograd                           |           |           |      |
|                           fusedsgd                           |           |           |      |
| Adam(*params*, *lr=0.001*, *betas=(0.9*, *0.999)*, *eps=1e-08*, *weight_decay=0*.0005) |           |           |      |
|                        adamw(lr=1e-3)                        |           |           |      |
|                        radam(lr=1e-3)                        |           |           |      |
|                        nadam(lr=1e-3)                        |           |           |      |
|      Adadelta(rho=0.9, eps=1e-06, weight_decay=0.0005)       |           |           |      |
|     Adagrad(*lr=0.01*, weight_decay=0.0005, *eps=1e-10*)     |           |           |      |

## 2.4  Activation Function

| Activation | $AP_{50}$ | $AP_{75}$ | $AP$ |
| ---------- | --------- | --------- | ---- |
|            |           |           |      |
|            |           |           |      |
|            |           |           |      |
|            |           |           |      |



# 3  可调参数

所有可调的超参数

## 3.1  未分类

- anchor 的base_size（论文中说anchor的size是数据集聚类得到，换个数据集，anchor也可以更改）
- loss fuction：loss_cls，loss_conf，loss_xy中的：
  - use sigmoid？（可选softmax）
  - 三者loss_weight的值（更加惩罚哪一个损失）

- image_scale

- batch_size: 128
- Dropout rate

## 3.2  Optimizer parameters

- opt: sgd   
  - 可选：
    - sgd、adam、nadam、adamw、radam、adadelta、rmsprop（内部alpha=0.9）、
      RMSpropTF、 novograd、nvnovograd
    - （APEX and CUDA required for fused optimizers）
      fusedsgd、fusedadam、fusedadamw、fusedlamb、fusednovograd
- momentum: 0.9   #SGD momentum (default: 0.9)

- weight_decay: 0.0001    #weight decay (default: 0.0001)

## 3.3  Learning rate schedule parameters

- sched: step   
  - LR scheduler 
  - 可选：step、 cosine 、tanh

|        LR Schedule        | $AP_{50}$ | $AP_{75}$ | $AP$ |
| :-----------------------: | :-------: | :-------: | :--: |
|       baseline(sgd)       |           |           |      |
|  adam+smoothing0.1+ step  |           |           |      |
| adam+smoothing0.1+ cosine |           |           |      |
|  adam+smoothing0.1+ tanh  |           |           |      |

- lr: 0.01   

  - learning rate (default: 0.01)
- warmup_lr: 0.0001   

  - warmup learning rate (default: 0.0001)
- min_lr: 1e-5   

  - lower lr bound for cyclic schedulers that hit 0 (default:1e-5)
- epochs: 120
- start_epoch: 0
- decay_epochs: 30    

  - epoch interval to decay LR (default=30)
- warmup_epochs: 5   

  - epochs to warmup LR, if scheduler supports
- cooldown_epochs: 10   

  - epochs to cooldown LR at min_lr, after cyclic schedule ends(default=10)
- decay_rate: 0.1   

  - LR decay rate (default: 0.1)

## 3.4  Augmentation parameters

- color_jitter: 0.4   

  - 修改亮度、对比度和饱和度 (default: 0.4),即三个属性都是0.4

  - 可以输入list分别调

    ```buildoutcfg
    if isinstance(color_jitter, (list, tuple)):
    # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation or 4 if also augmenting hue
    assert len(color_jitter) in (3, 4)
    ```

- aa:    

  - Use AutoAugment policy. "v0" or "original". (default: None)

- Random erase相关

  - reprob: 0.5   

    - Random erase prob (default: 0.)

  - remode: pixel   

    - Random erase mode 

    - 可选：const（无）、rand( ) 、  pixel（）

      ```buildoutcfg
      sl=0.02, sh=1/3
      area = img_h * img_w
      count = self.min_count if self.min_count == self.max_count else \
          random.randint(self.min_count, self.max_count)
      for _ in range(count):
          for attempt in range(10):
              target_area = random.uniform(self.sl, self.sh) * area / count
              log_ratio = (math.log(self.min_aspect), math.log(1 / self.min_aspect))
              aspect_ratio = math.exp(random.uniform(*log_ratio))
              h = int(round(math.sqrt(target_area * aspect_ratio)))
              w = int(round(math.sqrt(target_area / aspect_ratio)))
              if w < img_w and h < img_h:
                  top = random.randint(0, img_h - h)
                  left = random.randint(0, img_w - w)
                  img[:, top:top + h, left:left + w] = _get_pixels(
                      self.per_pixel, self.rand_color, (chan, h, w),
                  dtype=dtype, device=self.device)
      
      #=====================
      if per_pixel:
          return torch.empty(patch_size, dtype=dtype).normal_().to(device=device)
      elif rand_color:
      return torch.empty((patch_size[0], 1, 1), dtype=dtype).normal_().to(device=device)
      else:
      return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)
      ```

  - recount: 1   #Random erase count (default: 1)

- mixup相关

  - mixup: 0   #mixup alpha, mixup enabled if > 0. (default: 0.)
  - mixup_off_epoch: 0  #turn off mixup after this epoch, disabled if 0 (default: 0)

- smoothing: 0  #label smoothing (default: 0.1)

## Batch norm parameters 

(only works with gen_efficientnet based models currently)

- bn_tf: False     #F/T  Use Tensorflow BatchNorm defaults for models that support it (default: False)
- bn_momentum:  #float BatchNorm momentum override (default None)
- bn_eps: #float  BatchNorm epsilon override (default None)

##  Model Exponential Moving Average

- model_ema: False    #T/F Enable tracking moving average of model weights  default=False
- model_ema_force_cpu: False   #Force ema to be tracked on CPU, rank=0 node only. Disables - EMA validation   default=False
- model_ema_decay: 0.9998   #decay factor for model weights moving average (default: 0.9998)

## Misc

- log_interval: 50    
  - how many batches to wait before logging training status  default=50
- recovery_interval: 0    
  - how many batches to wait before writing recovery checkpoint  default=0
- amp: False   
  - T/F  use NVIDIA amp for mixed precision training   default=False
- sync_bn: False   
  - T/F   enabling apex sync BN   default=False
- no_prefetcher: False   
  -  T/F  disable fast prefetcher   default=False
- tta: 0  
  - int    Test/inference time augmentation (oversampling) factor. 0=None (default: 0)



# 问题：

- batch_size设为8才能运行，若为16及以上，报错：

```
RuntimeError: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 10.76 GiB total capacity; 9.56 GiB already allocated; 25.69 MiB free; 9.74 GiB reserved in total by PyTorch)

```

- lr_config的policy设置为`CosineAnnealing`可以work，但是设为`CosineRestart`报错，其具体设置为：

```python
lr_config = dict(
    policy='CosineRestart',
    warmup='linear',
    warmup_iters=2000,  # same as burn-in in darknet
    warmup_ratio=0.1,
    periods=[5],
    restart_weights=[2]
```















slowest epoch 37, average time is 0.3265
fastest epoch 38, average time is 0.3109
time std over epochs is 0.0032
average iter time: 0.3192 s/iter