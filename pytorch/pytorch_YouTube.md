# Neural Network Programming - Deep Learning with PyTorch

## 1 PyTorch Prerequisites - Syllabus For Neural Network Programming CoursePyTorch Prerequisites - Neural Network Programming Series

What's going on everyone? Welcome to this series on neural network programming with PyTorch.

![pytorch logo](https://deeplizard.com/images/pytorch-logo-dark.svg)

In this post, we will look at the prerequisites needed to be best prepared. We'll get an overview of the series and a sneak peek at a project we'll be working on. This will give us a good idea about what we'll be learning, and what skills we'll have by the end of the series. Without further ado, let's jump right in with the details.

There are two primary prerequisites needed for this series:

1. Programming experience
2. Neural network experience

Let's look at what we need to know for both of these categories.

#### Programming Experience

This neural network programming series will focus on programming neural networks using Python and PyTorch.

![python logo](https://deeplizard.com/images/python%20logo.svg)

Knowing Python beforehand is not necessary. However, understanding programming in general is a requirement. Any programming experience or exposure to concepts like variables, objects, and loops will be sufficient for successfully participating in this series.

#### Neural Network Experience

In this series, we'll be using PyTorch, and one of the things that we'll find about PyTorch itself is that it is a very thin deep learning neural network API for Python.

![img](https://deeplizard.com/images/png/neural%20network%203%20layers.png)

This means that, from a programming perspective, we'll be very close to programming neural networks from scratch. For this reason, it will definitely be beneficial to be aware of neural network and deep learning fundamentals. It's not a requirement, but it's recommended to take the [deep learning fundamentals series](https://deeplizard.com/learn/video/gZmobeGL0Yg) first.

### Neural Network Programming Series - Syllabus

To kick the series off, we have two parts. Let's look at the details of each part:

- PART 1: TENSORS AND OPERATIONS

- - Section 1: Introducing PyTorch
    - PyTorch Prerequisites - Neural Network Programming Series
    - PyTorch Explained - Python Deep Learning Neural Network API
    - PyTorch Install - Quick and Easy
    - CUDA Explained - Why Deep Learning Uses GPUs
  - Section 2: Introducing Tensors
    - Tensors Explained - Data Structures of Deep Learning
    - Rank, Axes, and Shape Explained - Tensors for Deep Learning
    - CNN Tensor Shape Explained - CNNs and Feature Maps
  - Section 3: PyTorch Tensors
    - PyTorch Tensors Explained - Neural Network Programming
    - Creating PyTorch Tensors for Deep Learning - Best Options
  - Section 4: Tensor Operations
    - Flatten, Reshape, and Squeeze Explained - Tensors for Deep Learning
    - CNN Flatten Operation Visualized - Tensor Batch Processing
    - Tensors for Deep Learning - Broadcasting and Element-wise Operations
    - ArgMax and Reduction Ops - Tensors for Deep Learning

- PART 2: NEURAL NETWORK TRAINING

- - Section 1: Data and Data Processing
    - Importance of Data in Deep Learning - Fashion MNIST for AI
    - Extract, Transform, Load (ETL) - Deep Learning Data Preparation
    - PyTorch Datasets and DataLoaders - Training Set Exploration
  - Section 2: Neural Networks and PyTorch Design
    - Build PyTorch CNN - Object Oriented Neural Networks
    - CNN Layers - Deep Neural Network Architecture
    - CNN Weights - Learnable Parameters in Neural Networks
    - Callable Neural Networks - Linear Layers in Depth
    - How to Debug PyTorch Source Code - Debugging Setup
    - CNN Forward Method - Deep Learning Implementation
    - Forward Propagation Explained - Pass Image to PyTorch Neural Network
    - Neural Network Batch Processing - Pass Image Batch to PyTorch CNN
    - CNN Output Size Formula - Bonus Neural Network Debugging Session
  - Section 3: Training Neural Networks
    - CNN Training - Using a Single Batch
    - CNN Training Loop - Using Multiple Epochs
    - Building a Confusion Matrix - Analyzing Results Part 1
    - Stack vs Concat - Deep Learning Tensor Ops
    - Using TensorBoard with PyTorch - Analyzing Results Part 2
    - Hyperparameter Experimenting - Training Neural Networks
  - Section 4: Neural Network Experimentation
    - Custom Code - Neural Network Experimentation Code
    - Custom Code - Simultaneous Hyperparameter Testing
    - Data Loading - Deep Learning Speed Limit Increase
    - On the GPU - Training Neural Networks with CUDA
    - Data Normalization - Normalize a Dataset
    - PyTorch DataLoader Source Code - Debugging Session
    - PyTorch Sequential Models - Neural Networks Made Easy
    - Batch Norm In PyTorch - Add Normalization To Conv Net Layers

#### Neural Network Programming: Part 1

Part one of the neural network programming series consists of two sections.

Section one will introduce [PyTorch and its features](https://deeplizard.com/learn/video/iTKbyFh-7GM). Importantly, we'll see why we should even use PyTorch in the first place. Stay tuned for that. It's a must see!

Additionally, we'll cover CUDA, a software platform for parallel computing on Nvidia GPUs. If you've ever wondered why deep learning uses GPUs in the first place, we'll be covering those details in the post on CUDA! This is also a must see!

Section two will be all about tensors, the data structures of deep learning. Having a strong understanding of tensors is essential for becoming a deep learning programming pro, so we'll be covering tensors in detail.

![img](https://deeplizard.com/images/rubik%20cube%20tensor2.jpg)

We'll be using PyTorch for this, of course, but the concepts and operations we learn in this section are necessary for understanding neural networks in general and will apply for any deep learning framework.

#### Neural Network Programming: Part 2

Part two of the neural network programming series is where we'll kick off the first deep learning project we'll be building together. Part two is comprised of three sections.

The first section will cover data and data processing for deep learning in general and how it relates to our deep learning project. Since tenors are the data structures of deep learning, we'll be using all of the knowledge learned about tensors from part one. We'll introduce the Fashion-MNIST dataset that we'll be using to build a convolutional neural network for image classification.

![img](https://deeplizard.com/images/fashion%20mnist%20grid%20sample.jpg)

We'll see how PyTorch datasets and data loaders are used to streamline data preprocessing and the training process.

The second section of part two will be all about building neural networks. We'll be building a convolutional neural neural network using PyTorch. This is where we'll see that PyTorch is super close to building neural networks from scratch. This section is also where the [deep learning fundamentals series](https://deeplizard.com/learn/video/gZmobeGL0Yg) will come in-handy most because we'll see the implementation of many concepts that are covered in that series.

The third section will show us how to train neural networks by constructing a training loop that optimizes the network's weights to fit our dataset. As we'll see, the training loop is built using an actual Python loop.

### Project Preview: Training A CNN With PyTorch

Our first project will consist of the following components:

1. Python imports
2. Data: ETL with the PyTorch Dataset and DataLoader classes
3. Model: Convolutional neural network
4. Training: The training loop
5. Analytics: Using a confusion matrix

By the end of part two of the neural network programming series, we'll have a complete understanding of this project, and this will enable us to be strong users of PyTorch as well as give us a deeper understanding of deep learning and neural networks in general.

### Resources For Getting Started

The neural network programming series will have plenty of resources that can ensure your success.

![img](https://deeplizard.com/images/zen-2040340_1920.jpg)

Here is a list:

- Video [playlist on YouTube](https://youtube.com/playlist?list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG)
- Text and video resources on [deeplizard.com](https://deeplizard.com/)
- Question and answer comments by the community [on YouTube](https://youtube.com/deeplizard)
- Code files are available for the [deeplizard hivemind](https://deeplizard.com/hivemind)

Be sure to check back this blog post as any resource updates will be reflected here and not in the video. Also, if you haven't already, check out the deeplizard [hivemind](https://deeplizard.com/hivemind) for exclusive perks and rewards and consider joining.

Deep learning and programming are both super powers that allow us humans to make the world a better place for all. We can now do more than just be intelligent. We can build intelligence.

![img](https://deeplizard.com/images/artificial-intelligence-3382507_1920-2.jpg)

We hope you'll join us in building collective intelligence by taking this series. Let us hear from you at the end, and importantly along the way! Good luck! I'll see you in the series.

## PyTorch Explained - Python Deep Learning Neural Network API

### PyTorch - Python Deep Learning Neural Network API

Welcome back to this series on neural network programming with PyTorch. To kick this series off, let's introduce PyTorch, a deep learning neural network package for Python. There's no better place to start as we'll be using PyTorch in this series to program our neural networks. Without further ado, let's get started.

![pytorch logo](https://deeplizard.com/images/pytorch-logo-dark.svg)

PyTorch is a deep learning framework and a scientific computing package. This is how the PyTorch core team describes PyTorch, anyway. The scientific computing aspect of PyTorch is primarily a result PyTorch's tensor library and associated tensor operations.



> A  *tensor* is an n-dimensional array.

![img](https://deeplizard.com/images/rubik%20cube%20tensor.jpg)

For example, PyTorch `torch.Tensor` objects that are created from NumPy `ndarray` objects, share memory. This makes the transition between PyTorch and NumPy very cheap from a performance perspective.

With PyTorch tensors, GPU support is built-in. It's very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system.

> PyTorch tensor operations can be performed on a GPU.

We'll talk more about GPUs and why we use them in deep learning in [the post on CUDA](https://deeplizard.com/learn/video/6stDhEA0wFQ). For now, just know that PyTorch tensors and their associated operations are very similar to NumPy n-dimensional arrays.

[Tensors](https://deeplizard.com/learn/video/Csa5R12jYRg) are super important for deep learning and neural networks because they are the data structure that we ultimately use for building and training our neural networks.

On top of the tensor library, PyTorch has much more to offer in terms of building and training neural networks. Before we touch on the deep learning specifics of PyTorch, let's look at some details on how PyTorch was created.

### PyTorch: A Brief History

The initial release of PyTorch was in October of 2016, and before PyTorch was created, there was and still is, another framework called *Torch*. [Torch](https://en.wikipedia.org/wiki/Torch_(machine_learning)) is a machine learning framework that's been around for quite a while and is based on the [Lua programming language](https://en.wikipedia.org/wiki/Lua_(programming_language)).

The connection between PyTorch and this Lua version, called Torch, exists because many of the developers who maintain the Lua version are the individuals who created PyTorch.

![python logo](https://deeplizard.com/images/pytorch-logo-dark.svg)

[Soumith Chintala](https://twitter.com/soumithchintala) is credited with bootstrapping the PyTorch project, and his reason for creating PyTorch is pretty simple, the Lua version of Torch was aging, and so a newer version written in Python was needed. As a result, PyTorch came to be.

#### Facebook Created PyTorch

One thing that you may hear about PyTorch is that it was created and is maintained by Facebook. This is because Soumith Chintala worked at [Facebook AI Research](https://research.fb.com/category/facebook-ai-research/) when PyTorch was created (still does at the time of this writing). However, there are many other companies with a vested interest in PyTorch.

![facebook logo](https://deeplizard.com/images/facebook%20logo.svg)

The PyTorch [GitHub repo](https://github.com/pytorch/pytorch/graphs/contributors) indicates that there are quite a few contributors, upwards of seven hundred at the current moment. Right near the top of the contributions by commit, we have Soumith, but there are many others.

Let's look now at the deep learning features of PyTorch.

### Deep Learning With PyTorch

This table gives us a list of PyTorch packages and their corresponding descriptions. These are the primary PyTorch components we'll be learning about and using as we build neural networks in this series.

| Package             | Description                                                  |
| ------------------- | ------------------------------------------------------------ |
| torch               | The top-level PyTorch package and tensor library.            |
| torch.nn            | A subpackage that contains modules and extensible classes for building neural networks. |
| torch.autograd      | A subpackage that supports all the differentiable Tensor operations in PyTorch. |
| torch.nn.functional | A functional interface that contains typical operations used for building neural networks like loss functions, activation functions, and [convolution](https://deeplizard.com/resource/pavq7noze2) operations. |
| torch.optim         | A subpackage that contains standard optimization operations like SGD and Adam. |
| torch.utils         | A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier. |
| torchvision         | A package that provides access to popular datasets, model architectures, and image transformations for computer vision. |

At the moment, the `torchvision` package is separate from the top-level torch package. However, this may change in the future if `torchvision` is pulled in as a subpackage of torch.

#### Why Use PyTorch For Deep Learning?

Let's talk about the prospects for learning PyTorch. For beginners to deep learning and neural networks, the top reason for learning PyTorch is that it is a thin framework that stays out of the way.

PyTorch is thin and stays out of the way!

When we build neural networks with PyTorch, we are super close to programming neural networks from scratch. The experience of programming in PyTorch is as close as it gets to the real thing.

PyTorch is as close as it gets to the real thing!

After understanding the process of programming neural networks with PyTorch, it's pretty easy to see how the process works from scratch in say pure Python. This is why PyTorch is great for beginners.

After using PyTorch, you'll have a much deeper understanding of neural networks and the deep learning. One of the top philosophies of PyTorch is to stay out of the way, and this makes it so that we can focus on neural networks and less on the actual framework.

#### Philosophy Of PyTorch

As of the writing of this post, PyTorch's development is guided by the following list:

1. Stay out of the way
2. Cater to the impatient
3. Promote linear code-flow
4. Full interop with the Python ecosystem
5. Be as fast as anything else

The fact that PyTorch stays out of the way makes PyTorch well suited for deepening our understanding of neural networks. When we write PyTorch code, we are just writing and extending standard Python classes, and when we debug PyTorch code, we are using the standard Python debugger.

PyTorch's design is modern, Pythonic, and thin. The source code is easy to read for Python developers because it's written mostly in Python, and only drops into C++ and CUDA code for operations that are performance bottlenecks.

Overall, PyTorch is a great tool for deepening our understanding of deep learning and neural networks.

#### Investing In PyTorch As A Deep Learning Framework

From a knowledge investment perspective, PyTorch can be seen as a safer option simply because Facebook is backing it, and it's built for Python, which unlike Lua, has a large and growing deep learning community.

In addition to the Facebook and Python edge, PyTorch is super thin and highly integrated with Python and very thin, which makes it more likely that PyTorch will be capable of adapting to the rapidly evolving deep learning environment as things change over time.

These characteristics promote the longevity of PyTorch as a deep learning framework.

#### PyTorch For Deep Learning Research

A common PyTorch characteristic that often pops up is that it's great for research. The reason for this research suitability has do do with a technical design consideration. To optimize neural networks, we need to calculate derivatives, and to do this computationally, deep learning frameworks use what are called [computational graphs](http://colah.github.io/posts/2015-08-Backprop/).

>  *Computational graphs* are used to graph the function operations that occur on tensors inside neural networks.

These graphs are then used to compute the derivatives needed to optimize the neural network. PyTorch uses a computational graph that is called a dynamic computational graph. This means that the graph is generated on the fly as the operations are created.

This is in contrast to static graphs that are fully determined before the actual operations occur.

It just so happens that many of the cutting edge research topics in deep learning are requiring or benefiting greatly from dynamic graphs.

### Convolutional Neural Network Project In PyTorch

The first project that we will tackle in this series using PyTorch will be to build a convolutional neural network for classifying images from the Fashion-MNIST dataset.

![fashion articles](https://deeplizard.com/images/fashion%20mnist%20grid%20sample.jpg)

This dataset contains a training set of sixty thousand examples from ten different classes of clothing items. We will use PyTorch to build a convolutional neural network that can accurately predict the correct article of clothing given an input piece, so stay tuned!

Let's get ready to move forward with deep learning and neural networks. In the [next post](https://deeplizard.com/learn/video/UWlFM0R_x6I), we will get PyTorch installed. I'll see you there!

## PyTorch Install - Quick And Easy

### Getting Ready To Install PyTorch

Welcome back to this series on neural network programming with PyTorch. In this episode, we are going to cover the needed prerequisites for installing PyTorch. Without further ado, let's get started.

![pytorch logo](https://deeplizard.com/images/pytorch-logo-dark.svg)

### Installing PyTorch With Anaconda And Conda

Getting started with PyTorch is very easy. The recommended best option is to use the Anaconda Python package manager.

With Anaconda, it's easy to get and manage Python, Jupyter Notebook, and other commonly used packages for scientific computing and data science, like PyTorch!

Let's go over the steps:

1. [Download and install Anaconda](https://anaconda.com/download/) (choose the latest Python version).
2. Go to [PyTorch's site](https://pytorch.org/) and find the *get started locally* section.
3. Specify the appropriate configuration options for your particular environment.
4. Run the presented command in the terminal to install PyTorch.

For the example, suppose we have the following configuration:

| Item            | Value   |
| --------------- | ------- |
| OS              | Windows |
| Package Manager | Conda   |
| Python          | 3.7     |
| CUDA            | 10.2    |

In this case, we have the following command:

```
conda install pytorch torchvision cudatoolkit=10.2 -c pytorch
```

Notice that we are installing both `PyTorch` and `torchvision`. Also, there is no need to install CUDA separately. The needed CUDA software comes installed with PyTorch if a CUDA version is selected in step (3). All we need to do is select a version of CUDA if we have a supported Nvidia GPU on our system.

```
conda list torch
# packages in environment at C:\Users\deeplizard\Anaconda3:
#
# Name         Version    Build                    Channel
pytorch        1.7.0      py3.6_cuda102_cudnn7_0   pytorch
torchvision    0.8.1      py36_cu102               pytorch
```

### Jupyter Notebook And VS Code (Optional)

In this series, we'll be using the following software for writing, debugging our code:

- [Visual Studio Code](https://code.visualstudio.com/) *- Integrated development environment*
- [Jupyter Notebook](https://jupyter.org/) *- Interactive environment*

Once you have Visual Studio Code installed, you'll also want to install the Python plugin. This is done from inside VS Code, in the *plugins* section.

We'll be using VS Code primarily for debugging our code. VS code makes debugging our code and inspecting our objects pretty easy. It's also useful for exploring the PyTorch source code. The navigation features for source code are pretty robust.

We won't use VS code until part two of the series, and most of our time will be spent inside Jupyter notebook. We automatically get Jupyter Notebook with the Anaconda installation. Neither of these tools are necessary, but they do make our lives as developers a lot easier.

### Verify The PyTorch Install

To verify our PyTorch installation is all set and that we are ready to code, we'll do this in a notebook. To organize the various parts of our project, we will create a folder called PyTorch and put everything in this folder.

Steps to verify the install:

1. To use PyTorch we `import torch`.
2. To check the version, we use `torch.__version__`

Now, to verify our GPU capabilities, we use `torch.cuda.is_available()` and check the cuda version.

```
> torch.cuda.is_available()
True

> torch.version.cuda
'10.2'
```

If your `torch.cuda.is_available()` call returns false, it may be because you don't have a supported Nvidia GPU installed on your system. However, don't worry, a GPU is *not required* to use PyTorch or to follow this series.

![nvidia logo](https://deeplizard.com/images/nvidia%20logo.svg)

We can obtain quite good results in a reasonable amount of time even without having a GPU. If you're interested in checking whether your Nvidia GPU supports CUDA, you can check for it [here](https://developer.nvidia.com/cuda-gpus).

### Why Deep Learning And Neural Networks Uses GPUs

Welcome back to this series on neural network programming with PyTorch. In this post, we are going to introduce CUDA at a high-level.

![cyborg](https://deeplizard.com/images/ai-cyborg-cropped-1.jpg)

The goal of this post is to help beginners understand what CUDA is and how it fits in with PyTorch, and more importantly, why we even use GPUs in neural network programming anyway.

#### Graphics Processing Unit (GPU)

To understand CUDA, we need to have a working knowledge of [graphics processing units](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPUs). A GPU is a processor that is good at handling *specialized* computations.

This is in contrast to a [central processing unit](https://en.wikipedia.org/wiki/Central_processing_unit) (CPU), which is a processor that is good at handling *general* computations. CPUs are the processors that power most of the typical computations on our electronic devices.

A GPU can be much faster at computing than a CPU. However, this is not always the case. The speed of a GPU relative to a CPU depends on the type of computation being performed. The type of computation most suitable for a GPU is a computation that can be done in parallel.

#### Parallel Computing

[Parallel computing](https://en.wikipedia.org/wiki/Parallel_computing) is a type of computation where by a particular computation is broken into independent smaller computations that can be carried out simultaneously. The resulting computations are then recombined, or synchronized, to form the result of the original larger computation.

![server room](https://deeplizard.com/images/server-2160321_1920.jpg)

The number of tasks that a larger task can be broken into depends on the number of cores contained on a particular piece of hardware. Cores are the units that actually do the computation within a given processor, and CPUs typically have four, eight, or sixteen cores while GPUs have potentially thousands.

There are other technical specifications that matter, but this description is meant to drive the general idea.

With this working knowledge, we can conclude that parallel computing is done using GPUs, and we can also conclude that tasks which are best suited to be solved using a GPU are tasks that can be done in parallel. If a computation can be done in parallel, we can accelerate our computation using parallel programming approaches and GPUs.

### Neural Networks Are Embarrassingly Parallel

Let's turn our attention now to neural networks and see why GPUs are used so heavily in deep learning. We have just seen that GPUs are well suited for parallel computing, and this fact about GPUs is why deep learning uses them. Neural networks are *embarrassingly parallel*.

In parallel computing, an [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) task is one where little or no effort is needed to separate the overall task into a set of smaller tasks to be computed in parallel.

Tasks that embarrassingly parallel are ones where it's easy to see that the set of smaller tasks are independent with respect to each other.

![neural network](https://deeplizard.com/images/png/neural%20network%202%203%202.png)

Neural networks are embarrassingly parallel for this reason. Many of the computations that we do with neural networks can be easily broken into smaller computations in such a way that the set of smaller computations do not depend on one another. One such example is a [convolution](https://deeplizard.com/resource/pavq7noze2).

#### Convolution Example

Let's look at an example, the convolution operation:

![convolution animation](https://deeplizard.com/images/convolution-animation-1.gif)

This animation showcases the [convolution](https://deeplizard.com/resource/pavq7noze2) process without numbers. We have an input channel in blue on the bottom. A convolutional filter shaded on the bottom that is sliding across the input channel, and a green output channel:

- Blue (bottom) - Input channel
- Shaded (on top of blue) - `3 x 3` convolutional filter
- Green (top) - Output channel

For each position on the blue input channel, the `3 x 3` filter does a computation that maps the shaded part of the blue input channel to the corresponding shaded part of the green output channel.

In the animation, these computations are happening sequentially one after the other. However, each computation is independent from the others, meaning that none of the computations depend on the results of any of the other computations.

As a result of this, all of these independent computations can happen in parallel on a GPU and the overall output channel can be produced.

This allows us to see that the [convolution](https://deeplizard.com/resource/pavq7noze2) operation can be accelerated by using a parallel programming approach and GPUs.

### Nvidia Hardware (GPU) And Software (CUDA)

This is where CUDA comes into the picture. [Nvidia](https://en.wikipedia.org/wiki/Nvidia) is a technology company that designs GPUs, and they have created CUDA as a software platform that pairs with their GPU hardware making it easier for developers to build software that accelerates computations using the parallel processing power of Nvidia GPUs.

![Nvidia logo](https://deeplizard.com/images/nvidia%20logo.svg)

An Nvidia GPU is the hardware that enables parallel computations, while CUDA is a software layer that provides an API for developers.

As a result, you might have guessed that an Nvidia GPU is required to use CUDA, and CUDA can be downloaded and installed from Nvidia's website for free.

Developers use CUDA by downloading the CUDA toolkit. With the toolkit comes specialized libraries like cuDNN, the CUDA Deep Neural Network library.

![cuDNN image](https://deeplizard.com/images/png/deep%20neural%20network%20with%204%20layers.png)

### PyTorch Comes With CUDA

One of the benefits of using PyTorch, or any other neural network API is that parallelism comes baked into the API. This means that as neural network programmers, we can focus more on building neural networks and less on performance issues.

With PyTorch, CUDA comes baked in from the start. There are no additional downloads required. All we need is to have a supported Nvidia GPU, and we can leverage CUDA using PyTorch. We don't need to know how to use the CUDA API directly.

Now, if we wanted to work on the PyTorch core development team or write PyTorch extensions, it would probably be useful to know how to use CUDA directly.

After all, PyTorch is written in all of these:

- Python
- C++
- CUDA

### Using CUDA With PyTorch

Taking advantage of CUDA is extremely easy in PyTorch. If we want a particular computation to be performed on the GPU, we can instruct PyTorch to do so by calling `cuda()` on our data structures (tensors).

Suppose we have the following code:

```
> t = torch.tensor([1,2,3])
> t
tensor([1, 2, 3])
```

The tensor object created in this way is on the CPU by default. As a result, any operations that we do using this tensor object will be carried out on the CPU.

Now, to move the tensor onto the GPU, we just write:

```
> t = t.cuda()
> t
tensor([1, 2, 3], device='cuda:0')
```

This ability makes PyTorch very versatile because computations can be selectively carried out either on the CPU or on the GPU.

#### GPU Can Be Slower Than CPU

We said that we can selectively run our computations on the GPU or the CPU, but why not just run *every* computation on the GPU?

Isn't a GPU faster than a CPU?

The answer is that a GPU is only faster for particular (specialized) tasks. One issue that we can run into is bottlenecks that slow our performance. For example, moving data from the CPU to the GPU is costly, so in this case, the overall performance might be slower if the computation task is a simple one.

![cpu vs cpu](https://deeplizard.com/images/gpu%20vs%20cpu.jpg)

Moving relatively small computational tasks to the GPU won't speed us up very much and may indeed slow us down. Remember, the GPU works well for tasks that can be broken into many smaller tasks, and if a compute task is already small, we won't have much to gain by moving the task to the GPU.

For this reason, it's often acceptable to simply use a CPU when just starting out, and as we tackle larger more complicated problems, begin using the GPU more heavily.

### GPGPU Computing

In the beginning, the main tasks that were accelerated using GPUs were computer graphics. Hence the name graphics processing unit, but in recent years, many more varieties parallel tasks have emerged. One such task as we have seen is deep learning.

Deep learning along with many other scientific computing tasks that use parallel programming techniques are leading to a new type of programming model called GPGPU or [general purpose GPU computing](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units).

GPGPU computing is more commonly just called GPU computing or accelerated computing now that it's becoming more common to preform a wide variety of tasks on a GPU.

Nvidia has been a pioneer in this space. Nvidia refers to general purpose GPU computing as simply GPU computing. Nvidia's CEO Jensen Huang's has envisioned GPU computing very early on which is why CUDA was created nearly 10 years ago.

Even though CUDA has been around for a long time, it is just now beginning to really take flight, and Nvidia's work on CUDA up until now is why Nvidia is leading the way in terms of GPU computing for deep learning.

When we hear Jensen talk about the GPU computing stack, he is referring to the GPU as the hardware on the bottom, CUDA as the software architecture on top of the GPU, and finally libraries like cuDNN on top of CUDA.

This GPU computing stack is what supports general purpose computing capabilities on a chip that is otherwise very specialized. We often see stacks like this in computer science as technology is built in layers, just like neural networks.

Sitting on top of CUDA and cuDNN is PyTorch, which is the framework were we'll be working that ultimately supports applications on top.

This [paper](https://arxiv.org/abs/1408.6923) takes a deep dive into GPU computing and CUDA, but it goes much deeper than we need. We will be working near the top of the stack here with PyTorch.

![research paper logo](https://deeplizard.com/images/logo-ArXiv.svg)

However, it's beneficial to have a birds eye view of just where we're operating within the overall stack.

### Tensors Are Up Next

We are ready now to jump in with section two of this neural network programming series, which is all about tensors.

![tensor operation](https://deeplizard.com/images/png/tensor%20operation.png)

We are ready now to jump in with section two of this neural network programming series, which is all about tensors. I hope you found this post useful. We should now have a good understand about why we use GPUs for neural network programming. For the first part of this series, we will be using a CPU. We are ready now to start working with `torch.Tensor`s and building our first neural networks.

I'll see you in the [next one!](https://deeplizard.com/learn/video/6stDhEA0wFQ)

### Introducing Tensors For Deep Learning

Welcome back to this series on neural network programming with PyTorch. In this post, we will kick off section two of the series, which is all about tensors.

![cyborg](https://deeplizard.com/images/artificial-intelligence-3382507_1920-cropped.jpg)

We'll talk tensors, terminology, and look at tensor indexes. This will give us the knowledge we need to look at some fundamental tensor attributes that are used in deep learning. Without further ado, let's get started.

### What Is A Tensor?

The inputs, outputs, and transformations within neural networks are all represented using tensors, and as a result, neural network programming utilizes tensors heavily.

A tensor is the primary [data structure](https://en.wikipedia.org/wiki/Data_structure) used by neural networks.

The concept of a tensor is a mathematical generalization of other more specific concepts. Let's look at some specific instances of tensors.

#### Specific Instances Of Tensors

Each of these examples are specific instances of the more general concept of a tensor:

- number
- scalar
- array
- vector
- 2d-array
- matrix

Let's organize the above list of example tensors into two groups:

- number, array, 2d-array
- scalar, vector, matrix

The first group of three terms (number, array, 2d-array) are terms that are typically used in computer science, while the second group (scalar, vector, matrix) are terms that are typically used in mathematics.

We often see this kind of thing where different areas of study use different words for the same concept. In deep learning, we usually just refer to all of these as tensors.

Let's investigate these terms further. The terms in each group correspond to one another as we move from left to right. To show this correspondence, we can reshape our list of terms to get three groups of two terms each:

- number, scalar
- array, vector
- 2d-array, matrix

#### Indexes Required To Access An Element

The relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure.

| Indexes required | Computer science | Mathematics |
| ---------------- | ---------------- | ----------- |
| 0                | number           | scalar      |
| 1                | array            | vector      |
| 2                | 2d-array         | matrix      |

For example, suppose we have this array:

```
> a = [1,2,3,4]
```

Now, suppose we want to access (refer to) the number 3 in this data structure. We can do it using a single index like so:

```
> a[2]
3
```

This logic works the same for a vector.

As another example, suppose we have this 2d-array:

```
> dd = [
[1,2,3],
[4,5,6],
[7,8,9]
]
```

Now, suppose we want to access (refer to) the number 3 in this data structure. In this case, we need two indexes to locate the specific element.

```
> dd[0][2]
3 
```

This logic works the same for a matrix.

Note that, if we have a number or scalar, we don't need an index, we can just refer to the number or scalar directly.

This gives us the working knowledge we need, so we are now ready to generalize.

### Tensors Are Generalizations

Let's look at what happens when there are more than two indexes required to access (refer to) a specific element within these data structures we have been considering.

![tensor](https://deeplizard.com/images/rubik%20cube%20tensor.jpg)

When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language.

#### Mathematics

In mathematics, we stop using words like scalar, vector, and matrix, and we start using the word *tensor* or nd-tensor. The n tells us the number of indexes required to access a specific element within the structure.

#### Computer Science

In computer science, we stop using words like, number, array, 2d-array, and start using the word *multidimensional array* or nd-array. The `n` tells us the number of indexes required to access a specific element within the structure.

| Indexes required | Computer science | Mathematics |
| ---------------- | ---------------- | ----------- |
| n                | nd-array         | nd-tensor   |

Let's make this clear. For practical purposes in neural network programming, tensors and nd-arrays are one in the same.

Tensors and nd-arrays are the same thing!

So tensors are multidimensional arrays or nd-arrays for short. The reason we say a tensor is a generalization is because we use the word tensor for all values of `n` like so:

- A scalar is a 0 dimensional tensor
- A vector is a 1 dimensional tensor
- A matrix is a 2 dimensional tensor
- A nd-array is an n dimensional tensor

Tensors allow us to drop these specific terms and just use an n to identify the number of dimensions we are working with.

One thing to note about the dimension of a tensor is that it differs from what we mean when we refer to the dimension of a vector in a vector space. The dimension of a tensor does not tell us how many components exist within the tensor.



If we have a three dimensional vector from three dimensional euclidean space, we have an ordered triple with three components.

A three dimensional tensor, however, can have many more than three components. Our two dimensional tensor dd for example has nine components.

```
> dd = [
[1,2,3],
[4,5,6],
[7,8,9]
]
```

### Wrapping Up

In the [next post](https://deeplizard.com/learn/video/Csa5R12jYRg), when we cover the concepts of rank, axes and shape, and we'll see how to determine the number of components contained within a tensor. These are the fundamental attributes of tensors that we use in deep learning.

Keep indexes in mind as we go over these concepts because indexes give us a concrete way of thinking about tensor related concepts. I'll see you in the next one!

### Rank, Axes And Shape - Tensors For Deep Learning

Welcome back to this series on neural network programming with PyTorch. In this post, we will dig in deeper with tensors and introduce three fundamental tensor attributes, rank, axes, and shape. Without further ado, let's get started.

![cyborg](https://deeplizard.com/images/artificial-intelligence-3382507_1920-cropped-gray.jpg)

The concepts of rank, axes, and shape are the tensor attributes that will concern us most in deep learning.

- Rank
- Axes
- Shape

The rank, axes, and shape are three tensor attributes that will concern us most when starting out with tensors in deep learning. These concepts build on one another starting with rank, then axes, and building up to shape, so keep any eye out for this relationship between these three.

![tensor cube](https://deeplizard.com/images/tensor%20cube%20forward%20pass.jpg)

Together rank, axes, and shape are all fundamentally connected to the concept of indexes that we discussed in the [previous post](https://deeplizard.com/learn/video/Csa5R12jYRg). If you haven't seen that one, I highly recommend you check it out. Let's kick things off starting at the ground floor by introducing the rank of a tensor.

### Rank Of A Tensor

The *rank* of a tensor refers to the number of dimensions present within the tensor. Suppose we are told that we have a rank-2 tensor. This means all of the following:

- We have a matrix
- We have a 2d-array
- We have a 2d-tensor

We are introducing the word *rank* here because it is commonly used in deep learning when referring to the number of dimensions present within a given tensor. This is just another one of the instances where different areas of study use different words to refer to the same concept. Don't let it throw you off!

#### Rank And Indexes

The rank of a tensor tells us how many indexes are required to access (refer to) a specific data element contained within the tensor data structure.

A tensor's rank tells us how many indexes are needed to refer to a specific element within the tensor.

Let's build on the concept of rank by looking at the axes of a tensor.

### Axes Of A Tensor

If we have a tensor, and we want to refer to a specific *dimension*, we use the word *axis* in deep learning.

An axis of a tensor is a specific dimension of a tensor.

If we say that a tensor is a rank `2` tensor, we mean that the tensor has `2` dimensions, or equivalently, the tensor has two axes.

Elements are said to exist or run along an axis. This *running* is constrained by the length of each axis. Let's look at the length of an axis now.

#### Length Of An Axis

The length of each axis tells us how many indexes are available along each axis.

Suppose we have a tensor called `t`, and we know that the first axis has a length of three while the second axis has a length of four.

Since the first axis has a length of three, this means that we can index three positions along the first axis like so:

```
t[0]
t[1]
t[2]
```

All of these indexes are valid, but we can't move passed index `2`.

Since the second axis has a length of four, we can index four positions along the second axis. This is possible for each index of the first axis, so we have

```
t[0][0]
t[1][0]
t[2][0]

t[0][1]
t[1][1]
t[2][1]

t[0][2]
t[1][2]
t[2][2]

t[0][3]
t[1][3]
t[2][3]
```

#### Tensor Axes Example

Let's look at some examples to make this solid. We'll consider the same tensor `dd` as before:

```
> dd = [
[1,2,3],
[4,5,6],
[7,8,9]
]
```

Each element along the first axis, is an array:

```
> dd[0]
[1, 2, 3]

> dd[1]
[4, 5, 6]

> dd[2]
[7, 8, 9]
```

Each element along the second axis, is a number:

```
> dd[0][0]
1

> dd[1][0]
4

> dd[2][0]
7

> dd[0][1]
2

> dd[1][1]
5

> dd[2][1]
8

> dd[0][2]
3

> dd[1][2]
6

> dd[2][2]
9
```

Note that, with tensors, the elements of the last axis are always numbers. Every other axis will contain n-dimensional arrays. This is what we see in this example, but this idea generalizes.

The rank of a tensor tells us how many axes a tensor has, and the length of these axes leads us to the very important concept known as the *shape* of a tensor.

### Shape Of A Tensor

The *shape* of a tensor is determined by the length of each axis, so if we know the shape of a given tensor, then we know the length of each axis, and this tells us how many indexes are available along each axis.

The shape of a tensor gives us the length of each axis of the tensor.

Let's consider the same tensor `dd` as before:

```
> dd = [
[1,2,3],
[4,5,6],
[7,8,9]
]
```

To work with this tensor's shape, we'll create a `torch.Tensor` object like so:

```
> t = torch.tensor(dd)
> t
tensor([
[1, 2, 3],
[4, 5, 6],
[7, 8, 9]
])

> type(t)
torch.Tensor
```

Now, we have a `torch.Tensor` object, and so we can ask to see the tensor's `shape`:

```
> t.shape
torch.Size([3,3])
```

This allows us to see the tensor's shape is `3 x 3`. Note that, in PyTorch, size and shape of a tensor are the same thing.

The shape of `3 x 3` tells us that each axis of this rank two tensor has a length of `3` which means that we have three indexes available along each axis. Let's look now at why the shape of a tensor is so important.

#### A Tensor's Shape Is Important

The shape of a tensor is important for a few reasons. The first reason is because the shape allows us to conceptually think about, or even visualize, a tensor. Higher rank tensors become more abstract, and the shape gives us something concrete to think about.

The shape also encodes all of the relevant information about axes, rank, and therefore indexes.

![tensor operation](https://deeplizard.com/images/png/tensor%20operation.png)

Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called *reshaping*.

As our tensors flow through our networks, certain shapes are expected at different points inside the network, and as neural network programmers, it is our job to understand the incoming shape and have the ability to reshape as needed.

#### Reshaping A Tensor

Before we look at reshaping tensors, recall how we reshaped the list of terms we started with [earlier](https://deeplizard.com/learn/video/Csa5R12jYRg):

**Shape `6 x 1`**

- number
- scalar
- array
- vector
- 2d-array
- matrix

**Shape `2 x 3`**

- number, array, 2d-array
- scalar, vector, matrix

**Shape `3 x 2`**

- number, scalar
- array, vector
- 2d-array, matrix

Each of these groups of terms represent the same underlying data only with differing shapes. This is just a little example to motivate the idea of reshaping.

The important take-away from this motivation is that the shape changes the grouping of the terms but does not change the underlying terms themselves.

Let's look at our example tensor dd again:

```
> t = torch.tensor(dd)
> t
tensor([
[1, 2, 3],
[4, 5, 6],
[7, 8, 9]
])
```

This `torch.Tensor` is a rank `2` tensor with a shape of `[3,3]` or `3 x 3`.

Now, suppose we need to reshape `t` to be of shape `[1,9]`. This would give us one array along the first axis and nine numbers along the second axis:

```
> t.reshape(1,9)
tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])

> t.reshape(1,9).shape
torch.Size([1, 9])
```

Now, one thing to notice about reshaping is that the product of the component values in the shape must equal the total number of elements in the tensor.

For example:

- 3 * 3 = 9
- 1 * 9 = 9

This makes it so that there are enough positions inside the tensor data structure to contain all of the original data elements after the reshaping.

Reshaping changes the shape but not the underlying data elements.

This was just a light introduction to tensor reshaping. In a [future post](https://deeplizard.com/learn/video/k6ZF1TSniYk), we'll cover the concept in more detail.

### Wrapping Up

This gives a an introduction to tensors. We should now have a good understanding of tensors and the lingo used to describe them like rank, axes, and shape. Soon, we'll see the various ways we can create tensors in PyTorch. I'll see you in the next one!

### CNN Tensor Input Shape And Feature Maps

Welcome back to this series on neural network programming. In this post, we will look at a practical example that demonstrates the use of the tensor concepts rank, axes, and shape.

![cyborg](https://deeplizard.com/images/artificial-intelligence-3382507_1920-cropped.jpg)

To do this, we'll consider a tensor input to a [convolutional neural network](https://deeplizard.com/learn/video/YRhxdVk_sIs). Without further ado, let's get started.

#### Convolutional Neural Network

In this neural network programming series, we are working our way up to building a convolutional neural network (CNN), so let's look at a tensor input for a CNN.

![cnn diagram](https://deeplizard.com/images/cnn%20diagram.jpg)

In the last two posts, we introduced [tensors](https://deeplizard.com/learn/video/Csa5R12jYRg) and the fundamental [tensor attributes rank, axes, and shape](https://deeplizard.com/learn/video/AiyK0idr4uM). If you haven't seen those posts yet, I highly recommend you check them out.

What I want to do now is put the concepts of rank, axes, and shape to use with a practical example. To do this, we'll consider an image input as a tensor to a CNN.

Convolutional neural networks are the go-to networks for image recognition tasks because they are well suited for detecting spacial patterns.

![fashion mnist](https://deeplizard.com/images/fashion%20mnist%20grid%20sample.jpg)

Remember that the shape of a tensor encodes all the relevant information about a tensor's axes, rank, and indexes, so we'll consider the shape in our example, and this will enable us to work out the other values. Let's begin.

### Shape Of A CNN Input

The shape of a CNN input typically has a length of four. This means that we have a rank-4 tensor with four axes. Each index in the tensor's shape represents a specific axis, and the value at each index gives us the length of the corresponding axis.

Each axis of a tensor usually represents some type of real world or logical feature of the input data. If we understand each of these features and their axis location within the tensor, then we can have a pretty good understanding of the tensor data structure overall.

To break this down, we'll work backwards, considering the axes from right to left. Remember, the last axis, which is where we'll start, is where the actual numbers or data values are located.

If we are running along the last axis and we stop to inspect an element there, we will be looking at a number. If we are running along any other axis, the elements are multidimensional arrays.

For images, the raw data comes in the form of pixels that are represented by a number and are laid out using two dimensions, height and width.

#### Image Height And Width

To represent two dimensions, we need two axes.

![boot image](https://deeplizard.com/images/fashion%20mnist%20ankle%20boot.jpg)

The image height and width are represented on the last two axes. Possible values here are `28 x 28`, as will be the case for our image data in the [fashion-MNIST dataset](https://deeplizard.com/learn/video/learn/video/EqpzfvxBx30) we'll be using in our CNN project, or the `224 x 224` image size that is used by VGG16 neural network, or any other image dimensions we can imagine.

#### Image Color Channels

The next axis represents the color channels. Typical values here are `3` for RGB images or `1` if we are working with grayscale images. This color channel interpretation only applies to the input tensor.

As we will reveal in a moment, the interpretation of this axis changes after the tensor passes through a convolutional layer.

Up to this point using the last three axes, we have represented a complete image as a tensor. We have the color channels and the height and width all laid out in tensor form using three axes.

In terms of accessing data at this point, we need three indexes. We choose a color channel, a height, and a width to arrive at a specific pixel value.

#### Image Batches

This brings us to the first axis of the four which represents the batch size. In neural networks, we usually work with batches of samples opposed to single samples, so the length of this axis tells us how many samples are in our batch.

![batch](https://deeplizard.com/images/forklift-835340_1920.jpg)

This allows us to see that an entire batch of images is represented using a single rank-4 tensor.

Suppose we have the following shape `[3, 1, 28, 28]` for a given tensor. Using the shape, we can determine that we have a batch of three images.

[Batch, Channels, Height, Width]

Each image has a single color channel, and the image height and width are `28 x 28` respectively.

1. Batch size
2. Color channels
3. Height
4. Width

This gives us a single rank-4 tensor that will ultimately flow through our convolutional neural network.

Given a tensor of images like this, we can navigate to a specific pixel in a specific color channel of a specific image in the batch using four indexes.

#### NCHW vs NHWC vs CHWN

It's common when reading API documentation and academic papers to see the `B` replaced by an `N`. The `N` standing for *number of samples* in a batch.

Furthermore, another difference we often encounter in the wild is a *reordering* of the dimensions. Common orderings are as follows:

- `NCHW`
- `NHWC`
- `CHWN`

As we have seen, PyTorch uses `NCHW`, and it is the case that TensorFlow and Keras use `NHWC` by default (it can be configured). Ultimately, the choice of which one to use depends mainly on performance. Some libraries and algorithms are more suited to one or the other of these orderings.

### Output Channels And Feature Maps

Let's look at how the interpretation of the color channel axis changes after the tensor is transformed by a convolutional layer.

Suppose we have a tensor that contains data from a single `28 x 28` grayscale image. This gives us the following tensor shape: `[1, 1, 28, 28]`.



Now suppose this image is passed to our CNN and passes through the first convolutional layer. When this happens, the shape of our tensor and the underlying data will be changed by the [convolution](https://deeplizard.com/resource/pavq7noze2) operation.

The [convolution](https://deeplizard.com/resource/pavq7noze2) changes the height and width dimensions as well as the number of channels. The number of output channels changes based on the number of filters being used in the convolutional layer.

![lion image](https://deeplizard.com/images/lion%20roar%20human%20girl%20edge%201.jpg)

Suppose we have three convolutional filters, and lets just see what happens to the channel axis.

Since we have three convolutional filters, we will have three channel outputs from the convolutional layer. These channels are outputs from the convolutional layer, hence the name output *channels opposed* to *color channels*.

Each of the three filters convolves the original single input channel producing three output channels. The output channels are still comprised of pixels, but the pixels have been modified by the convolution operation. Depending on the size of the filter, the height and width dimensions of the output will change also, but we'll leave those details for a future post.

#### Feature Maps

With the output channels, we no longer have color channels, but modified channels that we call *feature maps*. These so-called feature maps are the outputs of the convolutions that take place using the input color channels and the convolutional filters.

Feature maps are the output channels created from the convolutions.



The word feature is used because the outputs represent particular features from the image, like edges for example, and these mappings emerge as the network learns during the training process and become more complex as we move deeper into the network.

### Wrapping Up

We should now have a good understanding of the overall shape of an input tensor to a CNN, and how the concepts of rank, axes, and shape apply to this understanding.

We'll deepen our understanding of these concepts in future posts when we begin building our CNN. Until then, I'll see you in the [next one!](https://deeplizard.com/learn/video/jexkKugTg04)

### Introducing PyTorch Tensors

Welcome back to this series on neural network programming with PyTorch. In this post, we'll start to dig in deeper with PyTorch itself by exploring PyTorch tensors. Without further ado, let's get started.

![PyTorch logo](https://deeplizard.com/images/pytorch-logo-dark.svg)

PyTorch tensors are the data structures we'll be using when programming neural networks in PyTorch.

When programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form.

#### Instances Of The `torch.Tensor` Class

PyTorch tensors are instances of the `torch.Tensor` Python class. We can create a `torch.Tensor` object using the class constructor like so:

```
> t = torch.Tensor()
> type(t)
torch.Tensor
```

This creates an empty tensor (tensor with no data), but we'll get to adding data in just a moment.

#### Tensor Attributes

First, let's look at a few tensor attributes. Every `torch.Tensor` has these attributes:

- `torch.dtype`
- `torch.device`
- `torch.layout`

Looking at our Tensor `t`, we can see the following default attribute values:

```
> print(t.dtype)
> print(t.device)
> print(t.layout)
torch.float32
cpu
torch.strided
```

#### Tensors Have A `torch.dtype`

The `dtype`, which is `torch.float32` in our case, specifies the type of the data that is contained within the tensor. Tensors contain uniform (of the same type) numerical data with one of these types:

| Data type                | dtype         | CPU tensor         | GPU tensor              |
| ------------------------ | ------------- | ------------------ | ----------------------- |
| 32-bit floating point    | torch.float32 | torch.FloatTensor  | torch.cuda.FloatTensor  |
| 64-bit floating point    | torch.float64 | torch.DoubleTensor | torch.cuda.DoubleTensor |
| 16-bit floating point    | torch.float16 | torch.HalfTensor   | torch.cuda.HalfTensor   |
| 8-bit integer (unsigned) | torch.uint8   | torch.ByteTensor   | torch.cuda.ByteTensor   |
| 8-bit integer (signed)   | torch.int8    | torch.CharTensor   | torch.cuda.CharTensor   |
| 16-bit integer (signed)  | torch.int16   | torch.ShortTensor  | torch.cuda.ShortTensor  |
| 32-bit integer (signed)  | torch.int32   | torch.IntTensor    | torch.cuda.IntTensor    |
| 64-bit integer (signed)  | torch.int64   | torch.LongTensor   | torch.cuda.LongTensor   |

Notice how each type has a CPU and GPU version. One thing to keep in mind about tensor data types is that tensor operations between tensors must happen between tensors with the same type of data. However, this statement only applies to PyTorch versions lower than `1.3`. See the section below on *PyTorch Tensor Type Promotion* for details.

#### PyTorch Tensor Type Promotion

Arithmetic and comparison operations, as of PyTorch version `1.3`, can perform mixed-type operations that promote to a common `dtype`.

The example below was not allowed in version `1.2`. However, in version `1.3` and above, the same code returns a tensor with `dtype=torch.float32`.

```
torch.tensor([1], dtype=torch.int) + 
torch.tensor([1], dtype=torch.float32)
```

See the [full documentation](https://github.com/pytorch/pytorch/blob/master/docs/source/tensor_attributes.rst#type-promotion-doc) for more details.

- `torch.result_type` Provide function to determine result of mixed-type ops [26012](https://github.com/pytorch/pytorch/pull/26012).
- `torch.can_cast` Expose casting rules for type promotion [26805](https://github.com/pytorch/pytorch/pull/26805).
- `torch.promote_types` Expose promotion logic [26655](https://github.com/pytorch/pytorch/pull/26655).

#### Tensors Have A `torch.device`

The device, `cpu` in our case, specifies the device (CPU or GPU) where the tensor's data is allocated. This determines where tensor computations for the given tensor will be performed.

PyTorch supports the use of multiple devices, and they are specified using an index like so:

```
> device = torch.device('cuda:0')
> device
device(type='cuda', index=0)
```

If we have a device like above, we can create a tensor on the device by passing the device to the tensor's constructor. One thing to keep in mind about using multiple devices is that tensor operations between tensors must happen between tensors that exists on the same device.

Using multiple devices is typically something we will do as we become more advanced users, so there's no need to worry about that now.

#### Tensors Have A `torch.layout`

The layout, `strided` in our case, specifies how the tensor is stored in memory. To learn more about stride check [here](https://en.wikipedia.org/wiki/Stride_of_an_array).

For now, this is all we need to know.

#### Take Away From The Tensor Attributes

As neural network programmers, we need to be aware of the following:

1. Tensors contain data of a uniform type (`dtype`).
2. Tensor computations between tensors depend on the `dtype` and the `device`.

Let's look now at the common ways of creating tensors using data in PyTorch.

### Creating Tensors Using Data

These are the primary ways of creating tensor objects (instances of the `torch.Tensor` class), with data (array-like) in PyTorch:

1. `torch.Tensor(data)`
2. `torch.tensor(data)`
3. `torch.as_tensor(data)`
4. `torch.from_numpy(data)`

Let's look at each of these. They all accept some form of data and give us an instance of the `torch.Tensor` class. Sometimes when there are multiple ways to achieve the same result, things can get confusing, so let's break this down.

We'll begin by just creating a tensor with each of the options and see what we get. We'll start by creating some data.

We can use a Python list, or sequence, but `numpy.ndarray`s are going to be the more common option, so we'll go with a `numpy.ndarray` like so:

```
> data = np.array([1,2,3])
> type(data)
numpy.ndarray
```

This gives us a simple bit of data with a type of `numpy.ndarray`.

Now, let's create our tensors with each of these options 1-4, and have a look at what we get:

```
> o1 = torch.Tensor(data)
> o2 = torch.tensor(data)
> o3 = torch.as_tensor(data)
> o4 = torch.from_numpy(data)

> print(o1)
> print(o2)
> print(o3)
> print(o4)
tensor([1., 2., 3.])
tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)
```

All of the options (`o1`, `o2`, `o3`, `o4`) appear to have produced the same tensors except for the first one. The first option (`o1`) has dots after the number indicating that the numbers are `float`s, while the next three options have a type of `int32`.

```
// Python code example of what we mean

> type(2.)
float

> type(2)
int
```

In the [next post](https://deeplizard.com/learn/video/AglLTlms7HU), we will look more deeply at this difference as well as a few other important differences that are lurking behind the scenes.

The discussion in the [next post](https://deeplizard.com/learn/video/AglLTlms7HU) will allow us to see which of these options is best for creating tensors. For now, let's see some of the creation options available for creating tensors from scratch without having any data beforehand.

### Creation Options Without Data

Here are some other creation options that are available.

We have the `torch.eye()` function which returns a 2-D tensor with ones on the diagonal and zeros elsewhere. The name `eye()` is connected to the idea of an [identity matrix ](https://en.wikipedia.org/wiki/Identity_matrix), which is a square matrix with ones on the main diagonal and zeros everywhere else.

```
> print(torch.eye(2))
tensor([
    [1., 0.],
    [0., 1.]
])
```

We have the `torch.zeros()` function that creates a tensor of zeros with the shape of specified shape argument.

```
> print(torch.zeros([2,2]))
tensor([
    [0., 0.],
    [0., 0.]
])
```

Similarly, we have the `torch.ones()` function that creates a tensor of ones.

```
> print(torch.ones([2,2]))
tensor([
    [1., 1.],
    [1., 1.]
])
```

We also have the `torch.rand()` function that creates a tensor with a shape of the specified argument whose values are random.

```
> print(torch.rand([2,2]))
tensor([
    [0.0465, 0.4557],
    [0.6596, 0.0941]
])
```

This is a small subset of the available creation functions that don't require data. Check with the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for the full list.

I hope now you have a good understanding of how we can use PyTorch to create tensors from using data as well as the built in functions that don't require data. This task is a breeze if we are using `numpy.ndarray`s, so congratulations if you are already familiar with NumPy.

In the [next post](https://deeplizard.com/learn/video/AglLTlms7HU), we will look a little more deeply at the creation options that require data, and we'll discover the differences between these options as well as see which options work best. See you in the next one!

### reating PyTorch Tensors - Best Options

Welcome back to this series on neural network programming with PyTorch. In this post, we will look closely at the differences between the primary ways of transforming data into PyTorch tensors.

![cyborg](https://deeplizard.com/images/ai-cyborg-cropped-1.jpg)

By the end of this post, we'll know the differences between the primary options as well as which options should be used and when. Without further ado, let's get started.

PyTorch tensors as we have seen are instances of the `torch.Tensor` PyTorch class. The difference between the abstract concept of a tensor and a PyTorch tensor is that PyTorch tensors give us a concrete implementation that we can work with in code.

![img](https://deeplizard.com/images/pytorch-logo-dark.svg)

In the [last post](https://deeplizard.com/learn/video/jexkKugTg04), we saw how to create tensors in PyTorch using data like Python lists, sequences and NumPy ndarrays. Given a `numpy.ndarray`, we found that there are four ways to create a `torch.Tensor` object.

Here is a quick recap:

```
> data = np.array([1,2,3])
> type(data)
numpy.ndarray

> o1 = torch.Tensor(data)
> o2 = torch.tensor(data)
> o3 = torch.as_tensor(data)
> o4 = torch.from_numpy(data)
> print(o1)
tensor([1., 2., 3.])
> print(o2)
tensor([1, 2, 3], dtype=torch.int32)
> print(o3)
tensor([1, 2, 3], dtype=torch.int32)
> print(o4)
tensor([1, 2, 3], dtype=torch.int32)
```

Our task in this post is to explore the difference between these options and to suggest a best option for our tensor creation needs.

#### Numpy `dtype` Behavior On Different Systems

Depending on your machine and operating system, it is possible that your `dtype` may be different from what is shown here and in the video.

Numpy sets its default `dtype` based on whether it's running on a 32-bit or 64-bit system, and the behavior also differs on Windows systems.

This [link](https://stackoverflow.com/questions/36278590/numpy-array-dtype-is-coming-as-int32-by-default-in-a-windows-10-64-bit-machine) provides further information regrading the difference seen on Windows systems. The affected methods are: `tensor`, `as_tensor`, and `from_numpy`.

Thank you to David from the [hivemind](https://deeplizard.com/hivemind) for figuring this out!

### Tensor Creation Operations: *What's The Difference?*

Let's get started and figure out what these differences are all about.

#### Uppercase/Lowercase: `torch.Tensor()` Vs `torch.tensor()`

Notice how the first option `torch.Tensor()` has an uppercase `T` while the second option `torch.tensor()` has a lowercase `t`. What's up with this difference?

The first option with the uppercase `T` is the constructor of the `torch.Tensor` class, and the second option is what we call a *factory function* that constructs `torch.Tensor` objects and returns them to the caller.

![factory that represents a factory function](https://deeplizard.com/images/factory-3550550_1920.jpg)

You can think of the `torch.tensor()` function as a factory that builds tensors given some parameter inputs. Factory functions are a software design pattern for creating objects. If you want to read more about it check [here](https://en.wikipedia.org/wiki/Factory_(object-oriented_programming)).

Okay. That's the difference between the uppercase `T` and the lower case `t`, but which way is better between these two? The answer is that it's fine to use either one. However, the factory function `torch.tensor()` has better documentation and more configuration options, so it gets the winning spot at the moment.

#### Default `dtype` Vs Inferred `dtype`

Alright, before we knock the `torch.Tensor()` constructor off our list in terms of use, let's go over the difference we observed in the printed tensor outputs.

The difference is in the `dtype` of each tensor. Let's have a look:

```
> print(o1.dtype)
torch.float32

> print(o2.dtype)
torch.int32

> print(o3.dtype)
torch.int32

> print(o4.dtype)
torch.int32
```

The difference here arises in the fact that the `torch.Tensor()` constructor uses the default `dtype` when building the tensor. We can verify the default `dtype` using the `torch.get_default_dtype()` method:

```
> torch.get_default_dtype()
torch.float32
```

To verify with code, we can do this:

```
> o1.dtype == torch.get_default_dtype()
True
```

The other calls choose a dtype based on the incoming data. This is called *type inference*. The `dtype` is inferred based on the incoming data. Note that the `dtype` can also be explicitly set for these calls by specifying the `dtype` as an argument:

```
> torch.tensor(data, dtype=torch.float32)
> torch.as_tensor(data, dtype=torch.float32)
```

With `torch.Tensor()`, we are unable to pass a `dtype` to the constructor. This is an example of the `torch.Tensor()` constructor lacking in configuration options. This is one of the reasons to go with the `torch.tensor()` factory function for creating our tensors.

Let's look at the last hidden difference between these alternative creation methods.

#### Sharing Memory For Performance: Copy Vs Share

The third difference is lurking behind the scenes or underneath the hood. To reveal the difference, we need to make a change to the original input data in the `numpy.ndarray` after using the `ndarray` to create our tensors.

Let's do this and see what we get:

```
> print('old:', data)
old: [1 2 3]

> data[0] = 0

> print('new:', data)
new: [0 2 3]

> print(o1)
tensor([1., 2., 3.])

> print(o2)
tensor([1, 2, 3], dtype=torch.int32)

> print(o3)
tensor([0, 2, 3], dtype=torch.int32)

> print(o4)
tensor([0, 2, 3], dtype=torch.int32)
```

Note that originally, we had `data[0]=1`, and also note that we only changed the data in the original `numpy.ndarray`. Notice we didn't explicity make any changes to our tensors (`o1`, `o2`, `o3`, `o4`).

However, after setting `data[0]=0`, we can see some of our tensors have changes. The first two `o1` and `o2` still have the original value of `1` for index `0`, while the second two `o3` and `o4` have the new value of `0` for index `0`.



This happens because `torch.Tensor()` and `torch.tensor()` *copy* their input data while `torch.as_tensor()` and `torch.from_numpy()` *share* their input data in memory with the original input object.

| Share Data         | Copy Data      |
| ------------------ | -------------- |
| torch.as_tensor()  | torch.tensor() |
| torch.from_numpy() | torch.Tensor() |

This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the `torch.Tensor` and the `numpy.ndarray`.

Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory.

If we have a `torch.Tensor` and we want to convert it to a `numpy.ndarray`, we do it like so:

```
> print(o3.numpy())
[0 2 3]

> print(o4.numpy())
[0 2 3]
```

This gives:

```
> print(type(o3.numpy()))
<class 'numpy.ndarray'>

> print(type(o4.numpy()))
<class 'numpy.ndarray'>
```

This establishes that `torch.as_tensor()` and `torch.from_numpy()` both share memory with their input data. However, which one should we use, and how are they different?

The `torch.from_numpy()` function only accepts `numpy.ndarray`s, while the `torch.as_tensor()` function accepts a wide variety of [array-like objects](https://docs.scipy.org/doc/numpy/user/basics.creation.html#converting-python-array-like-objects-to-numpy-arrays) including other PyTorch tensors. For this reason, `torch.as_tensor()` is the winning choice in the memory sharing game.

### Best Options For Creating Tensors In PyTorch

Given all of these details, these two are the best options:

- `torch.tensor()`
- `torch.as_tensor()`

The `torch.tensor()` call is the sort of go-to call, while `torch.as_tensor()` should be employed when tuning our code for performance.

![img](https://deeplizard.com/images/png/tensor%20operation.png)

Some things to keep in mind about memory sharing (it works where it can):

1. Since `numpy.ndarray` objects are allocated on the CPU, the `as_tensor()` function must copy the data from the CPU to the GPU when a GPU is being used.
2. The memory sharing of `as_tensor()` doesn't work with built-in Python data structures like lists.
3. The `as_tensor()` call requires developer knowledge of the sharing feature. This is necessary so we don't inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects.
4. The `as_tensor()` performance improvement will be greater if there are a lot of back and forth operations between `numpy.ndarray` objects and tensor objects. However, if there is just a single load operation, there shouldn't be much impact from a performance perspective.

![pytorch logo](https://deeplizard.com/images/pytorch-logo-flame.svg)

#### Wrapping Up

At this point, we should now have a better understanding of the PyTorch `tensor` creation options. We've learned about factory functions and we've seen how memory *sharing vs copying* can impact performance and program behavior. I'll see you in the [next one](https://deeplizard.com/learn/video/fCVuiW9AFzY)!

### Reshaping Operations - Tensors For Deep Learning

Welcome back to this series on neural network programming. Starting with this post in this series, we'll begin using the knowledge we've learned about tensors up to this point and start covering essential tensor operations for neural networks and deep learning.

![cyborg](https://deeplizard.com/images/artificial-intelligence-3382507_1920-cropped.jpg)

We'll kick things off with reshaping operations. Without further ado, let's get started.

### Tensor Operation Types

Before we dive in with specific tensor operations, let's get a quick overview of the landscape by looking at the main operation categories that encompass the operations we'll cover. We have the following high-level categories of operations:

1. Reshaping operations
2. Element-wise operations
3. Reduction operations
4. Access operations

There are a lot of individual operations out there, so much so that it can sometimes be intimidating when you're just beginning, but grouping similar operations into categories based on their likeness can help make learning about tensor operations more manageable.

The reason for showing these categories is to give you the goal of understanding all four of these by the end of this section in the series.

![neural network](https://deeplizard.com/images/png/tensor%20operation.png)

The goal of these posts on tensor operations is to not only showcase specific tensor operations commonly used, but to also describe the operation landscape. Having knowledge of the types of operations that exist can stay with us longer than just knowing or memorizing individual operations.

Keep this in mind and work towards understanding these categories as we explore each of them. Let's jump in now with reshaping operations.

### Reshaping Operations For Tensors

*Reshaping operations* are perhaps the most important type of tensor operations. This is because, like we mentioned in [the post where we introduced tensors](https://deeplizard.com/learn/video/Csa5R12jYRg), the shape of a tensor gives us something concrete we can use to *shape* an intuition for our tensors.

#### An Analogy For Tensors

Suppose we are neural network programmers, and as such, we typically spend our days building neural networks. To do our job, we use various tools.

We use math tools like calculus and linear algebra, computer science tools like Python and PyTorch, physics and engineering tools like CPUs and GPUs, and machine learning tools like neural networks, layers, activation functions, etc.

![baker standing in font of dough](https://deeplizard.com/images/bakery-1868396_1920.jpg)

Our task is to build neural networks that can transform or map input data to the correct output we are seeking.

The primary ingredient we use to produce our product, a function that maps inputs to correct outputs, is data.

[Data](https://deeplizard.com/learn/video/DoJiau6mPFc) is somewhat of an abstract concept, so when we want to actually use the concept of data to implement something, we use a specific data structure called a tensor that can be efficiently implemented in code. Tensors have properties, mathematical and otherwise, that allow us to do our work.

[Tensors](https://deeplizard.com/learn/video/Csa5R12jYRg) are the primary ingredient that neural network programmers use to produce their product, intelligence.

This is very similar to how a baker uses dough to produce, say, a pizza. The dough is the input used to create an output, but before the pizza is produced there is usually some form of reshaping of the input that is required.

![pizza in stone oven](https://deeplizard.com/images/pizza-744405_1920.jpg)

As neural network programmers, we have to do the same with our tensors, and usually shaping and reshaping our tensors is a frequent task.

Our networks operate on tensors, after all, and this is why understanding a tensor's shape and the available reshaping operations are super important.

Instead of producing pizzas, we are producing intelligence! This may be lame, but whatever. Let's jump in with reshaping operations.

#### Tensor Shape Review

Suppose that we have the following tensor:

```
> t = torch.tensor([
    [1,1,1,1],
    [2,2,2,2],
    [3,3,3,3]
], dtype=torch.float32)
```

To determine the shape of this tensor, we look first at the rows `3` and then the columns `4`, and so this tensor is a `3 x 4` rank `2` tensor. Remember, *rank* is a word that is commonly used and just means the number of dimensions present within the tensor.

In PyTorch, we have two ways to get the shape:

```
> t.size()
torch.Size([3, 4])

> t.shape
torch.Size([3, 4])
```

In PyTorch the *size* and *shape* of a tensor mean the same thing.

Typically, after we know a tensor's shape, we can deduce a couple of things. First, we can deduce the tensor's rank. The rank of a tensor is equal to the length of the tensor's shape.

```
> len(t.shape)
2
```

We can also deduce the number of elements contained within the tensor. The number of elements inside a tensor (12 in our case) is equal to the product of the shape's component values.

```
> torch.tensor(t.shape).prod()
tensor(12)
```

In PyTorch, there is a dedicated function for this:

```
> t.numel()
12
```

The number of elements contained within a tensor is important for reshaping because the reshaping must account for the total number of elements present. Reshaping changes the tensor's shape but not the underlying data. Our tensor has `12` elements, so any reshaping must account for exactly `12` elements.

#### Reshaping A Tensor In PyTorch

Let's look now at all the ways in which this tensor `t` can be reshaped without changing the rank:

```
> t.reshape([1,12])
tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])

> t.reshape([2,6])
tensor([[1., 1., 1., 1., 2., 2.],
        [2., 2., 3., 3., 3., 3.]])

> t.reshape([3,4])
tensor([[1., 1., 1., 1.],
        [2., 2., 2., 2.],
        [3., 3., 3., 3.]])

> t.reshape([4,3])
tensor([[1., 1., 1.],
        [1., 2., 2.],
        [2., 2., 3.],
        [3., 3., 3.]])

> t.reshape(6,2)
tensor([[1., 1.],
        [1., 1.],
        [2., 2.],
        [2., 2.],
        [3., 3.],
        [3., 3.]])

> t.reshape(12,1)
tensor([[1.],
        [1.],
        [1.],
        [1.],
        [2.],
        [2.],
        [2.],
        [2.],
        [3.],
        [3.],
        [3.],
        [3.]])
```

Using the `reshape()` function, we can specify the `row x column` shape that we are seeking. Notice how all of the shapes have to account for the number of elements in the tensor. In our example this is:

rows * columns = 12 elements

We can use the intuitive words *rows* and *columns* when we are dealing with a rank `2` tensor. The underlying logic is the same for higher dimensional tenors even though we may not be able to use the intuition of rows and columns in higher dimensional spaces. For example:

```
> t.reshape(2,2,3)
tensor(
[
    [
        [1., 1., 1.],
        [1., 2., 2.]
    ],

    [
        [2., 2., 3.],
        [3., 3., 3.]
    ]
])
```

In this example, we increase the rank to `3`, and so we lose the *rows and columns* concept. However, the product of the shape's components (`2`,`2`,`3`) still has to be equal to the number of elements in the original tensor ( `12`).

Note that PyTorch has another function that you may see called `view()` that does the same thing as the `reshape()` function, but don't let these names through you off. No matter which deep learning framework we are using, these concepts will be the same.

### Changing Shape By Squeezing And Unsqueezing

The next way we can change the shape of our tensors is by *squeezing* and *unsqueezing* them.

- *Squeezing* a tensor removes the dimensions or axes that have a length of one.
- *Unsqueezing* a tensor adds a dimension with a length of one.

These functions allow us to expand or shrink the rank (number of dimensions) of our tensor. Let's see this in action.

```
> print(t.reshape([1,12]))
> print(t.reshape([1,12]).shape)
tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])
torch.Size([1, 12])

> print(t.reshape([1,12]).squeeze())
> print(t.reshape([1,12]).squeeze().shape)
tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])
torch.Size([12])

> print(t.reshape([1,12]).squeeze().unsqueeze(dim=0))
> print(t.reshape([1,12]).squeeze().unsqueeze(dim=0).shape)
tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])
torch.Size([1, 12])
```

Notice how the shape changes as we squeeze and unsqueeze the tensor.

Let's look at a common use case for squeezing a tensor by building a *flatten* function.

#### Flatten A Tensor

A *flatten* operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements.

*Flattening* a tensor means to remove all of the dimensions except for one.

Let's create a Python function called `flatten()`:

```
def flatten(t):
    t = t.reshape(1, -1)
    t = t.squeeze()
    return t
```

The `flatten()` function takes in a tensor `t` as an argument.

Since the argument `t` can be any tensor, we pass `-1` as the second argument to the `reshape()` function. In PyTorch, the `-1` tells the `reshape()` function to figure out what the value should be based on the number of elements contained within the tensor. Remember, the shape must equal the product of the shape's component values. This is how PyTorch can figure out what the value should be, given a `1` as the first argument.

Since our tensor `t` has `12` elements, the `reshape()` function is able to figure out that a `12` is required for the length of the second axis.

After squeezing, the first axis (axis-0) is removed, and we obtain our desired result, a 1d-array of length `12`.

Here's an example of this in action:

```
> t = torch.ones(4, 3)
> t
tensor([[1., 1., 1.],
    [1., 1., 1.],
    [1., 1., 1.],
    [1., 1., 1.]])

> flatten(t)
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
```

In a future post when we begin building a convolutional neural network, we will see the use of this `flatten()` function. We'll see that *flatten* operations are required when passing an output tensor from a convolutional layer to a linear layer.

In these examples, we have flattened the entire tensor, however, it is possible to flatten only specific parts of a tensor. For example, suppose we have a tensor of shape `[2,1,28,28]` for a CNN. This means that we have a batch of `2` grayscale images with height and width dimensions of `28 x 28`, respectively.

Here, we can specifically flatten the two images. To get the following shape: `[2,1,784]`. We could also squeeze off the channel axes to get the following shape: `[2,784]`.

#### Concatenating Tensors

We combine tensors using the `cat()` function, and the resulting tensor will have a shape that depends on the shape of the two input tensors.

Suppose we have two tensors:

```
> t1 = torch.tensor([
    [1,2],
    [3,4]
])
> t2 = torch.tensor([
    [5,6],
    [7,8]
])
```

We can combine `t1` and `t2` row-wise (axis-0) in the following way:

```
> torch.cat((t1, t2), dim=0)
tensor([[1, 2],
        [3, 4],
        [5, 6],
        [7, 8]])
```

We can combine them column-wise (axis-1) like this:

```
> torch.cat((t1, t2), dim=1)
tensor([[1, 2, 5, 6],
        [3, 4, 7, 8]])
```

When we concatenate tensors, we increase the number of elements contained within the resulting tensor. This causes the component values within the shape (lengths of the axes) to adjust to account for the additional elements.

```
> torch.cat((t1, t2), dim=0).shape
torch.Size([4, 2])

> torch.cat((t1, t2), dim=1).shape
torch.Size([2, 4])
```

### Conclusion About Reshaping Tensors

We should now have a good understanding of what it means to reshape a tensor. Any time we change a tensor's shape, we are said to be *reshaping* the tensor.

Remember the analogy. Bakers work with dough and neural network programmers work with tensors. Even though the concept of shaping is the same, instead of creating baked goods, we are creating intelligence. I'll see you in [the next one](https://deeplizard.com/learn/video/mFAIBMbACMA).

### Flatten Operation For A Batch Of Image Inputs To A CNN

Welcome back to this series on neural network programming. In this post, we will visualize a tensor flatten operation for a single grayscale image, and we'll show how we can flatten specific tensor axes, which is often required with CNNs because we work with batches of inputs opposed to single inputs.

![batches](https://deeplizard.com/images/png/cluster%20three%203%20groups.png)

Without further ado, let's get started.

### Flattening An Entire Tensor

A tensor [flatten operation](https://deeplizard.com/learn/video/fCVuiW9AFzY) is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input.

In past posts, we learned about a [tensor's shape](https://deeplizard.com/learn/video/AiyK0idr4uM) and then about [reshaping operations](https://deeplizard.com/learn/video/fCVuiW9AFzY). A flatten operation is a specific type of reshaping operation where by all of the axes are *smooshed* or *squashed* together.

To flatten a tensor, we need to have at least two axes. This makes it so that we are starting with something that is not already flat. Let's look now at a hand written image of an eight from the MNIST dataset. This image has `2` distinct dimensions, *height* and *width*.

![image and flattened output](https://deeplizard.com/images/CNN%20Flatten%20Operation%20Visualized.jpg)

The height and width are `18 x 18` respectively. These dimensions tell us that this is a cropped image because the MNIST dataset contains `28 x 28` images. Let's see now how these two axes of height and width are flattened out into a single axis of length `324`.

The image above shows our flattened output with a single axis of length `324`. The white on the edges corresponds to the white at the top and bottom of the image.

In this example, we are flattening the entire tensor image, but what if we want to only flatten specific axes within the tensor? This is typically required when working with CNNs.

Let's see how we can flatten out specific axes of a tensor in code with PyTorch.

### Flattening Specific Axes Of A Tensor

In the post on [CNN input tensor shape](https://deeplizard.com/learn/video/k6ZF1TSniYk), we learned how tensor inputs to a convolutional neural network typically have `4` axes, one for batch size, one for color channels, and one each for height and width.

(Batch Size, Channels, Height, Width)

Let's kick things off here by constructing a tensor to play around with that meets these specs. To start, suppose we have the following three tensors.

#### Building A Tensor Representation For A Batch Of Images

```
t1 = torch.tensor([
    [1,1,1,1],
    [1,1,1,1],
    [1,1,1,1],
    [1,1,1,1]
])

t2 = torch.tensor([
    [2,2,2,2],
    [2,2,2,2],
    [2,2,2,2],
    [2,2,2,2]
])

t3 = torch.tensor([
    [3,3,3,3],
    [3,3,3,3],
    [3,3,3,3],
    [3,3,3,3]
])
```

Each of these has a shape of `4 x 4`, so we have three rank-2 tensors. For our purposes here, we'll consider these to be three `4 x 4` images that well use to create a batch that can be passed to a CNN.

Remember, batches are represented using a single tensor, so we'll need to combine these three tensors into a single larger tensor that has three axes instead of `2`.

```
> t = torch.stack((t1, t2, t3))
> t.shape

torch.Size([3, 4, 4])
```

Here, we used the `stack()` method to concatenate our sequence of three tensors along a new axis. Since we have three tensors along a new axis, we know the length of this axis should be `3`, and indeed, we can see in the shape that we have `3` tensors that have height and width of `4`.

Want to know how the `stack()` method works? An explanation of the `stack()` method comes [later in the series](https://deeplizard.com/learn/video/kF2AlpykJGY).

The axis with a length of `3` represents the batch size while the axes of length `4` represent the height and width respectively. This is what the output for this this tensor representation of batch looks like.

```
> t
tensor([[[1, 1, 1, 1],
         [1, 1, 1, 1],
         [1, 1, 1, 1],
         [1, 1, 1, 1]],

        [[2, 2, 2, 2],
         [2, 2, 2, 2],
         [2, 2, 2, 2],
         [2, 2, 2, 2]],

        [[3, 3, 3, 3],
         [3, 3, 3, 3],
         [3, 3, 3, 3],
         [3, 3, 3, 3]]])
```

At this point, we have a rank-3 tensor that contains a batch of three `4 x 4` images. All we need to do now to get this tensor into a form that a CNN expects is add an axis for the color channels. We basically have an implicit single color channel for each of these image tensors, so in practice, these would be [grayscale](https://en.wikipedia.org/wiki/Grayscale) images.

A CNN will expect to see an explicit color channel axis, so let's add one by reshaping this tensor.

```
> t = t.reshape(3,1,4,4)
> t
tensor(
[
    [
        [
            [1, 1, 1, 1],
            [1, 1, 1, 1],
            [1, 1, 1, 1],
            [1, 1, 1, 1]
        ]
    ],
    [
        [
            [2, 2, 2, 2],
            [2, 2, 2, 2],
            [2, 2, 2, 2],
            [2, 2, 2, 2]
        ]
    ],
    [
        [
            [3, 3, 3, 3],
            [3, 3, 3, 3],
            [3, 3, 3, 3],
            [3, 3, 3, 3]
        ]
    ]
])
```

Notice how we have specified an axis of length `1` right after the batch size axis. Then, we follow with the height and width axes length `4`. Also, notice how the additional axis of length `1` doesn't change the number of elements in the tensor. This is because the product of the components values doesn't change when we multiply by one.

The first axis has `3` elements. Each element of the first axis represents an image. For each image, we have a single color channel on the channel axis. Each of these channels contain `4` arrays that contain `4` numbers or scalar components.

#### Flattening The Tensor Batch

Alright. Let's see how to flatten the images in this batch. Remember the whole batch is a single tensor that will be passed to the CNN, so we don't want to flatten the whole thing. We only want to flatten the image tensors within the batch tensor.

Let's flatten the whole thing first just to see what it will look like. Plus I want to do a shout out to everyone who provided alternative implementations of the `flatten()` function we created in the last post. Take a look.

```
> t.reshape(1,-1)[0] # Thank you Mick!
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])

> t.reshape(-1) # Thank you Aamir!
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])

> t.view(t.numel()) # Thank you Ulm!
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])

> t.flatten() # Thank you PyTorch!
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
```

At the bottom, you'll notice another way that comes built-in as method for tensor objects called, you guessed it, `flatten()`. This method produces the very same output as the other alternatives.

What I want you to notice about this output is that we have flattened the entire batch, and this smashes all the images together into a single axis. Remember the *ones* represent the pixels from the first image, the *twos* the second image, and the *threes* from the third.

This flattened batch won't work well inside our CNN because we need individual predictions for each image within our batch tensor, and now we have a flattened mess.

The solution here, is to flatten each image while still maintaining the batch axis. This means we want to flatten *only part of the tensor*. We want to flatten the, color channel axis with the height and width axes.

These axes need to be flattened: (C,H,W)

This can be done with PyTorch's built-in `flatten()` method.

#### Flattening Specific Axes Of A Tensor

Let's run the following code:

```
> t.flatten(start_dim=1).shape
torch.Size([3, 16])

> t.flatten(start_dim=1)
tensor(
[
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
    [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
]
)
```

Notice in the call how we specified the `start_dim` parameter. This tells the `flatten()` method which axis it should start the flatten operation. The one here is an index, so it's the second axis which is the color channel axis. We skip over the batch axis so to speak, leaving it intact.

Checking the shape, we can see that we have a rank-2 tensor with three single color channel images that have been flattened out into `16` pixels.

### Flattening An RGB Image

If we flatten an RGB image, what happens to the color ?

What happens to the Color Channels?

Each color channel will be flattened first. Then, the flattened channels will be lined up side by side on a single axis of the tensor. Let's look at an example in code.

We'll build an example RGB image tensor with a height of two and a width of two.

```
r = torch.ones(1,2,2)
g = torch.ones(1,2,2) + 1
b = torch.ones(1,2,2) + 2

img = torch.cat(
    (r,g,b)
    ,dim=0
)
```

This gives us the desired tensor. We can verify this by checking the shape like so:

```
> img.shape
torch.Size([3, 2, 2])
```

We have three color channels with a height and width of two. We can also inspect this tensor's data like so:

```
> img
tensor([
    [
        [1., 1.]
        ,[1., 1.]
    ]
    ,[
        [2., 2.]
        , [2., 2.]
    ],
    [
        [3., 3.]
        ,[3., 3.]
    ]
])
```

Now, we can see how this will look by flattening the image tensor.

```
> img.flatten(start_dim=0)
tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])
```

Note that the `start_dim` parameter here tells the `flatten()` method where to start flattening. In this case, we are flattening the whole image. However, we can also flatten only the channels like so:

```
> img.flatten(start_dim=1)
tensor([
    [1., 1., 1., 1.],
    [2., 2., 2., 2.],
    [3., 3., 3., 3.]
])
```

### Wrapping Up

We should now have a good understanding of flatten operations for tensors. We know how to flatten a whole tensor, and we know how to flatten specific tensor dimensions/axes. We will see this put to use when we build our CNN. Until then, i'll see you in the next one!

Element-wise operations are extremely common operations with tensors in neural network programming. Let's lead this discussion off with a definition of an element-wise operation.

An *element-wise* operation is an operation between two tensors that operates on corresponding elements within the respective tensors.

An *element-wise* operation operates on corresponding elements between tensors.

Two elements are said to be corresponding if the two elements occupy the same position within the tensor. The position is determined by the indexes used to locate each element.

Suppose we have the following two tensors:

```
> t1 = torch.tensor([
    [1,2],
    [3,4]
], dtype=torch.float32)

> t2 = torch.tensor([
    [9,8],
    [7,6]
], dtype=torch.float32)
```

Both of these tensors are rank-2 tensors with a shape of `2 x 2`.

This means that we have two axes that both have a length of two elements each. The elements of the first axis are arrays and the elements of the second axis are numbers.

```
# Example of the first axis
> print(t1[0])
tensor([1., 2.])

# Example of the second axis
> print(t1[0][0])
tensor(1.)
```

This is the kind of thing we are used to seeing in this series now. Alright let's build on this.

We know that two elements are said to be corresponding if the two elements occupy the same position within the tensor, and the position is determined by the indexes used to locate each element. Let's see an example of corresponding elements.

```
> t1[0][0]
tensor(1.)

> t2[0][0]
tensor(9.)
```

This allows us to see that the corresponding element for the `1` in `t1` is the `9` in `t2`.

The correspondence is defined by the indexes. This is important because it reveals an important feature of element-wise operations. We can deduce that tensors must have the same number of elements in order to perform an element-wise operation.

We'll go ahead and make this statement more restrictive. Two tensors must have the same shape in order to perform element-wise operations on them.

#### Addition Is An Element-Wise Operation

Let's look at our first element-wise operation, addition. Don't worry. It will get a more interesting.

```
> t1 + t2
tensor([[10., 10.],
        [10., 10.]])
```

This allow us to see that addition between tensors is an element-wise operation. Each pair of elements in corresponding locations are added together to produce a new tensor of the same shape.

So, addition is an element-wise operation, and in fact, all the arithmetic operations, add, subtract, multiply, and divide are element-wise operations.

### Arithmetic Operations Are Element-Wise Operations

An operation we commonly see with tensors are arithmetic operations using scalar values. There are two ways we can do this:

(1) Using these symbolic operations:

```
> print(t + 2)
tensor([[3., 4.],
        [5., 6.]])

> print(t - 2)
tensor([[-1.,  0.],
        [ 1.,  2.]])

> print(t * 2)
tensor([[2., 4.],
        [6., 8.]])

> print(t / 2)
tensor([[0.5000, 1.0000],
        [1.5000, 2.0000]])
```

or equivalently, (2) these built-in tensor object methods:

```
> print(t1.add(2))
tensor([[3., 4.],
        [5., 6.]])

> print(t1.sub(2))
tensor([[-1.,  0.],
        [ 1.,  2.]])

> print(t1.mul(2))
tensor([[2., 4.],
        [6., 8.]])

> print(t1.div(2))
tensor([[0.5000, 1.0000],
        [1.5000, 2.0000]])
```

Both of these options work the same. We can see that in both cases, the scalar value, `2`, is applied to each element with the corresponding arithmetic operation.

Something seems to be wrong here. These examples are breaking the rule we established that said element-wise operations operate on tensors of the same shape.

Scalar values are Rank-0 tensors, which means they have no shape, and our tensor `t1` is a rank-2 tensor of shape `2 x 2`.

So how does this fit in? Let's break it down.

The first solution that may come to mind is that the operation is simply using the single scalar value and operating on each element within the tensor.

This logic kind of works. However, it's a bit misleading, and it breaks down in more general situations where we're note using a scalar.

To think about these operations differently, we need to introduce the concept of *tensor broadcasting or broadcasting*.

#### Broadcasting Tensors

Broadcasting describes how tensors with different shapes are treated during element-wise operations.

Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors.

Let's think about the `t1 + 2` operation. Here, the scaler valued tensor is being broadcasted to the shape of `t1`, and then, the element-wise operation is carried out.

We can see what the broadcasted scalar value looks like using the `broadcast_to()` Numpy function:

```
> np.broadcast_to(2, t1.shape)
array([[2, 2],
        [2, 2]])
```

This means the scalar value is transformed into a rank-2 tensor just like `t1`, and just like that, the shapes match and the element-wise rule of having the same shape is back in play. This is all under the hood of course.

This piece of code here paints the picture so to speak. This

```
> t1 + 2
tensor([[3., 4.],
        [5., 6.]])
```

is really this:

```
> t1 + torch.tensor(
    np.broadcast_to(2, t1.shape)
    ,dtype=torch.float32
)
tensor([[3., 4.],
        [5., 6.]])
```

At this point you may be thinking that this seems convoluted, so let's look at a trickier example to hit this point home. Suppose we have the following two tensors.

#### Trickier Example Of Broadcasting

Let's look at a trickier example to hit this point home. Suppose we have the following tensor.

```
t1 = torch.tensor([
    [1,1],
    [1,1]
], dtype=torch.float32)

t2 = torch.tensor([2,4], dtype=torch.float32)
```

What will be the result of this element-wise addition operation? Is it even possible given the same shape rule for element-wise operations?

```
# t1 + t2 ???????

> t1.shape
torch.Size([2, 2])

> t2.shape
torch.Size([2])
```

Even though these two tenors have differing shapes, the element-wise operation is possible, and [broadcasting](https://deeplizard.com/learn/video/6_33ulFDuCg) is what makes the operation possible. The lower rank tensor `t2` will be transformed via [broadcasting](https://deeplizard.com/learn/video/6_33ulFDuCg) to match the shape of the higher rank tensor `t1`, and the element-wise operation will be performed as usual.

The concept of broadcasting is the key to understanding how this operation will be carried out. As before, we can check the broadcast transformation using the `broadcast_to()` numpy function.

```
> np.broadcast_to(t2.numpy(), t1.shape)
array([[2., 4.],
        [2., 4.]], dtype=float32)

> t1 + t2
tensor([[3., 5.],
        [3., 5.]])
```

After broadcasting, the addition operation between these two tensors is a regular element-wise operation between tensors of the same shape.

![mic](https://deeplizard.com/images/broadcasting%20microphone%20mic.jpg)

[Broadcasting](https://deeplizard.com/learn/video/6_33ulFDuCg) is a more advanced topic than the basic element-wise operations, so don't worry if it takes longer to get comfortable with the idea.

Understanding element-wise operations and the same shape requirement provide a basis for the concept of broadcasting and why it is used.

When do we actually use broadcasting? We often need to use broadcasting when we are preprocessing our data, and especially during normalization routines.

There is a post in the [TensorFlow.js](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xr83l8w44N_g3pygvajLrJ-) series that covers broadcasting in greater detail. There is a practical example, and the algorithm for determining how a particular tensor is broadcasted is also covered, so check that out for, [a deeper discussion on broadcasting](https://deeplizard.com/learn/video/6_33ulFDuCg).

Don't worry about not knowing [TensorFlow.js](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xr83l8w44N_g3pygvajLrJ-). It's not a requirement, and I highly recommend the content there on broadcasting.

### Comparison Operations Are Element-Wise

Comparison operations are also element-wise operations.

For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element containing either a `torch.bool` value of `True` or `False`.

#### Behavior Change In PyTorch Version 1.2.0

Comparison operations returned dtype has changed from `torch.uint8` to `torch.bool` ( [21113](https://github.com/pytorch/pytorch/pull/21113)).

*Version 1.1:*

```
> torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2])
tensor([1, 0, 0], dtype=torch.uint8)
```

*Version 1.2:*

```
> torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2])
tensor([True, False, False])
```

Relevant links:

- Release Notes: https://github.com/pytorch/pytorch/releases/tag/v1.2.0
- Pull Request: https://github.com/pytorch/pytorch/pull/21113

The examples below show output for PyTorch version 1.2.0 and greater.

#### Element-Wise Comparison Operation Examples

Suppose we have the following tensor:

```
> t = torch.tensor([
    [0,5,0],
    [6,0,7],
    [0,8,0]
], dtype=torch.float32)
```

Let's check out some of these comparison operations.

```
> t.eq(0)
tensor([[True, False, True],
        [False, True, False],
        [True, False, True]])

> t.ge(0)
tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])

> t.gt(0)
tensor([[False, True, False],
        [True, False, True],
        [False, True, False]])

> t.lt(0)
tensor([[False, False, False],
        [False, False, False],
        [False, False, False]])

> t.le(7)
tensor([[True, True, True],
        [True, True, True],
        [True, False, True]])
```

Thinking about these operations from a broadcasting perspective, we can see that the last one, `t.le(7)`, is really this:

```
> t <= torch.tensor(
    np.broadcast_to(7, t.shape)
    ,dtype=torch.float32
)

tensor([[True, True, True],
        [True, True, True],
        [True, False, True]])
```

and equivalently this:

```
> t <= torch.tensor([
    [7,7,7],
    [7,7,7],
    [7,7,7]
], dtype=torch.float32)

tensor([[True, True, True],
        [True, True, True],
        [True, False, True]])
```

### Element-Wise Operations Using Functions

With element-wise operations that are functions, it's fine to assume that the function is applied to each element of the tensor.

Here are some examples:

```
> t.abs() 
tensor([[0., 5., 0.],
        [6., 0., 7.],
        [0., 8., 0.]])

> t.sqrt()
tensor([[0.0000, 2.2361, 0.0000],
        [2.4495, 0.0000, 2.6458],
        [0.0000, 2.8284, 0.0000]])

> t.neg()
tensor([[-0., -5., -0.],
        [-6., -0., -7.],
        [-0., -8., -0.]])

> t.neg().abs()
tensor([[0., 5., 0.],
        [6., 0., 7.],
        [0., 8., 0.]])
```

#### Some Terminology

There are some other ways to refer to element-wise operations, so I just wanted to mention that all of these mean the same thing:

- Element-wise
- Component-wise
- Point-wise

Just keep this in mind if you encounter any of these terms in the wild.

### Wrapping Up

Now, we should have a good understanding of element-wise operations and how they are applied to tensor operations for neural networks and deep learning. In the [next post](https://deeplizard.com/learn/video/K3lX3Cltt4c), we will be covering the last two categories of tensor operations:

- Reshaping operations
- Element-wise operations
- **Reduction operations**
- **Access operations**

See you in the [next one](https://deeplizard.com/learn/video/K3lX3Cltt4c)!

### Tensor Reduction Ops For Deep Learning

Welcome back to this series on neural network programming. In this post, we'll learn about reduction operations for tensors.

![cyborg](https://deeplizard.com/images/artificial-intelligence-3382507_1920-cropped-eroded.jpg)

- Reshaping operations
- Element-wise operations
- **Reduction operations**
- **Access operations**

We'll focus in on the frequently used `argmax()` function, and we'll see how to access the data inside our tensors. Without further ado, let's get started.

### Tensor Reduction Operations

Let's kick things off by giving a definition for a reduction operation.

A *reduction operation* on a tensor is an operation that reduces the number of elements contained within the tensor.

So far in this series, we've learned that tensors are the data structures of deep learning. Our first task is to load our data elements into a tensor.

For this reason, tensors are super important, but ultimately, what we are doing with the operations we've been learning about in [this series](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG) is managing the data elements contained with our tensors.

Tensors give us the ability to manage our data.

[Reshaping](https://deeplizard.com/learn/video/fCVuiW9AFzY) operations gave us the ability to position our elements along particular axes. [Element-wise](https://deeplizard.com/learn/video/QscEWm0QTRY) operations allow us to perform operations on elements between *two tensors*, and reduction operations allow us to perform operations on elements within a *single tensor*.

Let's look at an example in code.

#### Reduction Operation Example

Suppose we the following `3 x 3` rank-2 tensor:

```
> t = torch.tensor([
    [0,1,0],
    [2,0,2],
    [0,3,0]
], dtype=torch.float32)
```

Let's look at our first reduction operation, a summation:

```
> t.sum()
tensor(8.)
```

Using the fact that

```
> t.numel()
9

> t.sum().numel()
1
```

We can see that

```
> t.sum().numel() < t.numel()
True
```

The sum of our tensor's scalar components is calculated using the `sum()` tensor method. The result of this call is a scalar valued tensor.

Checking the number of elements in the original tensor against the result of the `sum()` call, we can see that, indeed, the tensor returned by the call to `sum()` contains fewer elements than the original.

Since the number of elements have been reduced by the operation, we can conclude that the `sum()` method is a reduction operation.

#### Common Tensor Reduction Operations

As you may expect, here are some other common reduction functions:

```
> t.sum()
tensor(8.)

> t.prod()
tensor(0.)

> t.mean()
tensor(.8889)

> t.std()
tensor(1.1667)
```

All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the tensor's elements.

Reduction operations in general allow us to compute aggregate (total) values across data structures. In our case, our structures are tensors.

Here is a question though:

Do reduction operations always reduce to a tensor with a single element?

The answer is *no!*

In fact, we often reduce specific axes at a time. This process is important. It's just like we saw with reshaping when we aimed to flatten the image tensors within a batch while still maintaining the batch axis.

#### Reducing Tensors By Axes

To reduce a tensor with respect to a specific axis, we use the same methods, and we just pass a value for the dimension parameter. Let's see this in action.

Suppose we have the following tensor:

```
> t = torch.tensor([
    [1,1,1,1],
    [2,2,2,2],
    [3,3,3,3]
], dtype=torch.float32)
```

This is a `3 x 4` rank-2 tensor. Having different lengths for the two axes will help us understand these reduce operations.

Let's consider the `sum()` method again. Only, this time, we will specify a dimension to reduce. We have two axes so we'll do both. Check it out.

```
> t.sum(dim=0)
tensor([6., 6., 6., 6.])

> t.sum(dim=1)
tensor([ 4.,  8., 12.])
```

When I first saw this when I was learning how this works, I was confused. If you're confused like I was, I highly recommend you try to understand what's happening here before going forward.

Remember, we are reducing this tensor across the first axis, and elements running along the first axis are arrays, and the elements running along the second axis are numbers.

Let's go over what happened here.

#### Understanding Reductions By Axes

We'll tackle the first axis first. When take the summation of the first axis, we are summing the elements of the first axis.

It's like this:

```
> t[0]
tensor([1., 1., 1., 1.])

> t[1]
tensor([2., 2., 2., 2.])

> t[2]
tensor([3., 3., 3., 3.])

> t[0] + t[1] + t[2]
tensor([6., 6., 6., 6.])
```

Surprise! Element-wise operations are in play here.

When we sum across the first axis, we are taking the summation of all the elements of the first axis. To do this, we must utilize element-wise addition. This is why we covered element-wise operations before reduction operations in the series.

The second axis in this tensor contains numbers that come in groups of four. Since we have three groups of four numbers, we get three sums.

```
> t[0].sum()
tensor(4.)

> t[1].sum()
tensor(8.)

> t[2].sum()
tensor(12.)

> t.sum(dim=1)
tensor([ 4.,  8., 12.])
```

This may take a little bit of time to sink in. If it does, don't worry, you can do it.

Now, with this heavy lifting out of the way. Let's look now a very common reduction operation used in neural network programming called *Argmax*.

#### Argmax Tensor Reduction Operation

Argmax is a mathematical function that tells us which argument, when supplied to a function as input, results in the function's max output value.

*Argmax* returns the index location of the maximum value inside a tensor.

When we call the `argmax()` method on a tensor, the tensor is reduced to a new tensor that contains an index value indicating where the max value is inside the tensor. Let's see this in code.

Suppose we have the following tensor:

```
t = torch.tensor([
    [1,0,0,2],
    [0,3,3,0],
    [4,0,0,5]
], dtype=torch.float32)
```

In this tensor, we can see that the max value is the 5 in the last position of the last array.

Suppose we are tensor walkers. To arrive at this element, we walk down the first axis until we reach the last array element, and then we walk down to the end of this array passing by the 4, and the two 0s.

Let's see some code.

```
> t.max()
tensor(5.)

> t.argmax()
tensor(11)

> t.flatten()
tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])
```

The first piece of code confirms for us that the max is indeed `5`, but the call to the `argmax()` method tells us that the `5` is sitting at index `11`. What's happening here?

We'll have a look at the flattened output for this tensor. If we don't specific an axis to the `argmax()` method, it returns the index location of the max value from the flattened tensor, which in this case is indeed `11`.

Let's see how we can work with specific axes now.

```
> t.max(dim=0)
(tensor([4., 3., 3., 5.]), tensor([2, 1, 1, 2]))

> t.argmax(dim=0)
tensor([2, 1, 1, 2])

> t.max(dim=1)
(tensor([2., 3., 5.]), tensor([3, 1, 3]))

> t.argmax(dim=1)
tensor([3, 1, 3])
```

We're working with both axes of this tensor in this code. Notice how the call to the `max()` method returns two tensors. The first tensor contains the max values and the second tensor contains the index locations for the max values. This is what argmax gives us.

For the first axis, the max values are, `4`, `3`, `3`, and `5`. These values are determined by taking the element-wise maximum across each array running across the first axis.

For each of these maximum values, the `argmax()` method tells us which element along the first axis where the value lives.

- The `4` lives at index two of the first axis.
- The first `3` lives at index one of the first axis.
- The second `3` lives at index one of the first axis.
- The `5` lives at index two of the first axis.

For the second axis, the max values are `2`, `3`, and `5`. These values are determined by taking the maximum inside each array of the first axis. We have three groups of four, which gives us `3` maximum values.

The argmax values here, tell the index inside each respective array where the max value lives.

In practice, we often use the `argmax()` function on a network's output prediction tensor, to determine which category has the highest prediction value.

### Accessing Elements Inside Tensors

The last type of common operation that we need for tensors is the ability to access data from within the tensor. Let's look at these for PyTorch.

Suppose we have the following tensor:

```
> t = torch.tensor([
    [1,2,3],
    [4,5,6],
    [7,8,9]
], dtype=torch.float32)

> t.mean()
tensor(5.)

> t.mean().item()
5.0
```

Check out these operations on this one. When we call mean on this `3 x 3` tensor, the reduced output is a scalar valued tensor. If we want to actually get the value as a number, we use the `item()` tensor method. This works for scalar valued tensors.

Have a look at how we do it with multiple values:

```
> t.mean(dim=0).tolist()
[4.0, 5.0, 6.0]

> t.mean(dim=0).numpy()
array([4., 5., 6.], dtype=float32)
```

When we compute the mean across the first axis, multiple values are returned, and we can access the numeric values by transforming the output tensor into a Python `list` or a NumPy `array`.

#### Advanced Indexing And Slicing

With NumPy `ndarray` objects, we have a pretty robust set of operations for indexing and slicing, and PyTorch `tensor` objects support most of these operations as well.

Use this a resource for [advanced indexing and slicing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html).

### Deep Learning Project

Congrats for making it this far in the series. All of these tensor topics are pretty raw and low level, but having a strong understanding of them make our lives much easier as we develop as neural network programmers.

We're ready now to start part two of the series where we'll be putting all of this knowledge to use. We'll be kicking things off by exploring the dataset we'll be training on, the Fashion-MNIST dataset.

![fashion mnist](https://deeplizard.com/images/fashion-mnist.jpg)

This dataset contains a training set of sixty thousand examples from ten different classes of clothing. We will use PyTorch to build a convolutional neural network that can accurately predict the correct article of clothing given an input piece, so stay tuned!

I'll see you in the [next one](https://deeplizard.com/learn/video/EqpzfvxBx30)!

### Introducing Fashion-MNIST For Machine Learning

Welcome back to this series on neural network programming. In this post, we will introduce the Fashion-MNIST dataset.

![cyborg ai](https://deeplizard.com/images/ai-cyborg-cropped-2.jpg)

We'll look at the dataset spec, how the dataset was built, and how the dataset differs from the original MNIST dataset of handwritten digits. Without further ado, let's get started.

#### Why Study A Dataset?

Let's kick things off by pondering the question of why we should take the time to study a dataset. Data is the primary ingredient of deep learning, and although it's our task as neural network programmers to let our neural networks learn from our data, we still have the responsibility of knowing the nature and history of the data we are using to actually do the training.

[Computer programs](https://deeplizard.com/learn/video/x2H9cy-rrPg) in general consist of two primary components, code and [data](https://deeplizard.com/learn/video/DoJiau6mPFc). With traditional programming, the programmer's job is to directly write the software or code, but with deep learning and neural networks, the software so to speak is the network itself and in particular, the network's weights that emerge automatically during the training process.

![weights](https://deeplizard.com/images/weights.jpg)

It's the programmer's job to oversee and guide the learning process though training. We can think of this as an indirect way of writing software or code. By using data and deep learning, neural network programmers can produce software capable of performing computations without writing code to explicitly carry out these computations.

For this reason, the role of data in developing software is shifting, and we'll likely see the role of software developers shift as well.

Data focused considerations:

- Who created the dataset?
- How was the dataset created?
- What transformations were used?
- What intent does the dataset have?
- Possible unintentional consequences?
- Is the dataset biased?
- Are there ethical issues with the dataset?

In practice, acquiring and accessing data is often of the hardest parts of deep learning, so keep this in mind as we go though this particular dataset. Take note of the general concepts and ideas that we see here.

### What Is The MNIST Dataset?

The [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), *Modified National Institute of Standards and Technology database*, is a famous dataset of handwritten digits that is commonly used for training image processing systems for machine learning. NIST stands for *National Institute of Standards and Technology*.

The *M* in MNIST stands for *modified*, and this is because there was an original *NIST* dataset of digits that was modified to give us *MNIST*.

![mnist dataset samples](https://deeplizard.com/images/mnist%20dataset.jpg)

MNIST is famous because of how often the dataset is used. It's common for two reasons:

1. Beginners use it because it's easy
2. Researchers use it to benchmark (compare) different models.

The dataset consists of `70,000` images of hand written digits with the following split:

- `60,000` training images
- `10,000` testing images

The images were originally created by American Census Bureau employees and American high school students.

MNIST has been so widely used, and image recognition tech has improved so much that the dataset is considered to be too easy. This is why the Fashion-MNIST dataset was created.

### What Is Fashion-MNIST?

*Fashion-MNIST* as the name suggests is a dataset of fashion items. Specifically, the dataset has the following ten classes of fashion items:

| Index | Label       |
| ----- | ----------- |
| 0     | T-shirt/top |
| 1     | Trouser     |
| 2     | Pullover    |
| 3     | Dress       |
| 4     | Coat        |
| 5     | Sandal      |
| 6     | Shirt       |
| 7     | Sneaker     |
| 8     | Bag         |
| 9     | Ankle boot  |

As we have seen in a previous post, a sample of the items look like this:

![fashion mnist examples](https://deeplizard.com/images/fashion-mnist.jpg)

#### What's The Origin Of Fashion-MNIST

Where did the Fashion-MNIST images come from? Fashion-MNIST is based on the assortment on Zalando's website. [Zalando](https://en.wikipedia.org/wiki/Zalando) is a German based multi-national fashion commerce company that was founded in 2008.

This is why we see *zalandoresearch* in the [GitHub URL](https://github.com/zalandoresearch/fashion-mnist) where the Fashion-MNIST dataset is available for download.



Zalando Research is the group from within the company that created the dataset.

We'll see more about how the images were collected when we review the paper that introduced the dataset, but first, let's answer another lurking question.

What's *MNIST* about a fashion dataset?

#### What Puts The MNIST In Fashion-MNIST?

The reason the fashion MNIST dataset has MNIST in it's name is because the creators seek to replace the MNIST with Fashion-MNIST.

For this reason, the Fashion dataset was designed to mirror the original MNIST dataset as closely as possible while introducing higher difficulty in training due to simply having more complex data than hand written images.

We'll see the specific ways that Fashion-MNIST mirrors the original dataset in the paper, but one thing we have already seen is the number of classes.

- MNIST  has 10 classes (one for each digit 0-9)
- Fashion-MNIST  has 10 classes (this is intentional)

Let's check out the paper.

### Reading The Fashion-MNIST Paper On ArXiv

The paper can be found [here](https://arxiv.org/abs/1708.07747), and to see the paper, just click on the [PDF link](https://arxiv.org/pdf/1708.07747.pdf).

![logo-ArXiv](https://deeplizard.com/images/logo-ArXiv.svg)

The first thing to notice about the paper is that the authors are from Zalando Research (the origin of Fashion-MNIST).

After reading the paper's abstract, we see why the dataset has been named Fashion-MNIST.

#### The Fashion-MNIST Paper's Abstract

"We present Fashion-MNIST, a new dataset comprising of 28 by 28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct dropin replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist."

\- [arXiv paper](https://arxiv.org/abs/1708.07747)

The dataset was designed to be a drop-in replacement for the original MNIST. By making the Fashion-MNIST dataset specs match the original MNIST specs, the switch over from old to new can be smooth. The paper claims that the only change needed to switch datasets is to change the URL from where the MNIST dataset is fetched by pointing to the Fashion dataset.

The paper also give us some more insight as to why MNIST is so popular:

"The reason MNIST is so popular has to do with its size, allowing deep learning researchers to quickly check and prototype their algorithms. This is also complemented by the fact that all machine learning libraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, PyTorch) provide helper functions and convenient examples that use MNIST out of the box."

\- [arXiv paper](https://arxiv.org/abs/1708.07747)

PyTorch does provide us with a package called `torchvision` that makes it easy for us to get started with MNIST as well as Fashion-MNIST.

We'll be using `torchvision` in our next post to load our training set into our project.

#### How Fashion-MNIST Was Built

Unlike the MNIST dataset, the fashion set wasn't hand-drawn, but the images in the dataset are actual images from Zalando's website.

However, they have been transformed to more closely correspond to the MNIST specifications. This is the general conversion process that each image from the site went through:

1. Converted to PNG
2. Trimmed
3. Resized
4. Sharpened
5. Extended
6. Negated
7. Gray-scaled

To see a more detailed description of this process, be sure to check out section two of [the paper](https://arxiv.org/abs/1708.07747).

### Accessing Fashion-MNIST With `torchvision`

In summary, we have seen the origin and history of the Fashion-MNIST dataset, and although the dataset is designed to be more challenging as a computer vision problem, the set is still a great place to start.

![pytorch logo](https://deeplizard.com/images/pytorch-logo-flame.svg)

We will be accessing Fashion-MNIST though a PyTorch vision library called `torchvision` and building our first neural network that can accurately predict an output class given an input fashion image.

I'll see you in the [next one](https://deeplizard.com/learn/video/8n-TGaBZnk4).

### Extract, Transform, And Load (ETL) With PyTorch

Welcome back to this series on neural network programming with PyTorch. In this post, we will write our first code of part two of the series.

We'll demonstrate a very simple extract, transform and load pipeline using torchvision, PyTorch's computer vision package for machine learning. Without further ado, let's get started.

![ai cyborg](https://deeplizard.com/images/ai-cyborg-cropped-2.jpg)

#### The Project (Bird's-Eye View)

There are four general steps that we'll be following as we move through this project:

1. **Prepare the data**
2. Build the model
3. Train the model
4. Analyze the model's results

#### The ETL Process

In this post, we'll kick things off by preparing the data. To prepare our data, we'll be following what is loosely known as an [ETL process](https://en.wikipedia.org/wiki/Extract,_transform,_load).

- Extract data from a data source.
- Transform data into a desirable format.
- Load data into a suitable structure.

The ETL process can be thought of as a fractal process because it can be applied on various scales. The process can be applied on a small scale, like a single program, or on a large scale, all the way up to the enterprise level where there are huge systems handling each of the individual parts.

![servers](https://deeplizard.com/images/server-2160321_1920.jpg)

If you want to know more about the general data science pipeline, check out the [data science post](https://deeplizard.com/learn/video/d11chG7Z-xk), where we cover this in greater detail.

Once we have completed the ETL process, we are ready to begin building and training our deep learning model. PyTorch has some built-in packages and classes that make the ETL process pretty easy.

#### PyTorch Imports

We begin by importing all of the necessary PyTorch libraries.

```
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import torchvision
import torchvision.transforms as transforms
```

This table describes the of each of these packages:

| Package                | Description                                                  |
| ---------------------- | ------------------------------------------------------------ |
| torch                  | The top-level PyTorch package and tensor library.            |
| torch.nn               | A subpackage that contains modules and extensible classes for building neural networks. |
| torch.optim            | A subpackage that contains standard optimization operations like SGD and Adam. |
| torch.nn.functional    | A functional interface that contains typical operations used for building neural networks like loss functions and convolutions. |
| torchvision            | A package that provides access to popular datasets, model architectures, and image transformations for computer vision. |
| torchvision.transforms | An interface that contains common transforms for image processing. |

#### Other Imports

The next imports are standard packages used for data science in Python:

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix
#from plotcm import plot_confusion_matrix

import pdb

torch.set_printoptions(linewidth=120)
```

Note that `pdb` is the Python debugger and the commented `import` is a local file that we'll introduce in future posts for plotting the confusion matrix, and the last line sets the print options for PyTorch print statements.

We are ready now to prepare our data.

### Preparing Our Data Using PyTorch

Our ultimate goal when preparing our data is to do the following (ETL):

1. Extract  Get the [Fashion-MNIST](https://deeplizard.com/learn/video/EqpzfvxBx30) image data from the source.
2. Transform  Put our data into [tensor](https://deeplizard.com/learn/video/Csa5R12jYRg) form.
3. Load  Put our data into an object to make it easily accessible.

For these purposes, PyTorch provides us with two classes:

| Class                       | Description                                                 |
| --------------------------- | ----------------------------------------------------------- |
| torch.utils.data.Dataset    | An abstract class for representing a dataset.               |
| torch.utils.data.DataLoader | Wraps a dataset and provides access to the underlying data. |

An [abstract class](https://docs.python.org/library/abc.html) is a Python class that has methods we must implement, so we can create a custom dataset by creating a subclass that extends the functionality of the Dataset class.

To create a *custom* dataset using PyTorch, we extend the `Dataset` class by creating a subclass that implements these required methods. Upon doing this, our new subclass can then be passed to the a PyTorch `DataLoader` object.

We will be using the fashion-MNIST dataset that comes built-in with the `torchvision` package, so we won't have to do this for our project. Just know that the Fashion-MNIST built-in dataset class is doing this behind the scenes.

All subclasses of the Dataset class must override `__len__`, that provides the size of the dataset, and `__getitem__`, supporting integer indexing in range from `0` to `len(self)` exclusive.

Specifically, there are two methods that are required to be implemented. The `__len__` method which returns the length of the dataset, and the `__getitem__` method that gets an element from the dataset at a specific index location within the dataset.

### PyTorch Torchvision Package

The `torchvision` package, gives us access to the following resources:

- Datasets (like MNIST and Fashion-MNIST)
- Models (like VGG16)
- Transforms
- Utils

#### Computer Vision

All of these resources are related to deep learning computer vision tasks.

![vision](https://deeplizard.com/images/vision%20eye%20look.jpg)

When we learned about the Fashion-MNIST dataset in our [previous post](https://deeplizard.com/learn/video/EqpzfvxBx30), the arXiv paper that introduced the fashion dataset indicated that the authors wanted it to be a drop-in for the original MNIST dataset.

The idea was to make is so that frameworks like PyTorch could add Fashion-MNIST by just changing the URL for retrieving the data.

This is the case for PyTorch. The PyTorch `FashionMNIST` dataset simply extends the `MNIST` dataset and overrides the urls.

Here is the class definition from PyTorch's `torchvision` source code:

```
class FashionMNIST(MNIST):
    """`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.

    Args:
        root (string): Root directory of dataset where ``processed/training.pt``
            and  ``processed/test.pt`` exist.
        train (bool, optional): If True, creates dataset from ``training.pt``,
            otherwise from ``test.pt``.
        download (bool, optional): If true, downloads the dataset from the internet and
            puts it in root directory. If dataset is already downloaded, it is not
            downloaded again.
        transform (callable, optional): A function/transform that  takes in an PIL image
            and returns a transformed version. E.g, ``transforms.RandomCrop``
        target_transform (callable, optional): A function/transform that takes in the
            target and transforms it.
    """
    urls = [
        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',
        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',
        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',
        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz',
    ]
```

Let's see now how we can take advantage of `torchvision`.

#### PyTorch Dataset Class

To get an instance of the FashionMNIST dataset using `torchvision`, we just create one like so:

```
train_set = torchvision.datasets.FashionMNIST(
    root='./data'
    ,train=True
    ,download=True
    ,transform=transforms.Compose([
        transforms.ToTensor()
    ])
)
```

Note that the *root* argument used to be `'./data/FashionMNIST'`, however, it has since changed due to `torchvision` updates.

We specify the following arguments:

| Parameter | Description                                                  |
| --------- | ------------------------------------------------------------ |
| root      | The location on disk where the data is located.              |
| train     | If the dataset is the training set                           |
| download  | If the data should be downloaded.                            |
| transform | A composition of transformations that should be performed on the dataset elements. |

Since we want our images to be transformed into tensors, we use the built-in `transforms.ToTensor()` transformation, and since this dataset is going to be used for training, we'll name the instance `train_set`.

When we run this code for the first time, the Fashion-MNIST dataset will be downloaded locally. Subsequent calls check for the data before downloading it. Thus, we don't have to worry about double downloads or repeated network calls.

#### PyTorch DataLoader Class

To create a DataLoader wrapper for our training set, we do it like this:

```
train_loader = torch.utils.data.DataLoader(train_set
    ,batch_size=1000
    ,shuffle=True
)
```

We just pass train_set as an argument. Now, we can leverage the loader for tasks that would otherwise be pretty complicated to implement by hand:

- `batch_size` (1000 in our case)
- `shuffle` (True in our case)
- `num_workers` (Default is 0 which means the main process will be used)

### ETL Summary

From an ETL perspective, we have achieved the extract, and the transform using `torchvision` when we created the dataset:

1. Extract  The raw data was extracted from the web.
2. Transform  The raw image data was transformed into a tensor.
3. Load  The `train_set` wrapped by (loaded into) the data loader giving us access to the underlying data.

Now, we should have a good understanding of the `torchvision` module that is provided by PyTorch, and how we can use Datasets and DataLoaders in the PyTorch `torch.utils.data` package to streamline ETL tasks.

In the next post, we'll see how we can work with datasets and data loaders to access and view individual samples as well as batches of samples.

I'll see you in the next one!

### PyTorch Datasets And DataLoaders For Deep Learning

Welcome back to this series on neural network programming with PyTorch. In this post, we see how to work with the `Dataset` and `DataLoader` PyTorch classes.

![ai cyborg](https://deeplizard.com/images/ai-cyborg-cropped-2.jpg)

Our goal in this post is to get comfortable using the dataset and data loader objects as well as to get a feel for our training set. Without further ado, let's get started.

From a high-level perspective, we are still in the *preparing data* stage of our deep learning project.

- **Prepare the data**
- Build the model
- Train the model
- Analyze the model's results

In this post, we are going to see how we can work with the dataset and the data loader objects that we created in the previous post. Remember from the [previous post](https://deeplizard.com/learn/video/8n-TGaBZnk4), that we have two PyTorch objects, a `Dataset` and a `DataLoader`.

- `train_set`
- `train_loader`

We are now ready to see how to work with these objects, so let's get to it.

### PyTorch Dataset: Working With The Training Set

Let's begin by looking at some operations we can perform to better understand our data.

#### Exploring The Data

To see how many images are in our training set, we can check the length of the dataset using the Python `len()` function:

```
> len(train_set)
60000
```

This `60000` number makes sense based on what we learned in the post on the [Fashion-MNIST dataset](https://deeplizard.com/learn/video/EqpzfvxBx30). Suppose we want to see the labels for each image. This can be done like so:

```
> train_set.targets
tensor([9, 0, 0, ..., 3, 0, 5])
```

The first image is a `9` and the next two are zeros. Remember from [posts past](https://deeplizard.com/learn/video/EqpzfvxBx30), these values encode the actual class name or label. The `9` for example is an *ankle boot* while the `0` is a *t-shirt*.

![shirt](https://deeplizard.com/images/fashion%20mnist%20t-shirt.jpg)

If we want to see how many of each label exists in the dataset, we can use the PyTorch `bincount()` function like so:

```
> train_set.targets.bincount()
tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])
```

#### Class Imbalance: Balanced And Unbalanced Datasets

This shows us that the Fashion-MNIST dataset is uniform with respect to the number of samples in each class. This means we have `6000` samples for each class. As a result, this dataset is said to be balanced. If the classes had a varying number of samples, we would call the set an unbalanced dataset.

*Class imbalance* is a common problem, but in our case, we have just seen that the Fashion-MNIST dataset is indeed balanced, so we need not worry about that for our project.

To read more about the ways to mitigate unbalanced datasets in deep learning, see this paper: [A systematic study of the class imbalance problem in convolutional neural networks.](https://arxiv.org/abs/1710.05381)

#### Accessing Data In The Training Set

To access an individual element from the training set, we first pass the `train_set` object to Python's `iter()` built-in function, which returns an object representing a stream of data.

With the stream of data, we can use Python built-in `next()` function to get the next data element in the stream of data. From this we are expecting to get a single sample, so we'll name the result accordingly:

```
> sample = next(iter(train_set))
> len(sample)
2
```

After passing the sample to the `len()` function, we can see that the sample contains two items, and this is because the dataset contains image-label pairs. Each sample we retrieve from the training set contains the image data as a tensor and the corresponding label as a tensor.

Since the sample is a [sequence type](https://docs.python.org/3/library/stdtypes.html#typesseq), we can use *sequence unpacking* to assigned the image and the label. We will now check the type of the image and the label and see they are both `torch.Tensor` objects:

```
> image, label = sample

> type(image)
torch.Tensor

> type(label)
int
```

We'll check the shape to see that the image is a `1 x 28 x 28` tensor while the label is a scalar valued tensor:

```
> image.shape
torch.Size([1, 28, 28]) 

> torch.tensor(label).shape
torch.Size([])
```

We'll also call the `squeeze()` function on the image to see how we can remove the dimension of size `1`. This is [review](https://deeplizard.com/learn/video/fCVuiW9AFzY) of course.

```
> image.squeeze().shape
torch.Size([28, 28])
```

Again, based on our previous discussion of the [Fashion-MNIST dataset](https://deeplizard.com/learn/video/EqpzfvxBx30), we do expect to see the `28 x 28` shape for our image. The reason we see a `1` on the first dimension of the tensor is because the number of channels needs to be represented. Opposed to RGB images that have `3` color channels, grayscale images have a single color channel. This is why we have a `1 x 28 x 28` tensor. We have `1` color channel that has a size of `28 x 28`.

Let's plot the image now, and we'll see why we squeezed the tensor in the first place. We first squeeze the tensor and then pass it to the `imshow()` function.

```
> plt.imshow(image.squeeze(), cmap="gray")
> torch.tensor(label)
tensor(9)
```

![boot](https://deeplizard.com/images/fashion%20mnist%20ankle%20boot.jpg)

We get back an ankle-boot and the label of `9`. We know that the label `9` represents an ankle boot because it was specified in the paper that we looked at in the [previous post ](https://deeplizard.com/learn/video/EqpzfvxBx30).

Alright. Let's see how to work with the data loader now.

### PyTorch DataLoader: Working With Batches Of Data

We'll start by creating a new data loader with a smaller batch size of `10` so it's easy to demonstrate what's going on:

```
> display_loader = torch.utils.data.DataLoader(
    train_set, batch_size=10
)
```

We get a batch from the loader in the same way that we saw with the training set. We use the `iter()` and `next()` functions.

There is one thing to notice when working with the data loader. If `shuffle=True`, then the batch will be different each time a call to next occurs. With `shuffle=True`, the first samples in the training set will be returned on the first call to next. The *shuffle* functionality is turned off by default.

```
# note that each batch will be different when shuffle=True
> batch = next(iter(display_loader))
> print('len:', len(batch))
len: 2
```

Checking the length of the returned batch, we get `2` just like we did with the training set. Let's unpack the batch and take a look at the two tensors and their shapes:

```
> images, labels = batch

> print('types:', type(images), type(labels))
> print('shapes:', images.shape, labels.shape)
types: <class 'torch.Tensor'> <class 'torch.Tensor'>
shapes: torch.Size([10, 1, 28, 28]) torch.Size([10])
```

Since `batch_size=10`, we know we are dealing with a batch of `10` images and `10` corresponding labels. This is why we went plural on the variable names.

The types are tensors like we'd expect. However, the shapes are different from what we saw with the single sample. Instead of having a single scalar value as our label, we have a rank-1 tensor with `10` values. The size of each dimension in the tensor that contains the image data is defined by each of the following values:

(batch size, number of color channels, image height, image width)

The batch size of `10`, is why we now have a `10` in the leading dimension of the tensor, one index for each image. The following gives us the first ankle-boot we saw before:

```
> images[0].shape
torch.Size([1, 28, 28])

> labels[0]
9
```

To plot a batch of images, we can use the `torchvision.utils.make_grid()` function to create a grid that can be plotted like so:

```
> grid = torchvision.utils.make_grid(images, nrow=10)

> plt.figure(figsize=(15,15))
> plt.imshow(np.transpose(grid, (1,2,0)))

> print('labels:', labels)
labels: tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])
```

![img](https://deeplizard.com/images/fashion%20mnist%20grid%20sample.jpg)

Thanks to Amit Chaudhary for pointing out that the `permute()` PyTorch tensor method can be used in place of `np.transpose().` It's like this:

```
> grid = torchvision.utils.make_grid(images, nrow=10)

> plt.figure(figsize=(15,15))
> plt.imshow(grid.permute(1,2,0))

> print('labels:', labels)
labels: tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])
```

![img](https://deeplizard.com/images/fashion%20mnist%20grid%20sample.jpg)

Recall that we have the following table that shows the label mapping to class names below:

| Index | Label       |
| ----- | ----------- |
| 0     | T-shirt/top |
| 1     | Trouser     |
| 2     | Pullover    |
| 3     | Dress       |
| 4     | Coat        |
| 5     | Sandal      |
| 6     | Shirt       |
| 7     | Sneaker     |
| 8     | Bag         |
| 9     | Ankle boot  |

### How To Plot Images Using PyTorch DataLoader

Here is another was to plot the images using the PyTorch DataLoader. This method was inspired by Barry Mitchell. Enjoy!

```
how_many_to_plot = 20

train_loader = torch.utils.data.DataLoader(
    train_set, batch_size=1, shuffle=True
)

plt.figure(figsize=(50,50))
for i, batch in enumerate(train_loader, start=1):
    image, label = batch
    plt.subplot(10,10,i)
    plt.imshow(image.reshape(28,28), cmap='gray')
    plt.axis('off')
    plt.title(train_set.classes[label.item()], fontsize=28)
    if (i >= how_many_to_plot): break
plt.show()
```

![Plot of images from the fashion mnist dataset](https://deeplizard.com/images/fashion%20mnist%20grid%20sample%202.jpg)

### Building The Model Is Next

We should now have a good understanding of how to explore and interact with `Dataset`s and `DataLoader`s. Both of these will prove to be important as we begin building our convolutional neural network and our training loop. In fact, the data loader will be used directly inside our training loop.

![neural network](https://deeplizard.com/images/png/neural%20network%203%20layers.png)

Let's keep moving as we are ready to build our model in the next post. I'll see you there!

### Building Neural Networks With PyTorch

Welcome back to this series on neural network programming with PyTorch. In this post, we will begin building our first [convolutional neural network](https://deeplizard.com/learn/video/YRhxdVk_sIs) (CNN) using PyTorch. Without further ado, let's get started.

![ai cyborg](https://deeplizard.com/images/ai-cyborg-cropped-2.jpg)

#### Bird's Eye View Of The Process

From a high-level perspective or bird's eye view of our deep learning project, we prepared our data, and now, we are ready to build our model.

- Prepare the data
- **Build the model**
- Train the model
- Analyze the model's results

When say *model*, we mean our *network*. The words *model* and *network* mean the same thing. What we want our network to ultimately do is model or approximate a function that maps image inputs to the correct output class.

#### Prerequisites

To build neural networks in PyTorch, we extend the `torch.nn.Module` PyTorch class. This means we need to utilize a little bit of [object oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming) (OOP) in Python.

We'll do a quick OOP review in this post to cover the details needed for working with PyTorch neural networks, but if you find that you need more, the Python docs have an overview tutorial [here](https://docs.python.org/3/tutorial/classes.html).

To build a convolutional neural network, we need to have a general understanding of how CNNs work and what components are used to build CNNs. This [deep learning fundamentals series](https://deeplizard.com/learn/video/gZmobeGL0Yg) is a good prerequisite for this series, so I highly recommend you cover that one if you haven't already. If you just want a crash course on CNNs, these are the specific posts to see:

- [Convolutional Neural Networks (CNNs) explained](https://deeplizard.com/learn/video/YRhxdVk_sIs)
- [Visualizing Convolutional Filters from a CNN](https://deeplizard.com/learn/video/cNBBNAxC8l4)
- [Zero Padding in Convolutional Neural Networks explained](https://deeplizard.com/learn/video/qSTv_m-KFk0)
- [Max Pooling in Convolutional Neural Networks explained](https://deeplizard.com/learn/video/ZjM_XQa5s6s)
- [Learnable Parameters in a Convolutional Neural Network (CNN) explained](https://deeplizard.com/learn/video/gmBfb6LNnZs)

Let's jump in now with a quick object oriented programming review.

#### Quick Object Oriented Programming Review

When we're writing programs or building software, there are two key components, *code and data*. With object oriented programming, we orient our program design and structure around objects.

*Objects* are defined in code using classes. A *class* defines the object's specification or spec, which specifies what data and code each object of the class should have.

When we create an object of a class, we call the object an *instance of the class*, and all instances of a given class have two core components:

- *Methods* (code)
- *Attributes* (data)

The methods represent the code, while the attributes represent the data, and so the methods and attributes are defined by the class.

In a given program, many objects, a.k.a instances of a given class, can exist simultaneously, and all of the instances will have the same available attributes and the same available methods. They are uniform from this perspective.

The difference between objects of the same class is the values contained within the object for each attribute. Each object has its own attribute values. These values determine the internal state of the object. The code and data of each object is said to be [encapsulated](https://en.wikipedia.org/wiki/Encapsulation_(computer_programming)) within the object.

Let's build a simple lizard class to demonstrate how classes encapsulate data and code:

```
class Lizard: #class declaration
    def __init__(self, name): #class constructor (code)
        self.name = name #attribute (data)

    def set_name(self, name): #method declaration (code)
        self.name = name #method implementation (code)
```

The first line declares the class and specifies the class name, which in this case is `Lizard`.

The second line defines a special method called the class constructor. Class constructors are called when a new instance of the class is created. As parameters, we have `self` and `name`.

The `self` parameter gives us the ability to create attribute values that are stored or encapsulated within the object. When we call this constructor or any of the other methods, we don't pass the `self` parameter. Python does this for us automatically.

Argument values for any other parameter are arbitrarily passed by the caller, and these passed values that come in to the method can be used in a calculation or saved and accessed later using `self`.

After we're done with the constructor, we can create any number of specialized methods like this one here that allows a caller to change the name value that was stored in `self`. All we have to do here is call the method and pass a new value for the name. Let's see this in action.

```
> lizard = Lizard('deep')
> print(lizard.name)
deep

> lizard.set_name('lizard')
> print(lizard.name)
lizard
```

We create an object instance of the class by specifying the class name and passing the constructor arguments. The constructor will receive these arguments and the constructor code will run saving the passed name.

We can then access the `name` and print it, and also call the `set_name()` method to change the name. Multiple of these `Lizard` instances can exist inside a program, and each one will contain its own data.

From an object oriented standpoint, the important part about this setup is that the attributes and the methods are organized and contained within an object.

Let's switch gears now and look at how object oriented programming fits in with PyTorch.

![pytorch logo](https://deeplizard.com/images/pytorch-logo-dark.svg)

### PyTorch's `torch.nn` Package

To build neural networks in PyTorch, we use the `torch.nn` package, which is PyTorch's neural network (nn) library. We typically import the package like so:

```
import torch.nn as nn
```

This allows us to access neural network package using the `nn` alias. So from now on, if we say `nn`, we mean `torch.nn`. PyTorch's neural network library contains all of the typical components needed to build neural networks.

The primary component we'll need to build a neural network is a [layer](https://deeplizard.com/learn/video/FK77zZxaBoI), and so, as we might expect, PyTorch's neural network library contains classes that aid us in constructing layers.

#### PyTorch's `nn.Module` Class

As we know, deep neural networks are built using multiple layers. This is what makes the network *deep*. Each layer in a neural network has two primary components:

- A transformation (code)
- A collection of weights (data)

Like many things in life, this fact makes layers great candidates to be represented as *objects* using OOP. OOP is short for object oriented programming.

In fact, this is the case with PyTorch. Within the `nn` package, there is a class called `Module`, and it is the base class for all of neural network modules which includes layers.

This means that all of the layers in PyTorch extend the `nn.Module` class and inherit all of PyTorch's built-in functionality within the `nn.Module` class. In OOP this concept is known as inheritance.

Even neural networks extend the `nn.Module` class. This makes sense because neural networks themselves can be thought of as one big layer (if needed, let that sink in over time).

Neural networks and layers in PyTorch extend the `nn.Module` class. This means that we must extend the `nn.Module` class when building a new layer or neural network in PyTorch.

#### PyTorch `nn.Module`s Have A `forward()` Method

When we pass a tensor to our network as input, the tensor flows forward though each layer transformation until the tensor reaches the output layer. This process of a tensor flowing forward though the network is known as a *forward pass*.

The tensor input is passed *forward* though the network.

Each layer has its own transformation (code) and the tensor passes forward through each layer. The composition of all the individual layer forward passes defines the overall forward pass transformation for the network.

The goal of the overall transformation is to transform or map the input to the correct prediction output class, and during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to make the output closer to the correct prediction.

What this all means is that, every PyTorch `nn.Module` has a `forward()` method, and so when we are building layers and networks, we must provide an implementation of the `forward()` method. The forward method is the actual transformation.

#### PyTorch's `nn.functional` Package

When we implement the `forward()` method of our `nn.Module` subclass, we will typically use functions from the `nn.functional` package. This package provides us with many neural network operations that we can use for building layers. In fact, many of the `nn.Module` layer classes use `nn.functional` functions to perform their operations.

The `nn.functional` package contains methods that subclasses of `nn.Module` use for implementing their `forward()` functions. Later, we see an example of this by looking at the PyTorch source code of the `nn.Conv2d` convolutional layer class.

### Building A Neural Network In PyTorch

We now have enough information to provide an outline for building neural networks in PyTorch. The steps are as follows:

*Short version:*

1. Extend the `nn.Module` base class.
2. Define layers as class attributes.
3. Implement the `forward()` method.

*More detailed version:*

1. Create a neural network class that extends the `nn.Module` base class.
2. In the class constructor, define the network's layers as class attributes using pre-built layers from `torch.nn`.
3. Use the network's layer attributes as well as operations from the `nn.functional` API to define the network's forward pass.

#### Extending PyTorch's `nn.Module` Class

Like we did with the `Lizard` class example, let's create a simple class to represent a neural network.

```
class Network:
    def __init__(self):
        self.layer = None

    def forward(self, t):
        t = self.layer(t)
        return t
```

This gives us a simple network class that has a single dummy layer inside the constructor and a dummy implementation for the forward function.

The implementation for the `forward()` function takes in a tensor `t` and transforms it using the dummy layer. After the tensor is transformed, the new tensor is returned.

This is a good start, but the class hasn't yet extended the `nn.Module` class. To make our `Network` class extend `nn.Module`, we must do two additional things:

1. Specify the `nn.Module` class in parentheses on line `1`.
2. Insert a call to the super class constructor on line `3` inside the constructor.

This gives us:

```
class Network(nn.Module): # line 1
    def __init__(self):
        super().__init__() # line 3
        self.layer = None

    def forward(self, t):
        t = self.layer(t)
        return t
```

These changes transform our simple neural network into a PyTorch neural network because we are now extending PyTorch's `nn.Module` base class.

With this, we are done! Now we have a `Network` class that has all of the functionality of the PyTorch `nn.Module` class.

#### Define The Network's Layers As Class Attributes

At the moment, our Network class has a single dummy layer as an attribute. Let's replace this now with some real layers that come pre-built for us from PyTorch's `nn` library. We're building a CNN, so the two types of layers we'll use are linear layers and convolutional layers.

```
class Network(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)
        self.fc2 = nn.Linear(in_features=120, out_features=60)
        self.out = nn.Linear(in_features=60, out_features=10)

    def forward(self, t):
        # implement the forward pass
        return t
```

Alright. At this point, we have a Python class called `Network` that extends PyTorch's `nn.Module` class. Inside of our `Network` class, we have five layers that are defined as attributes. We have two convolutional layers, `self.conv1` and `self.conv2`, and three linear layers, `self.fc1`, `self.fc2`, `self.out`.

We used the abbreviation `fc` in `fc1` and `fc2` because linear layers are also called *fully connected layers*. They also have a third name that we may hear sometimes called *dense*. So linear, dense, and fully connected are all ways to refer to the same type of layer. PyTorch uses the word *linear*, hence the `nn.Linear` class name.

We used the name `out` for the last linear layer because the last layer in the network is the output layer.

### Wrapping Up

![what's next arrows](https://deeplizard.com/images/arrows-1834859_1920.jpg)

We should now have a good idea about how to get started building neural networks in PyTorch using the `torch.nn` library. In the next post we'll investigate the different types of parameters of our layers and gain an understanding of how they are chosen. I'll see you in the next one.

### PyTorch CNN Layer Parameters

Welcome back to this series on neural network programming with PyTorch. In this post, we are going to learn about the layers of our CNN by building an understanding of the parameters we used when constructing them.

![ai cyborg](https://deeplizard.com/images/artificial-intelligence-3382507_1920-cropped-eroded.jpg)

Without further ado, let's get to it!

### Our CNN Layers

In the [last post](https://deeplizard.com/learn/video/k4jY9L8H89U), we started building our CNN by extending the PyTorch neural network `Module` class and defining some layers as class attributes. We defined two convolutional layers and three linear layers by specifying them inside our constructor.

![img](https://deeplizard.com/images/pytorch-logo-dark.svg)

```
class Network(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)
        self.fc2 = nn.Linear(in_features=120, out_features=60)
        self.out = nn.Linear(in_features=60, out_features=10)

    def forward(self, t):
        # implement the forward pass
        return t
```

Each of our layers extends PyTorch's neural network `Module` class. For each layer, there are two primary items encapsulated inside, a forward function definition and a weight tensor.

The weight tensor inside each layer contains the weight values that are updated as the network learns during the training process, and this is the reason we are specifying our layers as attributes inside our `Network` class.

PyTorch's neural network `Module` class keeps track of the weight tensors inside each layer. The code that does this tracking lives inside the `nn.Module` class, and since we are extending the neural network module class, we inherit this functionality automatically.

Remember, inheritance is one of those object oriented concepts that we talked about last time. All we have to do to take advantage of this functionality is assign our layers as attributes inside our network module, and the `Module` base class will see this and register the weights as learnable parameters of our network.

### CNN Layer Parameters

Our goal in this post is to better understand the layers we have defined. To do this, we're going to learn about the parameters and the values that we passed for these parameters in the layer constructors.

#### Parameter Vs Argument

First, let's clear up some lingo that pertains to parameters in general. We often hear the words parameter and argument, but what's the difference between these two?

We'll parameters are used in function definitions as place-holders while arguments are the actual values that are passed to the function. The parameters can be thought of as local variables that live inside a function.

n our network's case, the names are the parameters and the values that we have specified are the arguments.

#### Two Types Of Parameters

![img](https://deeplizard.com/images/fast%20speed%20power%20boat.jpg)

To better understand the argument values for these parameters, let's consider two categories or types of parameters that we used when constructing our layers.

1. Hyperparameters
2. Data dependent hyperparameters

A lot of terms in deep learning are used loosely, and the word parameter is one of them. Try not to let it through you off. The main thing to remember about any type of parameter is that the parameter is a place-holder that will eventually hold or have a value.

The goal of these particular categories is to help us remember how each parameter's value is decided.

When we construct a layer, we pass values for each parameter to the layer's constructor. With our convolutional layers have three parameters and the linear layers have two parameters.

- Convolutional layers
  - `in_channels`
  - `out_channels`
  - `kernel_size`
- Linear layers
  - `in_features`
  - `out_features`

Let's see how the values for the parameters are decided. We'll start by looking at hyperparameters, and then, we'll see how the dependent hyperparameters fall into place.

#### Hyperparameters

In general, hyperparameters are parameters whose values are chosen manually and arbitrarily.

As neural network programmers, we choose hyperparameter values mainly based on trial and error and increasingly by utilizing values that have proven to work well in the past. For building our CNN layers, these are the parameters we choose manually.

- `kernel_size`
- `out_channels`
- `out_features`

This means we simply choose the values for these parameters. In neural network programming, this is pretty common, and we usually test and tune these parameters to find values that work best.

| Parameter      | Description                                                  |
| -------------- | ------------------------------------------------------------ |
| `kernel_size`  | Sets the filter size. The words *kernel* and *filter* are interchangeable. |
| `out_channels` | Sets the number of filters. One filter produces one output channel. |
| `out_features` | Sets the size of the output tensor.                          |

On pattern that shows up quite often is that we increase our `out_channels` as we add additional conv layers, and after we switch to linear layers we shrink our `out_features` as we filter down to our number of output classes.

All of these parameters impact our network's architecture. Specifically, these parameters directly impact the weight tensors inside the layers. We'll dive deeper into this in the next post when we talk about learnable parameters and inspect the weight tensors, but for now, let's cover dependent hyperparameters.

#### Data Dependent Hyperparameters

Data dependent hyperparameters are parameters whose values are dependent on data. The first two data dependent hyperparameters that stick out are the `in_channels` of the first convolutional layer, and the `out_features` of the output layer.

You see, the `in_channels` of the first convolutional layer depend on the number of color channels present inside the images that make up the training set. Since we are dealing with grayscale images, we know that this value should be a `1`.

![gears](https://deeplizard.com/images/gears%20machine.jpg)

The `out_features` for the output layer depend on the number of classes that are present inside our training set. Since we have `10` classes of clothing inside the Fashion-MNIST dataset, we know that we need `10` output features.

In general, the *input* to one layer is the *output* from the previous layer, and so all of the `in_channels` in the conv layers and `in_features` in the linear layers depend on the data coming from the previous layer.

When we switch from a conv layer to a linear layer, we have to [flatten our tensor](https://deeplizard.com/learn/video/mFAIBMbACMA). This is why we have `12*4*4`. The twelve comes from the number of output channels in the previous layer, but why do we have the two `4`s? We cover how we get these values in a future post.

#### Summary Of Layer Parameters

We'll learn more about the inner workings of our network and how our tensors flow through our network when we implement our `forward()` function. For now, be sure to check out this table that describes each of the parameters, to make sure you can understand how each parameter value is determined.

```
self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)
self.fc2 = nn.Linear(in_features=120, out_features=60)
self.out = nn.Linear(in_features=60, out_features=10)
```

| Layer | Param name   | Param value | The param value is                                      |
| ----- | ------------ | ----------- | ------------------------------------------------------- |
| conv1 | in_channels  | 1           | the number of color channels in the input image.        |
| conv1 | kernel_size  | 5           | a hyperparameter.                                       |
| conv1 | out_channels | 6           | a hyperparameter.                                       |
| conv2 | in_channels  | 6           | the number of out_channels in previous layer.           |
| conv2 | kernel_size  | 5           | a hyperparameter.                                       |
| conv2 | out_channels | 12          | a hyperparameter (higher than previous conv layer).     |
| fc1   | in_features  | 12*4*4      | the length of the flattened output from previous layer. |
| fc1   | out_features | 120         | a hyperparameter.                                       |
| fc2   | in_features  | 120         | the number of out_features of previous layer.           |
| fc2   | out_features | 60          | a hyperparameter (lower than previous linear layer).    |
| out   | in_features  | 60          | the number of out_channels in previous layer.           |
| out   | out_features | 10          | the number of prediction classes.                       |

### Wrapping Up

In the next post, we'll learn about learnable parameters, which are parameters whose values are learned during the training process. See you there!

### CNN Weights - Learnable Parameters In Neural Networks

Welcome back to this series on neural network programming with PyTorch. It's time now to learn about the weight tensors inside our CNN. We'll find that these weight tensors live inside our layers and are learnable parameters of our network. Without further ado, let's get started.

![ai cyborg](https://deeplizard.com/images/ai-cyborg-cropped-2.jpg)

### Our Neural Network

In the last couple of posts in [this series](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG), we've started building our CNN, and we put in some work to understand the layers we defined inside our network's constructor.

```
class Network(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)
        self.fc2 = nn.Linear(in_features=120, out_features=60)
        self.out = nn.Linear(in_features=60, out_features=10)

    def forward(self, t):
        # implement the forward pass
        return t
```

Ultimately, our next step in the overall process is to use these layers inside our network's forward method, but right now, let's take a look at the learnable parameters inside our network.

We already know about hyperparameters. We saw that hyperparameters are parameters whose values are picked arbitrarily.

The hyperparameters we've used up to this point were the parameters that we used to construct our network's architecture though the layers we constructed and assigned as class attributes.

Hyperparameter values are chosen arbitrarily.

These hyperparameters aren't the only hyperparameters though, and we will see more hyperparameters when we start the training process. What we are concerned with now is the learnable parameters of our network.

### Learnable Parameters

*Learnable parameters* are parameters whose values are learned during the training process.

With learnable parameters, we typically start out with a set of arbitrary values, and these values then get updated in an iterative fashion as the network learns.

In fact, when we say that a network is learning, we specifically mean that the network is learning the appropriate values for the learnable parameters. Appropriate values are values that minimize the loss function.

When it comes to our network, we might be thinking, where are these learnable parameters?

Where are the learnable parameters?

We'll the learnable parameters are the weights inside our network, and they live inside each layer.

### Getting An Instance The Network

In PyTorch, we can inspect the weights directly. Let's grab an instance of our network class and see this.

```
network = Network()                                    
```

Remember, to get an object instance of our Network class, we type the class name followed by parentheses. When this code executes, the code inside the `__init__` class constructor will run, assigning our layers as attributes before the object instance is returned.

The name `__init__` is short for initialize. In an object's case, the attributes are initialized with values, and these values can indeed be other objects. In this way, objects can be nested inside other objects.

This is the case with our network class whose class attributes are initialized with instances of PyTorch layer classes. After the object is initialized, we can then access our object using the network variable.

Before we start to work with our newly created network object, have a look at what happens when we pass our network to Python's `print()` function.

```
> print(network)
Network(
    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
    (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
    (fc1): Linear(in_features=192, out_features=120, bias=True)
    (fc2): Linear(in_features=120, out_features=60, bias=True)
    (out): Linear(in_features=60, out_features=10, bias=True)
)
```

The `print()` function prints to the console a string representation of our network. With a sharp eye, we can notice that the printed output here is detailing our network's architecture listing out our network's layers, and showing the values that were passed to the layer constructors.

#### Network String Representation

One question is though. How is that happening?

Where is this string representation coming from?

We'll our network class is inheriting this functionality from the PyTorch Module base class. Watch what happens if we stop extending the neural network module class.

```
> print(network)
<__main__.Network object at 0x0000017802302FD0>
```

Now, we don't get that nice descriptive output like before. Instead we get this technical gibberish which is the default Python string representation that we get if we don't provide one.

For this reason, in object oriented programming, we usually want to provide a string representation of our object inside our classes so that we get useful information when the object is printed. This string representation comes from Python's default base class called object.

#### How Overriding Works

All Python classes automatically extend the object class. If we want to provide a custom string representation for our object, we can do it, but we need to introduce another object oriented concept called *overriding*.

When we extend a class, we get all of its functionality, and to complement this, we can add additional functionality. However, we can also override existing functionality by changing it to behave differently.

We can override Python's default string representation using the `__repr__` function. This name is short for *representation*.

```
def __repr__(self):
    return "lizardnet"
```

This time when we pass the network to the print function the string that we specified in our class definition is printed in place of the Python's default string.

```
> print(network)
lizardnet
```

When we talked about OOP before, we learned about the `__init__` method and how it is a special Python method for constructing objects.

We'll there are other special methods we'll encounter and `__repr__` is one of them. All the special OOP Python methods typically have the double underscore pre and post-fixes.

This is how the PyTorch Module base class works as well. The Module base class overrides the `__repr__` function.

### What's In The String Representation?

For the most, the string representation that PyTorch gives us pretty much matches what we would expect based on how we configured our network's layers.

However, there is a bit of additional information that we should highlight.

#### Convolutional Layers

For the convolutional layers, the kernel_size argument is a Python tuple `(5,5)` even though we only passed the number `5` in the constructor.

```
Network(
    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
    (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
    (fc1): Linear(in_features=192, out_features=120, bias=True)
    (fc2): Linear(in_features=120, out_features=60, bias=True)
    (out): Linear(in_features=60, out_features=10, bias=True)
)
```

This is because our filters actually have a height and width, and when we pass a single number, the code inside the layer's constructor assumes that we want a square filter.

```
self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)
```

The stride is an additional parameter that we could have set, but we left it out. When the stride is not specified in the layer constructor the layer automatically sets it.

The stride tells the conv layer how far the filter should slide after each operation in the overall convolution. This tuple says to slide by one unit when moving to the right and also by one unit when moving down.

#### Linear Layers

For the linear layers, we have an additional parameter called bias which has a default parameter value of true. It is possible to turn this off by setting it to false.

```
self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)
self.fc2 = nn.Linear(in_features=120, out_features=60)
self.out = nn.Linear(in_features=60, out_features=10)                     
```

One thing to note about the information displayed for our objects when we print them is that it's completely arbitrary information.

As developers, we can decide to put any information there. However, the [Python documentation](https://docs.python.org/3/reference/datamodel.html#object.__repr__) tells us that the info should be complete enough that it can be used to reconstruct the object if needed.

### Accessing The Network's Layers

Well, now that we've got an instance of our network and we've reviewed our layers, let's see how we can access them in code.

In Python and many other programming languages, we access attributes and methods of objects using dot notation.

```
> network.conv1
Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))

> network.conv2
Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))

> network.fc1
Linear(in_features=192, out_features=120, bias=True)

> network.fc2                                    
Linear(in_features=120, out_features=60, bias=True)

> network.out
Linear(in_features=60, out_features=10, bias=True)
```

This is dot notation in action. With dot notation, we use a dot to indicate that we want to sort of open up the object and access something that's inside. We've already been using this quite a bit, so the mention here just gives us a label for the concept.

Something to notice about this that pertains directly to what we were just talking about with the string representation of the network is that each of these pieces of code are also giving us a string representation of each layer.

In the network's case, the network class is really just compiling all this data together to give us a single output.

```
Network(
    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
    (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
    (fc1): Linear(in_features=192, out_features=120, bias=True)
    (fc2): Linear(in_features=120, out_features=60, bias=True)
    (out): Linear(in_features=60, out_features=10, bias=True)
)
```

One last thing to mention about the string representation of these objects is that, in this case, we aren't actually using the print method.

The reason we are still getting back the string representation is because we are using Jupyter notebook, and behind the scenes the notebook is accessing the string representation so it can have something to show us. This is like a really good example of the main use case for string representations.

### Accessing The Layer Weights

Now that we have access to each of our layers, we can access the weights inside each layer. Let's see this for our first convolutional layer.

```
> network.conv1.weight
Parameter containing:
tensor([[[[ 0.0692,  0.1029, -0.1793,  0.0495,  0.0619],
            [ 0.1860,  0.0503, -0.1270, -0.1240, -0.0872],
            [-0.1924, -0.0684, -0.0028,  0.1031, -0.1053],
            [-0.0607,  0.1332,  0.0191,  0.1069, -0.0977],
            [ 0.0095, -0.1570,  0.1730,  0.0674, -0.1589]]],

        [[[-0.1392,  0.1141, -0.0658,  0.1015,  0.0060],
            [-0.0519,  0.0341,  0.1161,  0.1492, -0.0370],
            [ 0.1077,  0.1146,  0.0707,  0.0927,  0.0192],
            [-0.0656,  0.0929, -0.1735,  0.1019, -0.0546],
            [ 0.0647, -0.0521, -0.0687,  0.1053, -0.0613]]],

        [[[-0.1066, -0.0885,  0.1483, -0.0563,  0.0517],
            [ 0.0266,  0.0752, -0.1901, -0.0931, -0.0657],
            [ 0.0502, -0.0652,  0.0523, -0.0789, -0.0471],
            [-0.0800,  0.1297, -0.0205,  0.0450, -0.1029],
            [-0.1542,  0.1634, -0.0448,  0.0998, -0.1385]]],

        [[[-0.0943,  0.0256,  0.1632, -0.0361, -0.0557],
            [ 0.1083, -0.1647,  0.0846, -0.0163,  0.0068],
            [-0.1241,  0.1761,  0.1914,  0.1492,  0.1270],
            [ 0.1583,  0.0905,  0.1406,  0.1439,  0.1804],
            [-0.1651,  0.1374,  0.0018,  0.0846, -0.1203]]],

        [[[ 0.1786, -0.0800, -0.0995,  0.1690, -0.0529],
            [ 0.0685,  0.1399,  0.0270,  0.1684,  0.1544],
            [ 0.1581, -0.0099, -0.0796,  0.0823, -0.1598],
            [ 0.1534, -0.1373, -0.0740, -0.0897,  0.1325],
            [ 0.1487, -0.0583, -0.0900,  0.1606,  0.0140]]],

        [[[ 0.0919,  0.0575,  0.0830, -0.1042, -0.1347],
            [-0.1615,  0.0451,  0.1563, -0.0577, -0.1096],
            [-0.0667, -0.1979,  0.0458,  0.1971, -0.1380],
            [-0.1279,  0.1753, -0.1063,  0.1230, -0.0475],
            [-0.0608, -0.0046, -0.0043, -0.1543,  0.1919]]]], 
            requires_grad=True
)
```

The output is a tensor, but before we look at the tensor, let's talk OOP for a moment. This is a good example that showcases how objects are nested. We first access the conv layer object that lives inside the network object.

```
network.conv1.weight
```

Then, we access the weight tensor object that lives inside the conv layer object, so all of these objects are chained or linked together.

![chain](https://deeplizard.com/images/chain.jpg)

One thing to notice about the weight tensor output is that it says *parameter containing* at the top of the output. This is because this particular tensor is a special tensor because its values or scalar components are learnable parameters of our network.

This means that the values inside this tensor, the ones we see above, are actually learned as the network is trained. As we train, these weight values are updated in such a way that the [loss function ](https://deeplizard.com/learn/video/Skc8nqJirJg)is minimized.

#### PyTorch Parameter Class

To keep track of all the weight tensors inside the network. PyTorch has a special class called `Parameter`. The `Parameter` class extends the tensor class, and so the weight tensor inside every layer is an instance of this `Parameter` class. This is why we see the `Parameter containing` text at the top of the string representation output.

We can see in the Pytorch source code that the `Parameter` class is overriding the `__repr__` function by prepending the text parameter containing to the regular tensor class representation output.

```
def __repr__(self):
    return 'Parameter containing:\n' + super(Parameter, self).__repr__()
```

PyTorch's `nn.Module` class is basically looking for any attributes whose values are instances of the Parameter class, and when it finds an instance of the parameter class, it keeps track of it.

All of this is really technical PyTorch details that go on behind the scenes, and we'll see this come in to play in a bit.

For our understanding now though, the important part is the interpretation of the shape of the weight tensors. This is where we'll start to use the knowledge we learned about tensors early on in the series.

Let's look at the shapes now, and then interpret them.

### Weight Tensor Shape

In the last post, we said that the parameter values we pass to our layers directly impact our network's weights. This is where will see this impact.

![tensor cube](https://deeplizard.com/images/tensor%20cube%20forward%20pass.jpg)

For the convolutional layers, the weight values live inside the filters, and in code, the filters are actually the weight tensors themselves.

The [convolution](https://deeplizard.com/resource/pavq7noze2) operation inside a layer is an operation between the input channels to the layer and the filter inside the layer. This means that what we really have is an operation between two tensors.

With that being said, let's interpret these weight tensors which will allow us to better understand the convolution operations inside our network.

Remember, the shape of a tensor really encodes all the information we need to know about the tensor.

For the first conv layer, we have `1` color channel that should be convolved by `6` filters of size `5x5` to produce `6` output channels. This is how we interpret the values inside our layer constructor.

```
> network.conv1
Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
```

Inside our layer though, we don't explicitly have `6` weight tensors for each of the `6` filters. We actually represent all `6` filters using a single weight tensor whose shape reflects or accounts for the `6` filters.

The shape of the weight tensor for the first convolutional layer shows us that we have a rank-4 weight tensor. The first axis has a length of `6`, and this accounts for the `6` filters.

```
> network.conv1.weight.shape
torch.Size([6, 1, 5, 5])
```

The second axis has a length of `1` which accounts for the single input channel, and the last two axes account for the height and width of the filter.

The way to think about this is as if we are packaging all of our filters into a single tensor.

Now, the second conv layer has `12` filters, and instead of convolving a single input channel, there are `6` input channels coming from the previous layer.

```
> network.conv2.weight.shape
torch.Size([12, 6, 5, 5])
```

Think of this value of `6` here as giving each of the filters some depth. Instead of having a filter that convolves all of the channels iteratively, our filter has a depth that matches the number of channels.

The two main takeaways about these convolutional layers is that our filters are represented using a single tensor and that each filter inside the tensor also has a depth that accounts for the input channels that are being convolved.

1. All filters are represented using a single tensor.
2. Filters have depth that accounts for the input channels.

Our tensors are rank-4 tensors. The first axis represents the number of filters. The second axis represents the depth of each filter which corresponds to the number of input channels being convolved.

The last two axes represent the height and width of each filter. We can pull out any single filter by indexing into the weight tensor's first axis.

(Number of filters, Depth, Height, Width)

This gives us a single filter that has a height and width of `5` and a depth of `6`.

### Weight Matrix

With linear layers or fully connected layers, we have flattened rank-1 tensors as input and as output. The way we transform the in_features to the out_features in a linear layer is by using a rank-2 tensor that is commonly called a weight matrix.

This is due to the fact that the weight tensor is of rank-2 with height and width axes.

```
> network.fc1.shape
torch.Size([120, 192])

> network.fc2.shape                                    
torch.Size([60, 120])

> network.out.shape
torch.Size([10, 60])
```

Here we can see that each of our linear layers have a rank-2 weight tensor. The pattern that we can see here is that the height of the weight tensor has the length of the desired output features and a width of the input features.

#### Matrix Multiplication

This fact is due to how matrix multiplication is performed. Let's see this in action with a smaller example.

![matrix](https://deeplizard.com/images/matrix%20multiplication.jpg)

Suppose we have two rank-2 tensors. The first has a shape of `3x4` and the second has a shape of `4x1`. Now, since we are demonstrating something called matrix multiplication, we'll note that both of these rank-2 tensors are indeed *matrices*.

For each row-column combination in the output, the value is obtained by taking the dot product of the corresponding row of the first matrix with the corresponding column of the second matrix.

Since the second matrix in our example only has `1` column, we use it all three times, but this idea generalizes.

The rule for this operation to work is that the number of columns in the first matrix must match the number of rows in the second matrix. If this rule holds, matrix multiplication operations like this can be performed.

The dot product means that we sum the products of corresponding components. In case you are wondering, both the dot product and matrix multiplication are linear algebra concepts.

#### Linear Function Represented Using A Matrix

The important thing about matrix multiplications like this is that they represent linear functions that we can use to build up our neural network.

Specifically, the weight matrix is a linear function also called a linear map that maps a vector space of `4` dimensions to a vector space of `3` dimensions.

When we change the weight values inside the matrix, we are actually changing this function, and this is exactly what we want to do as we search for the function that our network is ultimately approximating.

Let's see how to perform this same computation using PyTorch.

#### Using PyTorch For Matrix Multiplication

Here, we have the `in_features` and the `weight_matrix` as tensors, and we're using the tensor method called `matmul()` to perform the operation. The name `matmul()` as we now know is short for matrix multiplication.

```
> weight_matrix.matmul(in_features)
tensor([30., 40., 50.])
```

A looming question is, how can we access all of the parameters at once? There is an easy way. Let me just show you.

### Accessing The Networks Parameters

The first example is the most common way, and we'll use this to iterate over our weights when we update them during the training process.

```
for param in network.parameters():
    print(param.shape)

torch.Size([6, 1, 5, 5])
torch.Size([6])
torch.Size([12, 6, 5, 5])
torch.Size([12])
torch.Size([120, 192])
torch.Size([120])
torch.Size([60, 120])
torch.Size([60])
torch.Size([10, 60])
torch.Size([10])
```

The second way is just to show how we can see the name as well. This reveals something that we won't cover in detail, the bias is also a learnable parameter. Each layer has a bias by default, so for each layer we have a weight tensor and a bias tensor.

```
for name, param in network.named_parameters():
    print(name, '\t\t', param.shape)

conv1.weight 		 torch.Size([6, 1, 5, 5])
conv1.bias 		 torch.Size([6])
conv2.weight 		 torch.Size([12, 6, 5, 5])
conv2.bias 		 torch.Size([12])
fc1.weight 		 torch.Size([120, 192])
fc1.bias 		 torch.Size([120])
fc2.weight 		 torch.Size([60, 120])
fc2.bias 		 torch.Size([60])
out.weight 		 torch.Size([10, 60])
out.bias 		 torch.Size([10])
```

### Wrapping Up

We should now have a good understanding of learnable parameters, where the live inside our network, and how to access the weight tensors using PyTorch.

In the next post, we'll see how to work with our layers by passing tensors to them. I'll see you there.

### PyTorch Callable Neural Networks - Deep Learning In Python

Welcome to this series on neural network programming with PyTorch. In this one, we'll learn about how PyTorch neural network modules are callable, what this means, and how it informs us about how our network and layer forward methods are called.

![img](https://deeplizard.com/images/pytorch-logo-dark.svg)

Without further ado, let's get started.

### How Linear Layers Work

In the last [post](https://deeplizard.com/learn/video/stWU37L91Yc) of this [series](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG), we learned about how linear layers use matrix multiplication to transform their in features to out features.

![img](https://deeplizard.com/images/matrix%20multiplication.jpg)

When the input features are received by a linear layer, they are received in the form of a [flattened](https://deeplizard.com/learn/video/fCVuiW9AFzY) 1-dimensional tensor and are then multiplied by the weight matrix. This matrix multiplication produces the output features.

Let's see an example of this in code.

#### Transform Using A Matrix

```
in_features = torch.tensor([1,2,3,4], dtype=torch.float32)

weight_matrix = torch.tensor([
    [1,2,3,4],
    [2,3,4,5],
    [3,4,5,6]
], dtype=torch.float32)

> weight_matrix.matmul(in_features)
tensor([30., 40., 50.])
```

Here, we have created a 1-dimensional tensor called `in_features`. We have also created a weight matrix which of course is a 2-dimensional tensor. Then, we've use the `matmul()` function to preform the matrix multiplication operation that produces a 1-dimensional tensor.

In general, the weight matrix defines a linear function that maps a 1-dimensional tensor with four elements to a 1-dimensional tensor that has three elements. We can think of this function as a mapping from 4-dimensional Euclidean space to 3-dimensional Euclidean space.

This is how linear layers work as well. They map an `in_feature` space to an `out_feature` space using a weight matrix.

#### Transform Using A PyTorch Linear Layer

Let's see how to create a PyTorch linear layer that will do this same operation.

```
fc = nn.Linear(in_features=4, out_features=3, bias=False)
```

Here, we have it. We've defined a linear layer that accepts `4` in features and transforms these into `3` out features, so we go from 4-dimensional space to 3-dimensional space. We know that a weight matrix is used to preform this operation, but where is the weight matrix in this example?

We'll the weight matrix is lives inside the PyTorch `LinearLayer` class and is created by PyTorch. The PyTorch `LinearLayer` class uses the numbers `4` and `3` that are passed to the constructor to create a `3 x 4` weight matrix. Let's verify this by taking a look at the PyTorch source code.

```
# torch/nn/modules/linear.py (version 1.0.1)

def __init__(self, in_features, out_features, bias=True):
    super(Linear, self).__init__()
    self.in_features = in_features
    self.out_features = out_features
    self.weight = Parameter(torch.Tensor(out_features, in_features))
    if bias:
        self.bias = Parameter(torch.Tensor(out_features))
    else:
        self.register_parameter('bias', None)
    self.reset_parameters()
```

As we have seen, when we multiply a `3 x 4` matrix with a `4 x 1` matrix, the result is a `3 x 1` matrix. This is why PyTorch builds the weight matrix in this way. These are linear algebra rules for matrix multiplication.

Let's see how we can call our layer now by passing the `in_features` tensor.

```
> fc(in_features)
tensor([-0.8877,  1.4250,  0.8370], grad_fn=<SqueezeBackward3>)
```

We can call the object instance like this because PyTorch neural network modules are [callable Python objects](https://en.wikipedia.org/wiki/Callable_object). We'll look at this important detail more closely in a minute, but first, check out this output. We did indeed get a 1-dimensional tensor with three elements. However, different values were produced.

This is because PyTorch creates a weight matrix and initializes it with random values. This means that the linear functions from the two examples are different, so we are using different function to produce these outputs.

![img](https://deeplizard.com/images/random%20numbers%20dice.jpg)

Remember the values inside the weight matrix define the linear function. This demonstrates how the network's mapping changes as the weights are updated during the training process.

Let's explicitly set the weight matrix of the linear layer to be the same as the one we used in our other example.

```
fc.weight = nn.Parameter(weight_matrix)  
```

PyTorch module weights need to be parameters. This is why we wrap the weight matrix tensor inside a parameter class instance. Let's see now how this layer transforms the input using the new weight matrix. We hope to see the same results as in our previous example.

```
> fc(in_features)
tensor([30.0261, 40.1404, 49.7643], grad_fn=<AddBackward0>)
```

This time we are much closer to the `30`, `40`, and `50` values. However, we're exact. Why is this? We'll, this is not exact because the linear layer is adding a [bias](https://deeplizard.com/learn/video/HetFihsXSys) tensor to the output. Watch what happens when we turn the bias off. We do this by passing a False flag to the constructor.

```
fc = nn.Linear(in_features=4, out_features=3, bias=False)
fc.weight = nn.Parameter(weight_matrix)
> fc(in_features)
tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)
```

There, now we have an exact match. This is how linear layers work.

#### Mathematical Notation Of The Linear Transformation

Sometimes we'll see linear layer operation referred to asy=Ax+b.In this equation, we have the following:

| Variable | Definition           |
| -------- | -------------------- |
| A        | Weight matrix tensor |
| x        | Input tensor         |
| b        | Bias tensor          |
| y        | Output tensor        |

We'll note that this is similar to the equation for a liney=mx+b.

### Callable Layers And Neural Networks

We pointed out before how it was kind of strange that we called the layer object instance as if it were a function.

```
> fc(in_features)
tensor([30.0261, 40.1404, 49.7643], grad_fn=<AddBackward0>)
```

What makes this possible is that PyTorch module classes implement another special Python function called `__call__()`. If a class implements the `__call__()` method, the special call method will be invoked anytime the object instance is called.

This fact is an important PyTorch concept because of the way the `__call__()` method interacts with the `forward()` method for our layers and networks.

Instead of calling the `forward()` method directly, we call the object instance. After the object instance is called, the `__call__()` method is invoked under the hood, and the `__call__()` in turn invokes the `forward()` method. This applies to all PyTorch neural network modules, namely, networks and layers.

Let's see this in the PyTorch source code.

```
# torch/nn/modules/module.py (version 1.0.1)

def __call__(self, *input, **kwargs):
    for hook in self._forward_pre_hooks.values():
        hook(self, input)
    if torch._C._get_tracing_state():
        result = self._slow_forward(*input, **kwargs)
    else:
        result = self.forward(*input, **kwargs)
    for hook in self._forward_hooks.values():
        hook_result = hook(self, input, result)
        if hook_result is not None:
            raise RuntimeError(
                "forward hooks should never return any values, but '{}'"
                "didn't return None".format(hook))
    if len(self._backward_hooks) > 0:
        var = result
        while not isinstance(var, torch.Tensor):
            if isinstance(var, dict):
                var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
            else:
                var = var[0]
        grad_fn = var.grad_fn
        if grad_fn is not None:
            for hook in self._backward_hooks.values():
                wrapper = functools.partial(hook, self)
                functools.update_wrapper(wrapper, hook)
                grad_fn.register_hook(wrapper)
    return result
```

The extra code that PyTorch runs inside the `__call__()` method is why we never invoke the `forward()` method directly. If we did, the additional PyTorch code would not be executed. As a result, any time we want to invoke our `forward()` method, we call the object instance. This applies to both layers, and networks because they are both PyTorch neural network modules.

We are now ready to implement our network's `forward()` method. I'll see you in the next one!

### How To Debug PyTorch Source Code

Welcome to deeplizard. My name's Chris. In this episode, we're going to see how we can debug PyTorch source code using the Visual Studio Code IDE.

![img](https://deeplizard.com/images/artificial-intelligence-3382507_1920-cropped.jpg)

Without further ado, let's get started.

### Getting Started Debugging PyTorch

The first step, of course, is to get [Visual Studio Code](https://code.visualstudio.com/) installed on your system. Once this is complete, we are ready to go.

#### A Program To Debug

To debug, we need a program that we can use to kick things off.

We have created a sample program here that we're going use to debug some PyTorch source code. We're going debug the Fashion MNIST dataset which actually lives in the `torchvision` package. But nonetheless, everything is going be the same.

```
import torch
import torchvision
import torchvision.transforms as transforms

train_set = torchvision.datasets.FashionMNIST(
    root='./data'
    ,train=True
    ,download=True
    ,transform=transforms.Compose([
        transforms.ToTensor()
    ])
)

image, label = train_set[0]

print(image.shape)
```

#### How To Debug

In order to debug any python code in Visual Studio Code, we need toe install the python debugging extension. This will give us debugging capabilities. Once the debugging extension is installed, we follow these steps.

1. Place a breakpoint
2. Run the program in debug mode
3. Use Keyboard to manually control program execution
4. Step into something PyTorch

Note that there is an additional requirement to debug imported libraries like PyTorch. We need to set the `justMyCode` attribute to `false` inside the `launch.json` file for VS Code.

Below is an example launch configuration.

```
{
    "name": "Python: Current File",
    "type": "python",
    "request": "launch",
    "program": "${file}",
    "justMyCode": false,
    "cwd": "${fileDirname}"
}
```

Debugging code is one of the absolute best ways to understand what's going on. Note that we don't have to write a lot of code to actually get set up to debug PyTorch. In our example it was just 15 or 16 lines of code.

To debug PyTorch source code, we simply need a path into the PyTorch code we want to debug.

### CNN Forward Pass Implementation

Welcome to this series on neural network programming with PyTorch. In this one, we'll show how to implement the forward method for a convolutional neural network in PyTorch.

![img](https://deeplizard.com/images/pytorch-logo-dark.svg)

Without further ado, let's get started.

### Neural Network Programming Series (Recap)

So far in this series, we've prepared our data, and we're now in the process of building our model.

We created our network by extending the `nn.Module` PyTorch base class, and then, in the class constructor, we defined the network's layers as class attributes. Now, we need to implement our network's `forward()` method, and then, finally, we'll be ready to train our model.

- Prepare the data
- Build the model
  1. Create a neural network class that extends the `nn.Module` base class.
  2. In the class constructor, define the network's layers as class attributes.
  3. **Use the network's layer attributes as well `nn.functional` API operations to define the network's forward pass.**
- Train the model
- Analyze the model's results

### Reviewing The Network

At the moment, we know that our `forward()` method accepts a tensor as input, and then, returns a tensor as output. Right now, the tensor that is returned is the same tensor that is passed.

However, after we build out the implementation, the returned tensor will be the output of the network.

This means that the forward method implementation will use all of the layers we defined inside the constructor. In this way, the forward method explicitly defines the network's transformation.

The `forward()` method is the actual network transformation. The forward method is the mapping that maps an input tensor to a prediction output tensor. Let's see how this is done.

Recall that in our network's constructor, we can see that we have five layers defined.

```
class Network(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)
        self.fc2 = nn.Linear(in_features=120, out_features=60)
        self.out = nn.Linear(in_features=60, out_features=10)

    def forward(self, t):
        # implement the forward pass      
        return t
```

We have two convolutional layers and three Linear layers. If we count the input layer, this gives us a network with a total of six layers.

### Implementing The `forward()` Method

Let's code this up. We'll kick things off with the input layer.

#### Input Layer #1

The input layer of any neural network is determined by the input data. For example, if our input tensor contains three elements, our network would have three nodes contained in its input layer.

For this reason, we can think of the input layer as the identity transformation. Mathematically, this is the function,f(x)=x.

We give any x as the input, and we get back the same x as the output. This logic is the same regardless of whether we're working with a tensor that has three elements, or a tensor that represents an image with three channels. The *in* is the data *out*!

This is pretty trivial, and this is the reason we usually don't see the input layer when we are working with neural network APIs. The input layer exists implicitly.

It's definitely not required, but for the sake of completion, we'll show the identity operation in our forward method.

```
# (1) input layer
t = t
```

#### Hidden Convolutional Layers: Layers #2 And #3

Both of the hidden convolutional layers are going to be very similar in terms of performing the transformation. In the [deep learning fundamentals series](https://deeplizard.com/learn/video/gZmobeGL0Yg), we explained in the [post on layers](https://deeplizard.com/learn/video/FK77zZxaBoI) that all layers that are not the input or output layers are called hidden layers, and this is why we are referring to these convolutional layers as *hidden layers*.

![convolution animation](https://deeplizard.com/images/convolution-animation-2.gif)

To preform the [convolution](https://deeplizard.com/resource/pavq7noze2) operation, we pass the tensor to the forward method of the first convolutional layer, `self.conv1`. We've learned how all PyTorch neural network modules have `forward()` methods, and when we call the `forward()` method of a `nn.Module`, there is a special way that we make the call.

When want to call the `forward()` method of a `nn.Module` instance, we call the actual instance instead of calling the `forward()` method directly.

Instead of doing this `self.conv1.forward(tensor)`, we do this `self.conv1(tensor)`. Make sure you see the [previous post](https://deeplizard.com/learn/video/rcc86nXKwkw) in this series to see all the details on this.

Let's go ahead and add all the calls needed to implement both of our convolutional layers.

```
# (2) hidden conv layer
t = self.conv1(t)
t = F.relu(t)
t = F.max_pool2d(t, kernel_size=2, stride=2)

# (3) hidden conv layer
t = self.conv2(t)
t = F.relu(t)
t = F.max_pool2d(t, kernel_size=2, stride=2)
```

As we can see here, our input tensor is transformed as we move through the convolutional layers. The first convolutional layer has a convolutional operation, followed by a [relu activation](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) operation whose output is then passed to a max pooling operation with `kernel_size=2` and `stride=2`.

The output tensor `t` of the first convolutional layer is then passed to the next convolutional layer, which is identical except for the fact that we call `self.conv2()` instead of `self.conv1()`.

Each of these layers is comprised of a collection of weights (data) and a collection operations (code). The weights are encapsulated inside the `nn.Conv2d()` class instance. The `relu()` and the `max_pool2d()` calls are just pure operations. Neither of these have weights, and this is why we call them directly from the `nn.functional` API.

Sometimes we may see [pooling operations](https://deeplizard.com/learn/video/ZjM_XQa5s6s) referred to as pooling layers. Sometimes we may even hear activation operations called *activation layers*.

However, what makes a layer distinct from an operation is that layers have weights. Since pooling operations and activation functions do not have weights, we will refer to them as operations and view them as being added to the collection of layer operations.

![img](https://deeplizard.com/images/gears%20machine.jpg)

For example, we'll say that the second layer in our network is a convolutional layer that contains a collection of weights, and preforms three operations, a convolution operation, the relu activation operation, and the max pooling operation.

Note that the rules and terminology here are not strict. This is just one way to describe a network. There are other ways to express these ideas. The main thing we need to be aware of is which operations are defined using weights and which ones don't use any weights.

Historically, the operations that are defined using weights are what we call [layers](https://deeplizard.com/learn/video/FK77zZxaBoI). Later, other operations were added to the mix like activation functions and pooling operations, and this caused some confusion in terminology.

Mathematically, the entire network is just a composition of functions, and a composition of functions is a function itself. So a network is just a function. All the terms like layers, activation functions, and weights, are just used to help describe the different parts.

Don't let these terms confuse the fact that the whole network is simply a composition of functions, and what we are doing now is defining this composition inside our `forward()` method.

#### Hidden Linear Layers: Layers #4 And #5

Before we pass our input to the first hidden linear layer, we must `reshape()` or flatten our tensor. This will be the case any time we are passing output from a convolutional layer as input to a linear layer.

Since the forth layer is the first linear layer, we will include our reshaping operation as a part of the forth layer.

```
# (4) hidden linear layer
t = t.reshape(-1, 12 * 4 * 4)
t = self.fc1(t)
t = F.relu(t)

# (5) hidden linear layer
t = self.fc2(t)
t = F.relu(t)
```

We saw in the [post on CNN weights](https://deeplizard.com/learn/video/stWU37L91Yc) that the number `12` in the reshaping operation is determined by the number of output channels coming from the previous convolutional layer.

However, the `4 * 4` was left as an open question. Let's reveal the answer now. The `4 * 4` is actually the height and width of each of the `12` output channels.

We started with a `1 x 28 x 28` input tensor. This gives a single [color channel](https://en.wikipedia.org/wiki/Channel_(digital_image)), `28 x 28` image, and by the time our tensor arrives at the first linear layer, the dimensions have changed.

The height and width dimensions have been reduced from `28 x 28` to `4 x 4` by the convolution and pooling operations.

Convolution and pooling operations are reduction operations on the height and width dimensions. We'll see how this works and see a formula for calculating these reductions in the next post . For now, let's finish implementing our this `forward()` method.

After the tensor is reshaped, we pass the [flattened](https://deeplizard.com/learn/video/fCVuiW9AFzY) tensor to the linear layer and pass this result to the `relu()` [activation function](https://deeplizard.com/learn/video/m0pIlLfpXWE).

#### Output Layer #6

The sixth and last layer of our network is a linear layer we call the output layer. When we pass our tensor to the output layer, the result will be the prediction tensor. Since our data has ten prediction classes, we know our output tensor will have ten elements.

```
# (6) output layer
t = self.out(t)
#t = F.softmax(t, dim=1)
```

The values inside each of the ten components will correspond to the prediction value for each of our prediction classes.

Inside the network we usually use `relu()` as our [non-linear activation function](https://deeplizard.com/learn/video/m0pIlLfpXWE), but for the output layer, whenever we have a single category that we are trying to predict, we use `softmax()`. The [softmax function](https://en.wikipedia.org/wiki/Softmax_function) returns a positive probability for each of the prediction classes, and the probabilities sum to `1`.

However, in our case, we won't use `softmax()` because the loss function that we'll use, `F.cross_entropy()`, implicitly performs the `softmax()` operation on its input, so we'll just return the result of the last linear transformation.

The implication of this is that our network will be trained using the softmax operation but will not need to compute the additional operation when the network is used for inference after the training process is complete.

### Conclusion

Great! We did it. This is how we implement a neural network forward method in PyTorch.

```
def forward(self, t):
    # (1) input layer
    t = t

    # (2) hidden conv layer
    t = self.conv1(t)
    t = F.relu(t)
    t = F.max_pool2d(t, kernel_size=2, stride=2)

    # (3) hidden conv layer
    t = self.conv2(t)
    t = F.relu(t)
    t = F.max_pool2d(t, kernel_size=2, stride=2)

    # (4) hidden linear layer
    t = t.reshape(-1, 12 * 4 * 4)
    t = self.fc1(t)
    t = F.relu(t)

    # (5) hidden linear layer
    t = self.fc2(t)
    t = F.relu(t)

    # (6) output layer
    t = self.out(t)
    #t = F.softmax(t, dim=1)

    return t
```

I'll see you in the next one!

